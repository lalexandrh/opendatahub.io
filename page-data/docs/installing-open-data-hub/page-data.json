{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/installing-open-data-hub/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/api-workbench/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"api-workbench-overview_api-workbench"},{"parentId":null,"name":"Creating a custom image by using the <code>ImageStream</code> CRD","level":1,"index":1,"id":"api-custom-image-creating_api-workbench"},{"parentId":null,"name":"Creating a workbench by using the <code>Notebook</code> CRD","level":1,"index":2,"id":"api-workbench-creating_api-workbench"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/configuring-your-model-serving-platform/"},"sections":[{"parentId":null,"name":"About model-serving platforms","level":1,"index":0,"id":"configuring-your-model-serving-platform_odh-admin"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"About model serving","level":2,"index":0,"id":"about-model-serving_odh-admin"},{"parentId":"about-model-serving_odh-admin","name":"Single-model serving platform","level":3,"index":0,"id":"_single_model_serving_platform"},{"parentId":"about-model-serving_odh-admin","name":"Multi-model serving platform","level":3,"index":1,"id":"_multi_model_serving_platform"},{"parentId":"about-model-serving_odh-admin","name":"NVIDIA NIM model serving platform","level":3,"index":2,"id":"_nvidia_nim_model_serving_platform"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"Model-serving runtimes","level":2,"index":1,"id":"model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes_odh-admin","name":"ServingRuntime","level":3,"index":0,"id":"_servingruntime"},{"parentId":"model-serving-runtimes_odh-admin","name":"InferenceService","level":3,"index":1,"id":"_inferenceservice"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"Model-serving runtimes for accelerators","level":2,"index":2,"id":"model-serving-runtimes-for-accelerators_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"NVIDIA GPUs","level":3,"index":0,"id":"_nvidia_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Intel Gaudi accelerators","level":3,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"AMD GPUs","level":3,"index":2,"id":"_amd_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"IBM Spyre AI accelerators on x86","level":3,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Supported model-serving runtimes","level":3,"index":4,"id":"supported-model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Tested and verified model-serving runtimes","level":3,"index":5,"id":"tested-verified-runtimes_odh-admin"},{"parentId":null,"name":"Configuring model servers on the single-model serving platform","level":1,"index":1,"id":"_configuring_model_servers_on_the_single_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_single_model_serving_platform","name":"Enabling the single-model serving platform","level":2,"index":0,"id":"enabling-the-single-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers_on_the_single_model_serving_platform","name":"Enabling speculative decoding and multi-modal inferencing","level":2,"index":1,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_odh-admin"},{"parentId":"_configuring_model_servers_on_the_single_model_serving_platform","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":2,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers_on_the_single_model_serving_platform","name":"Adding a tested and verified model-serving runtime for the single-model serving platform","level":2,"index":3,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_odh-admin"},{"parentId":null,"name":"Configuring model servers on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform","name":"Enabling the NVIDIA NIM model serving platform","level":2,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_odh-admin"},{"parentId":null,"name":"Configuring model servers on the multi-model serving platform","level":1,"index":3,"id":"_configuring_model_servers_on_the_multi_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_multi_model_serving_platform","name":"Enabling the multi-model serving platform","level":2,"index":0,"id":"enabling-the-multi-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers_on_the_multi_model_serving_platform","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":2,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers_on_the_multi_model_serving_platform","name":"Adding a tested and verified model-serving runtime for the multi-model serving platform","level":2,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_odh-admin"},{"parentId":null,"name":"Customizing model deployments","level":1,"index":4,"id":"_customizing_model_deployments"},{"parentId":"_customizing_model_deployments","name":"Customizing the parameters of a deployed model-serving runtime","level":2,"index":0,"id":"customizing-parameters-serving-runtime_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizable model serving runtime parameters","level":2,"index":1,"id":"customizable-model-serving-runtime-parameters_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizing the vLLM model-serving runtime","level":2,"index":2,"id":"Customizing-the-vllm-runtime_odh-admin"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/deploying-models/"},"sections":[{"parentId":null,"name":"Storing models","level":1,"index":0,"id":"deploying-models_odh-user"},{"parentId":"deploying-models_odh-user","name":"Using OCI containers for model storage","level":2,"index":0,"id":"using-oci-containers-for-model-storage_odh-user"},{"parentId":"deploying-models_odh-user","name":"Storing a model in an OCI image","level":2,"index":1,"id":"storing-a-model-in-oci-image_odh-user"},{"parentId":"deploying-models_odh-user","name":"Uploading model files to a Persistent Volume Claim (PVC)","level":2,"index":2,"id":"uploading-model-files-to-pvc_odh-user"},{"parentId":null,"name":"Deploying models on the single-model serving platform","level":1,"index":1,"id":"_deploying_models_on_the_single_model_serving_platform"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"About KServe deployment modes","level":2,"index":0,"id":"about-kserve-deployment-modes_odh-user"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"Deploying models on the single-model serving platform","level":2,"index":1,"id":"deploying-models-on-the-single-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"Deploying a model stored in an OCI image by using the CLI","level":2,"index":2,"id":"deploying-model-stored-in-oci-image_odh-user"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"Deploying models by using Distributed Inference with llm-d","level":2,"index":3,"id":"deploying-models-using-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Example usage for Distributed Inference with llm-d","level":3,"index":0,"id":"ref-example-distributed-inference_odh-user"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Single-node GPU deployment","level":4,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Multi-node deployment","level":4,"index":1,"id":"_multi_node_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Intelligent inference scheduler with KV cache routing","level":4,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"Monitoring models on the single-model serving platform","level":2,"index":4,"id":"_monitoring_models_on_the_single_model_serving_platform"},{"parentId":"_monitoring_models_on_the_single_model_serving_platform","name":"Viewing performance metrics for a deployed model","level":3,"index":0,"id":"viewing-performance-metrics-for-deployed-model_odh-user"},{"parentId":"_monitoring_models_on_the_single_model_serving_platform","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":3,"index":1,"id":"viewing-metrics-for-the-single-model-serving-platform_odh-user"},{"parentId":null,"name":"Deploying models on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_deploying_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Deploying models on the NVIDIA NIM model serving platform","level":2,"index":0,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing NVIDIA NIM metrics for a NIM model","level":2,"index":1,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing performance metrics for a NIM model","level":2,"index":2,"id":"viewing-performance-metrics-for-a-nim-model_odh-user"},{"parentId":null,"name":"Deploying models on the multi-model serving platform","level":1,"index":3,"id":"_deploying_models_on_the_multi_model_serving_platform"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Adding a model server for the multi-model serving platform","level":2,"index":0,"id":"adding-a-model-server-for-the-multi-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Deleting a model server","level":2,"index":1,"id":"deleting-a-model-server_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Deploying a model by using the multi-model serving platform","level":2,"index":2,"id":"deploying-a-model-using-the-multi-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Viewing a deployed model","level":2,"index":3,"id":"viewing-a-deployed-model_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Updating the deployment properties of a deployed model","level":2,"index":4,"id":"updating-the-deployment-properties-of-a-deployed-model_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Deleting a deployed model","level":2,"index":5,"id":"deleting-a-deployed-model_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":6,"id":"configuring-monitoring-for-the-multi-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":2,"index":7,"id":"viewing-metrics-for-the-multi-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Viewing performance metrics for all models on a model server","level":2,"index":8,"id":"viewing-performance-metrics-for-model-server_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Viewing HTTP request metrics for a deployed model","level":2,"index":9,"id":"viewing-http-request-metrics-for-a-deployed-model_odh-user"},{"parentId":null,"name":"Making inference requests to deployed models","level":1,"index":4,"id":"_making_inference_requests_to_deployed_models"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the authentication token for a deployed model","level":2,"index":0,"id":"accessing-authentication-token-for-deployed-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":1,"id":"accessing-inference-endpoint-for-deployed-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Making inference requests to models deployed on the single-model serving platform","level":2,"index":2,"id":"making-inference-requests-to-models-deployed-on-single-model-serving-platform_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Inference endpoints","level":2,"index":3,"id":"inference-endpoints_odh-user"},{"parentId":"inference-endpoints_odh-user","name":"Caikit TGIS ServingRuntime for KServe","level":3,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"Caikit Standalone ServingRuntime for KServe","level":3,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"TGIS Standalone ServingRuntime for KServe","level":3,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"OpenVINO Model Server","level":3,"index":3,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_odh-user","name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":3,"index":4,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":3,"index":5,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM AMD GPU ServingRuntime for KServe","level":3,"index":6,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":3,"index":7,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"NVIDIA Triton Inference Server","level":3,"index":8,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_odh-user","name":"Seldon MLServer","level":3,"index":9,"id":"_seldon_mlserver"},{"parentId":"inference-endpoints_odh-user","name":"Additional resources","level":3,"index":10,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"overview-for-getting-started_get-started"},{"parentId":"overview-for-getting-started_get-started","name":"Data science workflow","level":2,"index":0,"id":"_data_science_workflow"},{"parentId":"overview-for-getting-started_get-started","name":"About this guide","level":2,"index":1,"id":"_about_this_guide"},{"parentId":"overview-for-getting-started_get-started","name":"Glossary of common terms","level":2,"index":2,"id":"glossary-of-common-terms_get-started"},{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":1,"id":"logging-in_get-started"},{"parentId":"logging-in_get-started","name":"Viewing installed Open Data Hub components","level":2,"index":0,"id":"viewing-installed-components_get-started"},{"parentId":null,"name":"Creating a data science project","level":1,"index":2,"id":"creating-a-data-science-project_get-started"},{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":3,"id":"creating-a-workbench-select-ide_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_get-started"},{"parentId":null,"name":"Next steps","level":1,"index":4,"id":"next-steps_get-started"},{"parentId":"next-steps_get-started","name":"Additional resources","level":2,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":2,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Configuring pipelines with your own Argo Workflows instance","level":1,"index":1,"id":"configuring-pipelines-with-your-own-argo-workflows-instance_install"},{"parentId":null,"name":"Installing the distributed workloads components","level":1,"index":2,"id":"installing-the-distributed-workloads-components_install"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_install"},{"parentId":null,"name":"Working with certificates","level":1,"index":4,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding how Open Data Hub handles certificates","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Adding certificates","level":2,"index":1,"id":"_adding_certificates"},{"parentId":"working-with-certificates_certs","name":"Adding certificates to a cluster-wide CA bundle","level":2,"index":2,"id":"adding-certificates-to-a-cluster-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Adding certificates to a custom CA bundle","level":2,"index":3,"id":"adding-certificates-to-a-custom-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":4,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Accessing S3-compatible object storage with self-signed certificates","level":3,"index":0,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Configuring a certificate for data science pipelines","level":3,"index":1,"id":"configuring-a-certificate-for-pipelines_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Configuring a certificate for workbenches","level":3,"index":2,"id":"configuring-a-certificate-for-workbenches_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using the cluster-wide CA bundle for the single-model serving platform","level":3,"index":3,"id":"using-the-cluster-CA-bundle-for-single-model-serving_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates without the Open Data Hub Operator","level":2,"index":5,"id":"managing-certificates-without-the-operator_certs"},{"parentId":"working-with-certificates_certs","name":"Removing the CA bundle","level":2,"index":6,"id":"_removing_the_ca_bundle"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from all namespaces","level":3,"index":0,"id":"removing-the-ca-bundle-from-all-namespaces_certs"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from a single namespace","level":3,"index":1,"id":"removing-the-ca-bundle-from-a-single-namespace_certs"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":5,"id":"viewing-logs-and-audit-records_install"},{"parentId":"viewing-logs-and-audit-records_install","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_install"},{"parentId":"configuring-the-operator-logger_install","name":"Viewing the Open Data Hub Operator logs","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_logs"},{"parentId":"viewing-logs-and-audit-records_install","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_install"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-and-monitoring-models/"},"sections":[{"parentId":null,"name":"Managing model-serving runtimes","level":1,"index":0,"id":"managing-and-monitoring-models_cluster-admin"},{"parentId":"managing-and-monitoring-models_cluster-admin","name":"Adding a custom model-serving runtime for the single-model serving platform","level":2,"index":0,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models on the single-model serving platform","level":1,"index":1,"id":"_managing_and_monitoring_models_on_the_single_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Setting a timeout for KServe","level":2,"index":0,"id":"setting-timeout-for-kserve_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Deploying models by using multiple GPU nodes","level":2,"index":1,"id":"deploying-models-using-multiple-gpu-nodes_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Configuring an inference service for Kueue","level":2,"index":2,"id":"configuring-an-inference-service-for-kueue_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Configuring an inference service for Spyre","level":2,"index":3,"id":"configuring-inference-service-for-spyre_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Optimizing performance and tuning","level":2,"index":4,"id":"_optimizing_performance_and_tuning"},{"parentId":"_optimizing_performance_and_tuning","name":"Determining GPU requirements for LLM-powered applications","level":3,"index":0,"id":"determining-gpu-requirements-for-llm-powered-applications_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Performance considerations for text-summarization and retrieval-augmented generation (RAG) applications","level":3,"index":1,"id":"performance-considerations-for-document-based-apps_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Inference performance metrics","level":3,"index":2,"id":"inference-performance-metrics_cluster-admin"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Latency","level":4,"index":0,"id":"_latency"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Throughput","level":4,"index":1,"id":"_throughput"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Cost per million tokens","level":4,"index":2,"id":"_cost_per_million_tokens"},{"parentId":"_optimizing_performance_and_tuning","name":"Configuring metrics-based autoscaling","level":3,"index":3,"id":"configuring-metrics-based-autoscaling_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Guidelines for metrics-based autoscaling","level":3,"index":4,"id":"guidelines-for-metrics-based-autoscaling_cluster-admin"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing metrics for latency and throughput-optimized scaling","level":4,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing the right sliding window","level":4,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Optimizing HPA scale-down configuration","level":4,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Considering model size for optimal scaling","level":4,"index":3,"id":"_considering_model_size_for_optimal_scaling"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Monitoring models on the single-model serving platform","level":2,"index":5,"id":"_monitoring_models_on_the_single_model_serving_platform"},{"parentId":"_monitoring_models_on_the_single_model_serving_platform","name":"Configuring monitoring for the single-model serving platform","level":3,"index":0,"id":"configuring-monitoring-for-the-single-model-serving-platform_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Using Grafana to monitor model performance","level":2,"index":6,"id":"_using_grafana_to_monitor_model_performance"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a Grafana metrics dashboard","level":3,"index":0,"id":"Deploying-a-grafana-metrics-dashboard_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a vLLM/GPU metrics dashboard on a Grafana instance","level":3,"index":1,"id":"deploying-vllm-gpu-metrics-dashboard-grafana_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Grafana metrics","level":3,"index":2,"id":"ref-grafana-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"Accelerator metrics","level":4,"index":0,"id":"ref-accelerator-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"CPU metrics","level":4,"index":1,"id":"ref-cpu-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"vLLM metrics","level":4,"index":2,"id":"ref-vllm-metrics_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Customizing model selection options for the NVIDIA NIM model serving platform","level":2,"index":0,"id":"Customizing-model-selection-options_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":2,"index":1,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin","name":"Enabling graph generation for an existing NIM deployment","level":3,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin","name":"Enabling metrics collection for an existing NIM deployment","level":3,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"},{"parentId":null,"name":"Managing and monitoring models on the multi-model serving platform","level":1,"index":3,"id":"_managing_and_monitoring_models_on_the_multi_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_multi_model_serving_platform","name":"Configuring monitoring for the multi-model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-the-multi-model-serving-platform_cluster-admin"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-odh/"},"sections":[{"parentId":null,"name":"Managing users and groups","level":1,"index":0,"id":"managing-users-and-groups"},{"parentId":"managing-users-and-groups","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-odh"},{"parentId":"managing-users-and-groups","name":"Viewing Open Data Hub users","level":2,"index":1,"id":"viewing-data-science-users_managing-odh"},{"parentId":"managing-users-and-groups","name":"Adding users to Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Selecting Open Data Hub administrator and user groups","level":2,"index":3,"id":"selecting-admin-and-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Deleting users","level":2,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":3,"index":0,"id":"about-deleting-users-and-resources_managing-odh"},{"parentId":"_deleting_users","name":"Stopping basic workbenches owned by other users","level":3,"index":1,"id":"stopping-basic-workbenches-owned-by-other-users_managing-odh"},{"parentId":"_deleting_users","name":"Revoking user access to basic workbenches","level":3,"index":2,"id":"revoking-user-access-to-basic-workbenches_managing-odh"},{"parentId":"_deleting_users","name":"Backing up storage data","level":3,"index":3,"id":"backing-up-storage-data_managing-odh"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":3,"index":4,"id":"cleaning-up-after-deleting-users_managing-odh"},{"parentId":null,"name":"Creating custom workbench images","level":1,"index":1,"id":"creating-custom-workbench-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from a default Open Data Hub image","level":2,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from your own image","level":2,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":3,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":3,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-custom-workbench-images","name":"Enabling custom images in Open Data Hub","level":2,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Importing a custom workbench image","level":2,"index":3,"id":"importing-a-custom-workbench-image_custom-images"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":2,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Disabling applications connected to Open Data Hub","level":2,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about available applications","level":2,"index":3,"id":"showing-hiding-information-about-available-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default basic workbench application","level":2,"index":4,"id":"hiding-the-default-basic-workbench-application_dashboard"},{"parentId":null,"name":"Creating project-scoped resources","level":1,"index":3,"id":"creating-project-scoped-resources_managing-odh"},{"parentId":null,"name":"Allocating additional resources to Open Data Hub users","level":1,"index":4,"id":"allocating-additional-resources-to-data-science-users_managing-odh"},{"parentId":null,"name":"Customizing component deployment resources","level":1,"index":5,"id":"customizing-component-deployment-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Overview of component resource customization","level":2,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Customizing component resources","level":2,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Disabling component resource customization","level":2,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Re-enabling component resource customization","level":2,"index":3,"id":"reenabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":6,"id":"enabling-accelerators"},{"parentId":"enabling-accelerators","name":"Enabling NVIDIA GPUs","level":2,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":"enabling-accelerators","name":"Intel Gaudi AI Accelerator integration","level":2,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":"intel-gaudi-ai-accelerator-integration_managing-odh","name":"Enabling Intel Gaudi AI accelerators","level":3,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_managing-odh"},{"parentId":"enabling-accelerators","name":"AMD GPU Integration","level":2,"index":2,"id":"amd-gpu-integration_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Verifying AMD GPU availability on your cluster","level":3,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Enabling AMD GPUs","level":3,"index":1,"id":"enabling-amd-gpus_managing-odh"},{"parentId":null,"name":"Managing workloads with Kueue","level":1,"index":7,"id":"managing-workloads-with-kueue"},{"parentId":"managing-workloads-with-kueue","name":"Overview of managing workloads with Kueue","level":2,"index":0,"id":"overview-of-managing-workloads-with-kueue_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue management states","level":3,"index":0,"id":"_kueue_management_states"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Queue enforcement for projects","level":3,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Restrictions for managing workloads with Kueue","level":3,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue workflow","level":3,"index":3,"id":"kueue-workflow_kueue"},{"parentId":"managing-workloads-with-kueue","name":"Configuring workload management with Kueue","level":2,"index":1,"id":"configuring-workload-management-with-kueue_kueue"},{"parentId":"configuring-workload-management-with-kueue_kueue","name":"Enabling Kueue in the dashboard","level":3,"index":0,"id":"enabling-kueue-in-the-dashboard_kueue"},{"parentId":"managing-workloads-with-kueue","name":"Troubleshooting common problems with Kueue","level":2,"index":2,"id":"troubleshooting-common-problems-with-Kueue_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"failed to call webhook\" error message for Kueue","level":3,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":3,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"local_queue provided does not exist\" error message","level":3,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"The pod provisioned by Kueue is terminated before the image is pulled","level":3,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"Additional resources","level":3,"index":4,"id":"_additional_resources"},{"parentId":"managing-workloads-with-kueue","name":"Migrating to the Red Hat build of Kueue Operator","level":2,"index":3,"id":"migrating-to-the-rhbok-operator_kueue"},{"parentId":null,"name":"Managing distributed workloads","level":1,"index":8,"id":"managing-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring quota management for distributed workloads","level":2,"index":0,"id":"configuring-quota-management-for-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Example Kueue resource configurations for distributed workloads","level":2,"index":1,"id":"ref-example-kueue-resource-configurations_managing-odh"},{"parentId":"ref-example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs without shared cohort","level":3,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":4,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":4,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":4,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":4,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"ref-example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":3,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":4,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":4,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":4,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":4,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring the CodeFlare Operator","level":2,"index":2,"id":"configuring-the-codeflare-operator_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring a cluster for RDMA","level":2,"index":3,"id":"configuring-a-cluster-for-rdma_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Troubleshooting common problems with distributed workloads for administrators","level":2,"index":4,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a suspended state","level":3,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a failed state","level":3,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user receives a \"failed to call webhook\" error message for the CodeFlare Operator","level":3,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster does not start","level":3,"index":3,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user cannot create a Ray cluster or submit jobs","level":3,"index":4,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"Additional resources","level":3,"index":5,"id":"_additional_resources_2"},{"parentId":null,"name":"Backing up data","level":1,"index":9,"id":"backing-up-data_data-mgmt"},{"parentId":"backing-up-data_data-mgmt","name":"Backing up storage data","level":2,"index":0,"id":"backing-up-storage-data_data-mgmt"},{"parentId":"backing-up-data_data-mgmt","name":"Backing up your cluster","level":2,"index":1,"id":"backing-up-your-cluster_data-mgmt"},{"parentId":null,"name":"Managing observability","level":1,"index":10,"id":"managing-observability_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Enabling the observability stack","level":2,"index":0,"id":"enabling-the-observability-stack_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Collecting metrics from user workloads","level":2,"index":1,"id":"collecting-metrics-from-user-workloads_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Exporting metrics to external observability tools","level":2,"index":2,"id":"exporting-metrics-to-external-observability-tools_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Viewing traces in external tracing platforms","level":2,"index":3,"id":"viewing-traces-in-external-tracing-platforms_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Accessing built-in alerts","level":2,"index":4,"id":"accessing-built-in-alerts_managing-odh"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":11,"id":"viewing-logs-and-audit-records_managing-odh"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_managing-odh"},{"parentId":"configuring-the-operator-logger_managing-odh","name":"Viewing the Open Data Hub Operator logs","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_logs"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_managing-odh"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Selecting Open Data Hub administrator and user groups","level":1,"index":0,"id":"selecting-admin-and-user-groups_managing-resources"},{"parentId":null,"name":"Customizing the dashboard","level":1,"index":1,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration","level":2,"index":0,"id":"editing-the-dashboard-configuration_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":2,"id":"importing-a-custom-workbench-image_managing-resources"},{"parentId":null,"name":"Managing cluster PVC size","level":1,"index":3,"id":"managing-cluster-pvc-size"},{"parentId":"managing-cluster-pvc-size","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-pvc-size","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":null,"name":"Managing connection types","level":1,"index":4,"id":"managing-connection-types"},{"parentId":"managing-connection-types","name":"Viewing connection types","level":2,"index":0,"id":"viewing-connection-types_managing-resources"},{"parentId":"managing-connection-types","name":"Creating a connection type","level":2,"index":1,"id":"creating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Duplicating a connection type","level":2,"index":2,"id":"duplicating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Editing a connection type","level":2,"index":3,"id":"editing-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Enabling a connection type","level":2,"index":4,"id":"enabling-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Deleting a connection type","level":2,"index":5,"id":"deleting-a-connection-type_managing-resources"},{"parentId":null,"name":"Managing storage classes","level":1,"index":5,"id":"managing-storage-classes"},{"parentId":"managing-storage-classes","name":"About persistent storage","level":2,"index":0,"id":"about-persistent-storage_managing-resources"},{"parentId":"about-persistent-storage_managing-resources","name":"Storage classes in Open Data Hub","level":3,"index":0,"id":"_storage_classes_in_open_data_hub"},{"parentId":"about-persistent-storage_managing-resources","name":"Access modes","level":3,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":4,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":"managing-storage-classes","name":"Configuring storage class settings","level":2,"index":1,"id":"configuring-storage-class-settings_managing-resources"},{"parentId":"managing-storage-classes","name":"Configuring the default storage class for your cluster","level":2,"index":2,"id":"configuring-the-default-storage-class-for-your-cluster_managing-resources"},{"parentId":"managing-storage-classes","name":"Overview of object storage endpoints","level":2,"index":3,"id":"overview-of-object-storage-endpoints_managing-resources"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"MinIO (On-Cluster)","level":3,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Amazon S3","level":3,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Other S3-Compatible Object Stores","level":3,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Verification and Troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing basic workbenches","level":1,"index":6,"id":"managing-basic-workbenches"},{"parentId":"managing-basic-workbenches","name":"Accessing the administration interface for basic workbenches","level":2,"index":0,"id":"accessing-the-administration-interface-for-basic-workbenches_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Starting basic workbenches owned by other users","level":2,"index":1,"id":"starting-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Accessing basic workbenches owned by other users","level":2,"index":2,"id":"accessing-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Stopping basic workbenches owned by other users","level":2,"index":3,"id":"stopping-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Stopping idle workbenches","level":2,"index":4,"id":"stopping-idle-workbenches_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Adding workbench pod tolerations","level":2,"index":5,"id":"adding-workbench-pod-tolerations_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Troubleshooting common problems in workbenches for administrators","level":2,"index":6,"id":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":3,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"A user&#8217;s workbench does not start","level":3,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":3,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Overview of model monitoring","level":1,"index":0,"id":"overview-of-model-monitoring_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for your model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling TrustyAI Integration with KServe RawDeployment","level":2,"index":4,"id":"enabling-trustyai-kserve-integration_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Uploading training data to TrustyAI","level":2,"index":1,"id":"uploading-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":2,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":3,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Using bias metrics","level":2,"index":3,"id":"using-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":4,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing data drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using drift metrics","level":2,"index":3,"id":"using-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":5,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Using explainers","level":2,"index":2,"id":"using-explainers_explainers"},{"parentId":null,"name":"Evaluating large language models","level":1,"index":6,"id":"evaluating-large-language-models_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"Setting up LM-Eval","level":2,"index":0,"id":"setting-up-lmeval_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"Enabling external resource access for LMEval jobs","level":2,"index":1,"id":"enabling-external-resource-access-for-lmeval-jobs_monitor"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_monitor","name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":3,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_monitor"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_monitor","name":"Updating LMEval job configuration using the web console","level":3,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job","level":2,"index":2,"id":"lmeval-evaluation-job_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job properties","level":2,"index":3,"id":"lmeval-evaluation-job-properties_monitor"},{"parentId":"lmeval-evaluation-job-properties_monitor","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":3,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":"evaluating-large-language-models_monitor","name":"Performing model evaluations in the dashboard","level":2,"index":4,"id":"performing-model-evaluations-in-the-dashboard_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval scenarios","level":2,"index":5,"id":"lmeval-scenarios_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Accessing Hugging Face models with an environment variable token","level":3,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using a custom Unitxt card","level":3,"index":1,"id":"using-a-custom-unitxt-card_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using PVCs as storage","level":3,"index":2,"id":"using-pvcs-as-storage_monitor"},{"parentId":"using-pvcs-as-storage_monitor","name":"Managed PVCs","level":4,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_monitor","name":"Existing PVCs","level":4,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_monitor","name":"Using a KServe Inference Service","level":3,"index":3,"id":"using-a-kserve-inference-service_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Setting up LM-Eval S3 Support","level":3,"index":4,"id":"setting-up-lmeval-s3-support_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using LLM-as-a-Judge metrics with LM-Eval","level":3,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_monitor"},{"parentId":null,"name":"Configuring the Guardrails Orchestrator service","level":1,"index":7,"id":"configuring-the-guardrails-orchestrator-service_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Deploying the Guardrails Orchestrator service","level":2,"index":0,"id":"deploying-the-guardrails-orchestrator-service_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Auto-configuring Guardrails","level":2,"index":1,"id":"auto-configuring-guardrails_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Guardrails Orchestrator parameters","level":2,"index":2,"id":"guardrails-orchestrator-parameters_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Monitoring user inputs with the Guardrails Orchestrator service","level":2,"index":3,"id":"guardrails-orchestrator-hap-scenario_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Detectors","level":2,"index":4,"id":"guardrails-detectors_monitor"},{"parentId":"guardrails-detectors_monitor","name":"Configuring the Guardrails Detector Hugging Face serving runtime","level":3,"index":0,"id":"configuring-the-guardrails-detector-hugging-face-serving-runtime_monitor"},{"parentId":"guardrails-detectors_monitor","name":"Using a Hugging Face Prompt Injection detector with the Guardrails Orchestrator","level":3,"index":1,"id":"using-a-hugging-face-prompt-injection-detector-with-guardrails-orchestrator_monitor"},{"parentId":"guardrails-detectors_monitor","name":"Using Hugging Face models with Guardrails Orchestrator","level":3,"index":2,"id":"using-hugging-face-models-with-guardrails-orchestrator_monitor"},{"parentId":"guardrails-detectors_monitor","name":"Configuring the built-in detector and guardrails gateway","level":3,"index":3,"id":"configuring-the-built-in-detector-and-guardrails-gateway_monitor"},{"parentId":"guardrails-detectors_monitor","name":"Sending requests to the regex detector","level":3,"index":4,"id":"sending-requests-to-the-regex-detector_monitor"},{"parentId":"guardrails-detectors_monitor","name":"Querying using guardrails gateway","level":3,"index":5,"id":"querying-using-guardrails-gateway_monitor"},{"parentId":"configuring-the-guardrails-orchestrator-service_monitor","name":"Configuring the OpenTelemetry exporter","level":2,"index":5,"id":"configuring-the-opentelemetry-exporter_monitor"},{"parentId":null,"name":"Using llama stack with TrustyAI","level":1,"index":8,"id":"using-llama-stack-with-trustyai_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI","level":2,"index":0,"id":"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Running custom evaluations with LM-Eval and Llama Stack","level":2,"index":1,"id":"running-custom-evaluations-with-LMEval-and-llama-stack_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Using Guardrails Orchestrator with Llama Stack","level":2,"index":2,"id":"using-guardrails-orchestrator-with-llama-stack_monitor"},{"parentId":null,"name":"Bias monitoring tutorial - Gender bias example","level":1,"index":9,"id":"bias-monitoring-tutorial_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Introduction","level":2,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":3,"index":0,"id":"_about_the_example_models"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Setting up your environment","level":2,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":3,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":3,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":3,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":3,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":3,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":3,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Deploying models","level":2,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Sending training data to the models","level":2,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Labeling data fields","level":2,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Checking model fairness","level":2,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling a fairness metric request","level":2,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling an identity metric request","level":2,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Simulating real world data","level":2,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Reviewing the results","level":2,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":3,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":3,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2 or later","level":1,"index":1,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Requirements for upgrading Open Data Hub version 2","level":2,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Upgrading the Open Data Hub Operator","level":2,"index":1,"id":"upgrading-the-odh-operator_upgradev2"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Requirements for upgrading Open Data Hub version 1","level":2,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator","level":2,"index":1,"id":"upgrading-the-odh-operator_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":3,"id":"installing-odh-components_upgrade"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-in-your-data-science-ide/"},"sections":[{"parentId":null,"name":"Accessing your workbench IDE","level":1,"index":0,"id":"accessing-your-workbench-ide_ide"},{"parentId":null,"name":"Working in JupyterLab","level":1,"index":1,"id":"_working_in_jupyterlab"},{"parentId":"_working_in_jupyterlab","name":"Creating and importing Jupyter notebooks","level":2,"index":0,"id":"creating-and-importing-jupyter-notebooks_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Creating a Jupyter notebook","level":3,"index":0,"id":"creating-a-jupyter-notebook_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":3,"index":2,"id":"deleting-files-in-trash-directory_ide"},{"parentId":"deleting-files-in-trash-directory_ide","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":4,"index":0,"id":"emptying-trash-directory_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Additional resources","level":3,"index":3,"id":"_additional_resources"},{"parentId":"_working_in_jupyterlab","name":"Collaborating on Jupyter notebooks by using Git","level":2,"index":1,"id":"collaborating-on-jupyter-notebooks-by-using-git_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_ide"},{"parentId":"_working_in_jupyterlab","name":"Managing Python packages","level":2,"index":2,"id":"managing-python-packages_ide"},{"parentId":"managing-python-packages_ide","name":"Viewing Python packages installed on your workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_ide"},{"parentId":"managing-python-packages_ide","name":"Installing Python packages on your workbench","level":3,"index":1,"id":"installing-python-packages-on-your-workbench_ide"},{"parentId":"_working_in_jupyterlab","name":"Troubleshooting common problems in workbenches for users","level":2,"index":3,"id":"troubleshooting-common-problems-in-workbenches-for-users_ide"},{"parentId":null,"name":"Working in code-server","level":1,"index":2,"id":"_working_in_code_server"},{"parentId":"_working_in_code_server","name":"Creating code-server workbenches","level":2,"index":0,"id":"creating-code-server-workbenches_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Creating a workbench","level":3,"index":0,"id":"creating-a-project-workbench_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Uploading an existing notebook file to code-server from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_ide"},{"parentId":"_working_in_code_server","name":"Collaborating on workbenches in code-server by using Git","level":2,"index":1,"id":"collaborating-on-workbenches-in-code-server-by-using-git_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using code-server","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Updating your project in code-server with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Pushing project changes in code-server to a Git repository","level":3,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_ide"},{"parentId":"_working_in_code_server","name":"Managing Python packages in code-server","level":2,"index":2,"id":"managing-python-packages-in-code-server_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Viewing Python packages installed on your code-server workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Installing Python packages on your code-server workbench","level":3,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_ide"},{"parentId":"_working_in_code_server","name":"Installing extensions with code-server","level":2,"index":3,"id":"installing-extensions-with-code-server_ide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-data-science-projects/"},"sections":[{"parentId":null,"name":"Using data science projects","level":1,"index":0,"id":"using-data-science-projects_projects"},{"parentId":"using-data-science-projects_projects","name":"Creating a data science project","level":2,"index":0,"id":"creating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Updating a data science project","level":2,"index":1,"id":"updating-a-data-science-project_projects"},{"parentId":"using-data-science-projects_projects","name":"Deleting a data science project","level":2,"index":2,"id":"deleting-a-data-science-project_projects"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"using-project-workbenches_projects"},{"parentId":"using-project-workbenches_projects","name":"Creating a workbench and selecting an IDE","level":2,"index":0,"id":"creating-a-workbench-select-ide_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"About workbench images","level":3,"index":0,"id":"about-workbench-images_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"Creating a workbench","level":3,"index":1,"id":"creating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Deleting a workbench from a data science project","level":2,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_projects"},{"parentId":null,"name":"Using connections","level":1,"index":2,"id":"using-connections_projects"},{"parentId":"using-connections_projects","name":"Adding a connection to your data science project","level":2,"index":0,"id":"adding-a-connection-to-your-data-science-project_projects"},{"parentId":"using-connections_projects","name":"Updating a connection","level":2,"index":1,"id":"updating-a-connection_projects"},{"parentId":"using-connections_projects","name":"Deleting a connection","level":2,"index":2,"id":"deleting-a-connection_projects"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"configuring-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"About persistent storage","level":2,"index":0,"id":"about-persistent-storage_projects"},{"parentId":"about-persistent-storage_projects","name":"Storage classes in Open Data Hub","level":3,"index":0,"id":"_storage_classes_in_open_data_hub"},{"parentId":"about-persistent-storage_projects","name":"Access modes","level":3,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":4,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":"configuring-cluster-storage_projects","name":"Adding cluster storage to your data science project","level":2,"index":1,"id":"adding-cluster-storage-to-your-data-science-project_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Updating cluster storage","level":2,"index":2,"id":"updating-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Changing the storage class for an existing cluster storage instance","level":2,"index":3,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Deleting cluster storage from a data science project","level":2,"index":4,"id":"deleting-cluster-storage-from-a-data-science-project_projects"},{"parentId":null,"name":"Managing access to data science projects","level":1,"index":4,"id":"managing-access-to-data-science-projects_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Granting access to a data science project","level":2,"index":0,"id":"granting-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Updating access to a data science project","level":2,"index":1,"id":"updating-access-to-a-data-science-project_projects"},{"parentId":"managing-access-to-data-science-projects_projects","name":"Removing access to a data science project","level":2,"index":2,"id":"removing-access-to-a-data-science-project_projects"},{"parentId":null,"name":"Creating project-scoped resources for your project","level":1,"index":5,"id":"creating-project-scoped-resources-for-your-project_projects"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":1,"id":"enabling-accelerators_accelerators"},{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":2,"id":"enabling-nvidia-gpus_accelerators"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":3,"id":"intel-gaudi-ai-accelerator-integration_accelerators"},{"parentId":null,"name":"AMD GPU Integration","level":1,"index":4,"id":"amd-gpu-integration_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Verifying AMD GPU availability on your cluster","level":2,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Enabling AMD GPUs","level":2,"index":1,"id":"enabling-amd-gpus_accelerators"},{"parentId":null,"name":"IBM Spyre integration","level":1,"index":5,"id":"ibm-spyre-integration_accelerators"},{"parentId":null,"name":"Working with accelerator profiles","level":1,"index":6,"id":"working-with-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Viewing accelerator profiles","level":2,"index":0,"id":"viewing-accelerator-profiles_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Creating an accelerator profile","level":2,"index":1,"id":"creating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Updating an accelerator profile","level":2,"index":2,"id":"updating-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Deleting an accelerator profile","level":2,"index":3,"id":"deleting-an-accelerator-profile_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for workbench images","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-workbench-images_accelerators"},{"parentId":"working-with-accelerator-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":5,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":null,"name":"Working with hardware profiles","level":1,"index":7,"id":"working-with-hardware-profiles_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Creating a hardware profile","level":2,"index":0,"id":"creating-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Updating a hardware profile","level":2,"index":1,"id":"updating-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Deleting a hardware profile","level":2,"index":2,"id":"deleting-a-hardware-profile_accelerators"},{"parentId":null,"name":"About GPU time slicing","level":1,"index":8,"id":"about-gpu-time-slicing_accelerators"},{"parentId":null,"name":"Enabling GPU time slicing","level":1,"index":9,"id":"enabling-gpu-time-slicing_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-connected-applications/"},"sections":[{"parentId":null,"name":"Viewing applications that are connected to Open Data Hub","level":1,"index":0,"id":"viewing-connected-applications_connected-apps"},{"parentId":null,"name":"Enabling applications that are connected to Open Data Hub","level":1,"index":1,"id":"enabling-applications-connected_connected-apps"},{"parentId":null,"name":"Removing disabled applications from the dashboard","level":1,"index":2,"id":"removing-disabled-applications_connected-apps"},{"parentId":null,"name":"Using basic workbenches","level":1,"index":3,"id":"using-basic-workbenches_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Starting a basic workbench","level":2,"index":0,"id":"starting-a-basic-workbench_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Creating and importing Jupyter notebooks","level":2,"index":1,"id":"creating-and-importing-jupyter-notebooks_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Creating a Jupyter notebook","level":3,"index":0,"id":"creating-a-jupyter-notebook_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":3,"index":2,"id":"deleting-files-in-trash-directory_connected-apps"},{"parentId":"deleting-files-in-trash-directory_connected-apps","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":4,"index":0,"id":"emptying-trash-directory_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Additional resources","level":3,"index":3,"id":"_additional_resources"},{"parentId":"using-basic-workbenches_connected-apps","name":"Collaborating on Jupyter notebooks by using Git","level":2,"index":2,"id":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Managing Python packages","level":2,"index":3,"id":"managing-python-packages_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Viewing Python packages installed on your workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Installing Python packages on your workbench","level":3,"index":1,"id":"installing-python-packages-on-your-workbench_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Updating workbench settings by restarting your workbench","level":2,"index":4,"id":"updating-workbench-settings-by-restarting-your-workbench_connected-apps"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":1,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Managing data science pipelines","level":1,"index":0,"id":"managing-data-science-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ds-pipelines"},{"parentId":"configuring-a-pipeline-server_ds-pipelines","name":"Configuring a pipeline server with an external Amazon RDS database","level":3,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ds-pipelines"},{"parentId":"defining-a-pipeline_ds-pipelines","name":"Compiling the pipeline YAML with the Kubeflow Pipelines SDK","level":3,"index":0,"id":"compiling-the-pipeline-yaml-with-kfp-sdk_ds-pipelines"},{"parentId":"defining-a-pipeline_ds-pipelines","name":"Compiling Kubernetes-native manifests with the Kubeflow Pipelines SDK","level":3,"index":1,"id":"compiling-kubernetes-native-manifests-with-kfp-sdk_ds-pipelines"},{"parentId":"defining-a-pipeline_ds-pipelines","name":"Defining a pipeline by using the Kubernetes API","level":3,"index":2,"id":"defining-a-pipeline-by-using-the-kubernetes-api_ds-pipelines"},{"parentId":"defining-a-pipeline_ds-pipelines","name":"Migrating pipelines from database to Kubernetes API storage","level":3,"index":3,"id":"migrating-pipelines-from-database-to-kubernetes-api_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Importing a data science pipeline","level":2,"index":2,"id":"importing-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a data science pipeline","level":2,"index":3,"id":"deleting-a-data-science-pipeline_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline server","level":2,"index":4,"id":"deleting-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline server","level":2,"index":5,"id":"viewing-the-details-of-a-pipeline-server_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing existing pipelines","level":2,"index":6,"id":"viewing-existing-pipelines_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of pipeline versions","level":2,"index":7,"id":"overview-of-pipeline-versions_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Uploading a pipeline version","level":2,"index":8,"id":"uploading-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Deleting a pipeline version","level":2,"index":9,"id":"deleting-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Viewing the details of a pipeline version","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Downloading a data science pipeline version","level":2,"index":11,"id":"downloading-a-data-science-pipeline-version_ds-pipelines"},{"parentId":"managing-data-science-pipelines_ds-pipelines","name":"Overview of data science pipelines caching","level":2,"index":12,"id":"overview-of-data-science-pipelines-caching_ds-pipelines"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Caching criteria","level":3,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Viewing cached steps in the Open Data Hub user interface","level":3,"index":1,"id":"_viewing_cached_steps_in_the_open_data_hub_user_interface"},{"parentId":"overview-of-data-science-pipelines-caching_ds-pipelines","name":"Controlling caching in data science pipelines","level":3,"index":2,"id":"controlling-caching-in-data-science-pipelines_ds-pipelines"},{"parentId":"controlling-caching-in-data-science-pipelines_ds-pipelines","name":"Disabling caching for individual tasks","level":4,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"controlling-caching-in-data-science-pipelines_ds-pipelines","name":"Disabling caching for a pipeline at submit time","level":4,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":"controlling-caching-in-data-science-pipelines_ds-pipelines","name":"Disabling caching for a pipeline at compile time","level":4,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":"controlling-caching-in-data-science-pipelines_ds-pipelines","name":"Disabling caching for all pipelines (pipeline server)","level":4,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"},{"parentId":null,"name":"Managing pipeline experiments","level":1,"index":1,"id":"managing-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Overview of pipeline experiments","level":2,"index":0,"id":"overview-of-pipeline-experiments_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Creating a pipeline experiment","level":2,"index":1,"id":"creating-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Archiving a pipeline experiment","level":2,"index":2,"id":"archiving-a-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Deleting an archived pipeline experiment","level":2,"index":3,"id":"deleting-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Restoring an archived pipeline experiment","level":2,"index":4,"id":"restoring-an-archived-pipeline-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline task executions","level":2,"index":5,"id":"viewing-pipeline-task-executions_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Viewing pipeline artifacts","level":2,"index":6,"id":"viewing-pipeline-artifacts_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs in an experiment","level":2,"index":7,"id":"comparing-runs-in-an-experiment_ds-pipelines"},{"parentId":"managing-pipeline-experiments_ds-pipelines","name":"Comparing runs in different experiments","level":2,"index":8,"id":"comparing-runs-in-different-experiments_ds-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":2,"id":"managing-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Storing data with data science pipelines","level":2,"index":1,"id":"storing-data-with-data-science-pipelines_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing active pipeline runs","level":2,"index":2,"id":"viewing-active-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Executing a pipeline run","level":2,"index":3,"id":"executing-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Stopping an active pipeline run","level":2,"index":4,"id":"stopping-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an active pipeline run","level":2,"index":5,"id":"duplicating-an-active-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing scheduled pipeline runs","level":2,"index":6,"id":"viewing-scheduled-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run using a cron job","level":2,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Scheduling a pipeline run","level":2,"index":8,"id":"scheduling-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating a scheduled pipeline run","level":2,"index":9,"id":"duplicating-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting a scheduled pipeline run","level":2,"index":10,"id":"deleting-a-scheduled-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing the details of a pipeline run","level":2,"index":11,"id":"viewing-the-details-of-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Viewing archived pipeline runs","level":2,"index":12,"id":"viewing-archived-pipeline-runs_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Archiving a pipeline run","level":2,"index":13,"id":"archiving-a-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Restoring an archived pipeline run","level":2,"index":14,"id":"restoring-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Deleting an archived pipeline run","level":2,"index":15,"id":"deleting-an-archived-pipeline-run_ds-pipelines"},{"parentId":"managing-pipeline-runs_ds-pipelines","name":"Duplicating an archived pipeline run","level":2,"index":16,"id":"duplicating-an-archived-pipeline-run_ds-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":3,"id":"working-with-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ds-pipelines"},{"parentId":"working-with-pipeline-logs_ds-pipelines","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ds-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":4,"id":"working-with-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Disabling node caching in Elyra","level":2,"index":2,"id":"disabling-node-caching-in-elyra_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Creating a runtime configuration","level":2,"index":3,"id":"creating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Updating a runtime configuration","level":2,"index":4,"id":"updating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Deleting a runtime configuration","level":2,"index":5,"id":"deleting-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Duplicating a runtime configuration","level":2,"index":6,"id":"duplicating-a-runtime-configuration_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Running a pipeline in JupyterLab","level":2,"index":7,"id":"running-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ds-pipelines","name":"Exporting a pipeline in JupyterLab","level":2,"index":8,"id":"exporting-a-pipeline-in-jupyterlab_ds-pipelines"},{"parentId":null,"name":"Troubleshooting DSPA component errors","level":1,"index":5,"id":"troubleshooting-dspa-component-errors_ds-pipelines"},{"parentId":"troubleshooting-dspa-component-errors_ds-pipelines","name":"Common errors across DSP components","level":2,"index":0,"id":"_common_errors_across_dsp_components"},{"parentId":null,"name":"Additional resources","level":1,"index":6,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Distributed workloads infrastructure","level":2,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Types of distributed workloads","level":2,"index":1,"id":"_types_of_distributed_workloads"},{"parentId":null,"name":"Preparing the distributed training environment","level":1,"index":1,"id":"preparing-the-distributed-training-environment_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Creating a workbench for distributed training","level":2,"index":0,"id":"creating-a-workbench-for-distributed-training_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Using the cluster server and token to authenticate","level":2,"index":1,"id":"using-the-cluster-server-and-token-to-authenticate_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Managing custom training images","level":2,"index":2,"id":"managing-custom-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"About base training images","level":3,"index":0,"id":"about-base-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Creating a custom training image","level":3,"index":1,"id":"creating-a-custom-training-image_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Pushing an image to the integrated OpenShift image registry","level":3,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_distributed-workloads"},{"parentId":null,"name":"Running Ray-based distributed workloads","level":1,"index":2,"id":"running-ray-based-distributed-workloads_distributed-workloads"},{"parentId":"running-ray-based-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from Jupyter notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Downloading the demo Jupyter notebooks from the CodeFlare SDK","level":3,"index":0,"id":"downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Running the demo Jupyter notebooks from the CodeFlare SDK","level":3,"index":1,"id":"running-the-demo-jupyter-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Managing Ray clusters from within a Jupyter notebook","level":3,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_distributed-workloads"},{"parentId":"running-ray-based-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from data science pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads"},{"parentId":null,"name":"Running Training Operator-based distributed training workloads","level":1,"index":3,"id":"running-kfto-based-distributed-training-workloads_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Using the Kubeflow Training Operator to run distributed training workloads","level":2,"index":0,"id":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":3,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorchJob resource","level":3,"index":1,"id":"creating-a-kfto-pytorchjob-resource_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":3,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorch training scripts","level":3,"index":3,"id":"example-kfto-pytorch-training-scripts_distributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: NCCL","level":4,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccldistributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: DDP","level":4,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_distributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: FSDP","level":4,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Dockerfile for a Training Operator PyTorch training script","level":3,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorchJob resource for multi-node training","level":3,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Using the Training Operator SDK to run distributed training workloads","level":2,"index":1,"id":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"Configuring a training job by using the Training Operator SDK","level":3,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"Running a training job by using the Training Operator SDK","level":3,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"TrainingClient API: Job-related methods","level":3,"index":2,"id":"ref-trainingclient-api-job-related-methods_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Fine-tuning a model by using Kubeflow Training","level":2,"index":2,"id":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Configuring the fine-tuning job","level":3,"index":0,"id":"configuring-the-fine-tuning-job_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Running the fine-tuning job","level":3,"index":1,"id":"running-the-fine-tuning-job_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Deleting the fine-tuning job","level":3,"index":2,"id":"deleting-the-fine-tuning-job_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Creating a multi-node PyTorch training job with RDMA","level":2,"index":3,"id":"creating-a-multi-node-pytorch-training-job-with-rdma_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorchJob resource configured to run with RDMA","level":2,"index":4,"id":"ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":4,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing Kueue alerts for distributed workloads","level":2,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for users","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a suspended state","level":2,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a failed state","level":2,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"failed to call webhook\" error message for the CodeFlare Operator","level":2,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"failed to call webhook\" error message for Kueue","level":2,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster does not start","level":2,"index":4,"id":"_my_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"Default Local Queue not found\" error message","level":2,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"local_queue provided does not exist\" error message","level":2,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I cannot create a Ray cluster or submit jobs","level":2,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My pod provisioned by Kueue is terminated before my image is pulled","level":2,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"Additional resources","level":2,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-llama-stack/"},"sections":[{"parentId":null,"name":"Overview of Llama Stack","level":1,"index":0,"id":"overview-of-llama-stack_rag"},{"parentId":"overview-of-llama-stack_rag","name":"The <code>LlamaStackDistribution</code> custom resource API providers","level":2,"index":0,"id":"_the_llamastackdistribution_custom_resource_api_providers"},{"parentId":"overview-of-llama-stack_rag","name":"OpenAI compatibility for RAG APIs in Llama Stack","level":2,"index":1,"id":"openai-compatibility-for-rag-apis-in-llama-stack_rag"},{"parentId":"overview-of-llama-stack_rag","name":"OpenAI-compatible APIs in Llama Stack","level":2,"index":2,"id":"openai-compatible-apis-in-Llama-Stack_rag"},{"parentId":"openai-compatible-apis-in-Llama-Stack_rag","name":"Supported OpenAI-compatible APIs in Open Data Hub","level":3,"index":0,"id":"_supported_openai_compatible_apis_in_open_data_hub"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Chat Completions API","level":4,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Completions API","level":4,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Embeddings API","level":4,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Files API","level":4,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Vector Stores API","level":4,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Vector Store Files API","level":4,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Models API","level":4,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Responses API","level":4,"index":7,"id":"_responses_api"},{"parentId":null,"name":"Activating the Llama Stack Operator","level":1,"index":1,"id":"activating-the-llama-stack-operator_rag"},{"parentId":null,"name":"Deploying a RAG stack in a data science project","level":1,"index":2,"id":"deploying-a-rag-stack-in-a-data-science-project_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Overview of RAG","level":2,"index":0,"id":"overview-of-rag_rag"},{"parentId":"overview-of-rag_rag","name":"Audience for RAG","level":3,"index":0,"id":"_audience_for_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Overview of vector databases","level":2,"index":1,"id":"overview-of-vector-databases_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Overview of Milvus vector databases","level":2,"index":2,"id":"overview-of-milvus-vector-databases_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Deploying a Llama model with KServe","level":2,"index":3,"id":"Deploying-a-llama-model-with-kserve_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Testing your vLLM model endpoints","level":2,"index":4,"id":"testing-your-vllm-model-endpoints_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Deploying a remote Milvus vector database","level":2,"index":5,"id":"deploying-a-remote-milvus-vector-database_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Deploying a LlamaStackDistribution instance","level":2,"index":6,"id":"deploying-a-llamastackdistribution-instance_rag"},{"parentId":"deploying-a-llamastackdistribution-instance_rag","name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":3,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_rag","name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":3,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Ingesting content into a Llama model","level":2,"index":7,"id":"ingesting-content-into-a-llama-model_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Querying ingested content in a Llama model","level":2,"index":8,"id":"querying-ingested-content-in-a-llama-model_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"Preparing documents with Docling for Llama Stack retrieval","level":2,"index":9,"id":"preparing-documents-with-docling-for-llama-stack-retrieval_rag"},{"parentId":"deploying-a-rag-stack-in-a-data-science-project_rag","name":"About Llama stack search types","level":2,"index":10,"id":"about-llama-stack-search-types_rag"},{"parentId":"about-llama-stack-search-types_rag","name":"Supported search modes","level":3,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":4,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":4,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":4,"index":2,"id":"_hybrid_search"},{"parentId":"about-llama-stack-search-types_rag","name":"Retrieval database support","level":3,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-machine-learning-features/"},"sections":[{"parentId":null,"name":"Overview of machine learning features and Feature Store","level":1,"index":0,"id":"overview-of-ml-features-and-feature-store.adoc_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Overview of machine learning features","level":2,"index":0,"id":"overview-of-machine-learning-features_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Overview of Feature Store","level":2,"index":1,"id":"overview-of-feature-store_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Audience for Feature Store","level":2,"index":2,"id":"audience-for-feature-store_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Feature Store workflow","level":2,"index":3,"id":"feature-store-workflow_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Setting up the Feature Store user interface for initial use","level":2,"index":4,"id":"setting-up-feature-store-UI_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Additional resources","level":2,"index":5,"id":"_additional_resources"},{"parentId":null,"name":"Configuring Feature Store","level":1,"index":1,"id":"_configuring_feature_store"},{"parentId":"_configuring_feature_store","name":"Setting up Feature Store","level":2,"index":0,"id":"setting-up-feature-store_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Before you begin","level":3,"index":0,"id":"before-you-begin_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Enabling the Feature Store component","level":3,"index":1,"id":"enabling-the-feature-store-component_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Creating a Feature Store instance in a data science project","level":3,"index":2,"id":"creating-a-feature-store-instance-in-a-data-science-project_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Adding feature definitions and initializing your Feature Store instance","level":3,"index":3,"id":"adding-feature-definitions-and-initializing-your-feature-store-instance_featurestore"},{"parentId":"adding-feature-definitions-and-initializing-your-feature-store-instance_featurestore","name":"Specifying files to ignore","level":4,"index":0,"id":"specifying-files-to-ignore_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Viewing Feature Store objects in the web-based UI","level":3,"index":4,"id":"viewing-feature-store-objects-in-the-web-based-ui_featurestore"},{"parentId":"_configuring_feature_store","name":"Customizing your Feature Store configuration","level":2,"index":1,"id":"customizing-your-feature-store-configuration_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring an offline store","level":3,"index":0,"id":"configuring-an-offline-store_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring an online store","level":3,"index":1,"id":"configuring-an-online-store_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring the feature registry","level":3,"index":2,"id":"configuring-the-feature-registry_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Example PVC configuration","level":3,"index":3,"id":"ref-example-pvc-configuration_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Editing an existing Feature Store instance","level":3,"index":4,"id":"editing-an-existing-feature-store-instance_featurestore"},{"parentId":null,"name":"Defining machine learning features","level":1,"index":2,"id":"defining-ml-features_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Setting up your working environment","level":2,"index":0,"id":"setting-up-your-working-environment_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"About feature definitions","level":2,"index":1,"id":"about-feature-definitions_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Specifying the data source for features","level":2,"index":2,"id":"specifying-the-data-source-for-features_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"About organizing features by using entities","level":2,"index":3,"id":"about-organizing-features-by-using-entities_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Creating feature views","level":2,"index":4,"id":"creating-feature-views_featurestore"},{"parentId":null,"name":"Retrieving features for model training","level":1,"index":3,"id":"retrieving-features-for-model-training_featurestore"},{"parentId":"retrieving-features-for-model-training_featurestore","name":"Making features available for real-time inference","level":2,"index":0,"id":"making-features-available-for-real-time-inference_featurestore"},{"parentId":"retrieving-features-for-model-training_featurestore","name":"Retrieving online features for model inference","level":2,"index":1,"id":"retrieving-online-features-for-model-inference_featurestore"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Overview of model registries and model catalog","level":1,"index":0,"id":"overview-of-model-registries_model-registry"},{"parentId":"overview-of-model-registries_model-registry","name":"Model registry","level":2,"index":0,"id":"_model_registry"},{"parentId":"overview-of-model-registries_model-registry","name":"Model catalog","level":2,"index":1,"id":"_model_catalog"},{"parentId":null,"name":"Enabling the model registry component","level":0,"index":1,"id":"_enabling_the_model_registry_component"},{"parentId":"_enabling_the_model_registry_component","name":"Enabling the model registry component","level":1,"index":0,"id":"enabling-the-model-registry-component_model-registry"},{"parentId":null,"name":"Managing model registries","level":0,"index":2,"id":"_managing_model_registries"},{"parentId":"_managing_model_registries","name":"Creating a model registry","level":1,"index":0,"id":"creating-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Editing a model registry","level":1,"index":1,"id":"editing-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Managing model registry permissions","level":1,"index":2,"id":"managing-model-registry-permissions_model-registry"},{"parentId":"_managing_model_registries","name":"Deleting a model registry","level":1,"index":3,"id":"deleting-a-model-registry_model-registry"},{"parentId":null,"name":"Working with model registries","level":0,"index":3,"id":"_working_with_model_registries"},{"parentId":"_working_with_model_registries","name":"Working with model registries","level":1,"index":0,"id":"working-with-model-registries_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model from the dashboard","level":2,"index":0,"id":"registering-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model version","level":2,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered models","level":2,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered model versions","level":2,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model metadata in a model registry","level":2,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model version metadata in a model registry","level":2,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deploying a model version from a model registry","level":2,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing the deployment properties of a deployed model version from a model registry","level":2,"index":7,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the multi-model serving platform","level":3,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform_model-registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the single-model serving platform","level":3,"index":1,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deleting a deployed model version from a model registry","level":2,"index":8,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model","level":2,"index":9,"id":"archiving-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model version","level":2,"index":10,"id":"archiving-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model","level":2,"index":11,"id":"restoring-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model version","level":2,"index":12,"id":"restoring-a-model-version_model-registry"},{"parentId":null,"name":"Working with the model catalog","level":0,"index":4,"id":"_working_with_the_model_catalog"},{"parentId":"_working_with_the_model_catalog","name":"Working with the model catalog","level":1,"index":0,"id":"working-with-the-model-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Viewing models in the model catalog","level":2,"index":0,"id":"viewing-models-in-the-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Registering a model from the model catalog","level":2,"index":1,"id":"registering-a-model-from-the-model-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Deploying a model from the model catalog","level":2,"index":2,"id":"deploying-a-model-from-the-model-catalog_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/backing-up-data/"},"sections":[{"parentId":null,"name":"Backing up storage data","level":1,"index":0,"id":"backing-up-storage-data_data-mgmt"},{"parentId":null,"name":"Backing up your cluster","level":1,"index":1,"id":"backing-up-your-cluster_data-mgmt"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/bias-monitoring-tutorial/"},"sections":[{"parentId":null,"name":"Introduction","level":1,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":2,"index":0,"id":"_about_the_example_models"},{"parentId":null,"name":"Setting up your environment","level":1,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":2,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":2,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":2,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":2,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":2,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":2,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":null,"name":"Deploying models","level":1,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":null,"name":"Sending training data to the models","level":1,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":null,"name":"Labeling data fields","level":1,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":null,"name":"Checking model fairness","level":1,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":null,"name":"Scheduling a fairness metric request","level":1,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":null,"name":"Scheduling an identity metric request","level":1,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":null,"name":"Simulating real world data","level":1,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":null,"name":"Reviewing the results","level":1,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":2,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":2,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-jupyter-notebooks-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-workbenches-in-code-server-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using code-server","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project in code-server with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes in code-server to a Git repository","level":1,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-cluster-storage/"},"sections":[{"parentId":null,"name":"About persistent storage","level":1,"index":0,"id":"about-persistent-storage_{context}"},{"parentId":"about-persistent-storage_{context}","name":"Storage classes in {productname-short}","level":2,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":"about-persistent-storage_{context}","name":"Access modes","level":2,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":3,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":null,"name":"Adding cluster storage to your data science project","level":1,"index":1,"id":"adding-cluster-storage-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating cluster storage","level":1,"index":2,"id":"updating-cluster-storage_{context}"},{"parentId":null,"name":"Changing the storage class for an existing cluster storage instance","level":1,"index":3,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_{context}"},{"parentId":null,"name":"Deleting cluster storage from a data science project","level":1,"index":4,"id":"deleting-cluster-storage-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-feature-store-role-based-access-control/"},"sections":[{"parentId":null,"name":"Configuring role-based access control","level":1,"index":0,"id":"configuring-role-based-access-control_{context}"},{"parentId":null,"name":"Default authorization configuration","level":1,"index":1,"id":"ref-default-authorization-configuration_{context}"},{"parentId":null,"name":"Example OIDC Authorization configuration","level":1,"index":2,"id":"ref-example-oidc-authorization-configuration_{context}"},{"parentId":null,"name":"Example Kubernetes Authorization configuration","level":1,"index":3,"id":"ref-example-kubernetes-authorization-configuration_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-the-guardrails-orchestrator-service/"},"sections":[{"parentId":null,"name":"Deploying the Guardrails Orchestrator service","level":1,"index":0,"id":"deploying-the-guardrails-orchestrator-service_{context}"},{"parentId":null,"name":"Auto-configuring Guardrails","level":1,"index":1,"id":"auto-configuring-guardrails_{context}"},{"parentId":null,"name":"Guardrails Orchestrator parameters","level":1,"index":2,"id":"guardrails-orchestrator-parameters_{context}"},{"parentId":null,"name":"Monitoring user inputs with the Guardrails Orchestrator service","level":1,"index":3,"id":"guardrails-orchestrator-hap-scenario_{context}"},{"parentId":null,"name":"Detectors","level":1,"index":4,"id":"guardrails-detectors_{context}"},{"parentId":"guardrails-detectors_{context}","name":"Configuring the Guardrails Detector Hugging Face serving runtime","level":2,"index":0,"id":"configuring-the-guardrails-detector-hugging-face-serving-runtime_{context}"},{"parentId":"guardrails-detectors_{context}","name":"Using a Hugging Face Prompt Injection detector with the Guardrails Orchestrator","level":2,"index":1,"id":"using-a-hugging-face-prompt-injection-detector-with-guardrails-orchestrator_{context}"},{"parentId":"guardrails-detectors_{context}","name":"Using Hugging Face models with Guardrails Orchestrator","level":2,"index":2,"id":"using-hugging-face-models-with-guardrails-orchestrator_{context}"},{"parentId":"guardrails-detectors_{context}","name":"Configuring the built-in detector and guardrails gateway","level":2,"index":3,"id":"configuring-the-built-in-detector-and-guardrails-gateway_{context}"},{"parentId":"guardrails-detectors_{context}","name":"Sending requests to the regex detector","level":2,"index":4,"id":"sending-requests-to-the-regex-detector_{context}"},{"parentId":"guardrails-detectors_{context}","name":"Querying using guardrails gateway","level":2,"index":5,"id":"querying-using-guardrails-gateway_{context}"},{"parentId":null,"name":"Configuring the OpenTelemetry exporter","level":1,"index":5,"id":"configuring-the-opentelemetry-exporter_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-trustyai/"},"sections":[{"parentId":null,"name":"Configuring monitoring for your model serving platform","level":1,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_{context}"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":1,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Configuring TrustyAI with a database","level":1,"index":2,"id":"configuring-trustyai-with-a-database_{context}"},{"parentId":null,"name":"Installing the TrustyAI service for a project","level":1,"index":3,"id":"installing-trustyai-service_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the dashboard","level":2,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the CLI","level":2,"index":1,"id":"installing-trustyai-service-using-cli_{context}"},{"parentId":null,"name":"Enabling TrustyAI Integration with KServe RawDeployment","level":1,"index":4,"id":"enabling-trustyai-kserve-integration_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-your-model-serving-platform/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_odh-admin"},{"parentId":"about-model-serving_odh-admin","name":"Single-model serving platform","level":2,"index":0,"id":"_single_model_serving_platform"},{"parentId":"about-model-serving_odh-admin","name":"Multi-model serving platform","level":2,"index":1,"id":"_multi_model_serving_platform"},{"parentId":"about-model-serving_odh-admin","name":"NVIDIA NIM model serving platform","level":2,"index":2,"id":"_nvidia_nim_model_serving_platform"},{"parentId":null,"name":"Model-serving runtimes","level":1,"index":1,"id":"model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes_odh-admin","name":"ServingRuntime","level":2,"index":0,"id":"_servingruntime"},{"parentId":"model-serving-runtimes_odh-admin","name":"InferenceService","level":2,"index":1,"id":"_inferenceservice"},{"parentId":null,"name":"Model-serving runtimes for accelerators","level":1,"index":2,"id":"model-serving-runtimes-for-accelerators_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"NVIDIA GPUs","level":2,"index":0,"id":"_nvidia_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Intel Gaudi accelerators","level":2,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"AMD GPUs","level":2,"index":2,"id":"_amd_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"IBM Spyre AI accelerators on x86","level":2,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Supported model-serving runtimes","level":2,"index":4,"id":"supported-model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Tested and verified model-serving runtimes","level":2,"index":5,"id":"tested-verified-runtimes_odh-admin"},{"parentId":null,"name":"Configuring model servers on the single-model serving platform","level":0,"index":3,"id":"_configuring_model_servers_on_the_single_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_single_model_serving_platform","name":"Enabling the single-model serving platform","level":1,"index":0,"id":"enabling-the-single-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers_on_the_single_model_serving_platform","name":"Enabling speculative decoding and multi-modal inferencing","level":1,"index":1,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_odh-admin"},{"parentId":"_configuring_model_servers_on_the_single_model_serving_platform","name":"Adding a custom model-serving runtime for the single-model serving platform","level":1,"index":2,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers_on_the_single_model_serving_platform","name":"Adding a tested and verified model-serving runtime for the single-model serving platform","level":1,"index":3,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-single-model-serving-platform_odh-admin"},{"parentId":null,"name":"Configuring model servers on the NVIDIA NIM model serving platform","level":0,"index":4,"id":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform","name":"Enabling the NVIDIA NIM model serving platform","level":1,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_odh-admin"},{"parentId":null,"name":"Configuring model servers on the multi-model serving platform","level":0,"index":5,"id":"_configuring_model_servers_on_the_multi_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_multi_model_serving_platform","name":"Enabling the multi-model serving platform","level":1,"index":0,"id":"enabling-the-multi-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers_on_the_multi_model_serving_platform","name":"Adding a custom model-serving runtime for the multi-model serving platform","level":1,"index":1,"id":"adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers_on_the_multi_model_serving_platform","name":"Adding a tested and verified model-serving runtime for the multi-model serving platform","level":1,"index":2,"id":"adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_odh-admin"},{"parentId":null,"name":"Customizing model deployments","level":0,"index":6,"id":"_customizing_model_deployments"},{"parentId":"_customizing_model_deployments","name":"Customizing the parameters of a deployed model-serving runtime","level":1,"index":0,"id":"customizing-parameters-serving-runtime_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizable model serving runtime parameters","level":1,"index":1,"id":"customizable-model-serving-runtime-parameters_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizing the vLLM model-serving runtime","level":1,"index":2,"id":"Customizing-the-vllm-runtime_odh-admin"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-jupyter-notebooks/"},"sections":[{"parentId":null,"name":"Creating a Jupyter notebook","level":1,"index":0,"id":"creating-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_{context}"},{"parentId":null,"name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":1,"index":2,"id":"deleting-files-in-trash-directory_{context}"},{"parentId":"deleting-files-in-trash-directory_{context}","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":2,"index":0,"id":"emptying-trash-directory_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":3,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-code-server-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench","level":1,"index":0,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-custom-workbench-images/"},"sections":[{"parentId":null,"name":"Creating a custom image from a default {productname-short} image","level":1,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":null,"name":"Creating a custom image from your own image","level":1,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":2,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":2,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Enabling custom images in {productname-short}","level":1,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":3,"id":"importing-a-custom-workbench-image_custom-images"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-component-deployment-resources/"},"sections":[{"parentId":null,"name":"Overview of component resource customization","level":1,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":null,"name":"Customizing component resources","level":1,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":null,"name":"Disabling component resource customization","level":1,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Re-enabling component resource customization","level":1,"index":3,"id":"reenabling-component-resource-customization_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration","level":1,"index":0,"id":"editing-the-dashboard-configuration_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-your-feature-store-configuration/"},"sections":[{"parentId":null,"name":"Configuring an offline store","level":1,"index":0,"id":"configuring-an-offline-store_{context}"},{"parentId":null,"name":"Configuring an online store","level":1,"index":1,"id":"configuring-an-online-store_{context}"},{"parentId":null,"name":"Configuring the feature registry","level":1,"index":2,"id":"configuring-the-feature-registry_{context}"},{"parentId":null,"name":"Example PVC configuration","level":1,"index":3,"id":"ref-example-pvc-configuration_{context}"},{"parentId":null,"name":"Editing an existing Feature Store instance","level":1,"index":4,"id":"editing-an-existing-feature-store-instance_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/defining-ml-features/"},"sections":[{"parentId":null,"name":"Setting up your working environment","level":1,"index":0,"id":"setting-up-your-working-environment_{context}"},{"parentId":null,"name":"About feature definitions","level":1,"index":1,"id":"about-feature-definitions_{context}"},{"parentId":null,"name":"Specifying the data source for features","level":1,"index":2,"id":"specifying-the-data-source-for-features_{context}"},{"parentId":null,"name":"About organizing features by using entities","level":1,"index":3,"id":"about-organizing-features-by-using-entities_{context}"},{"parentId":null,"name":"Creating feature views","level":1,"index":4,"id":"creating-feature-views_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deploying-a-rag-stack-in-a-data-science-project/"},"sections":[{"parentId":null,"name":"Overview of RAG","level":1,"index":0,"id":"overview-of-rag_{context}"},{"parentId":"overview-of-rag_{context}","name":"Audience for RAG","level":2,"index":0,"id":"_audience_for_rag"},{"parentId":null,"name":"Overview of vector databases","level":1,"index":1,"id":"overview-of-vector-databases_{context}"},{"parentId":null,"name":"Overview of Milvus vector databases","level":1,"index":2,"id":"overview-of-milvus-vector-databases_{context}"},{"parentId":null,"name":"Deploying a Llama model with KServe","level":1,"index":3,"id":"Deploying-a-llama-model-with-kserve_{context}"},{"parentId":null,"name":"Testing your vLLM model endpoints","level":1,"index":4,"id":"testing-your-vllm-model-endpoints_{context}"},{"parentId":null,"name":"Deploying a remote Milvus vector database","level":1,"index":5,"id":"deploying-a-remote-milvus-vector-database_{context}"},{"parentId":null,"name":"Deploying a LlamaStackDistribution instance","level":1,"index":6,"id":"deploying-a-llamastackdistribution-instance_{context}"},{"parentId":"deploying-a-llamastackdistribution-instance_{context}","name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":2,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_{context}","name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":2,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":null,"name":"Ingesting content into a Llama model","level":1,"index":7,"id":"ingesting-content-into-a-llama-model_{context}"},{"parentId":null,"name":"Querying ingested content in a Llama model","level":1,"index":8,"id":"querying-ingested-content-in-a-llama-model_{context}"},{"parentId":null,"name":"Preparing documents with Docling for Llama Stack retrieval","level":1,"index":9,"id":"preparing-documents-with-docling-for-llama-stack-retrieval_{context}"},{"parentId":null,"name":"About Llama stack search types","level":1,"index":10,"id":"about-llama-stack-search-types_{context}"},{"parentId":"about-llama-stack-search-types_{context}","name":"Supported search modes","level":2,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":3,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":3,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":3,"index":2,"id":"_hybrid_search"},{"parentId":"about-llama-stack-search-types_{context}","name":"Retrieval database support","level":2,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deploying-models/"},"sections":[{"parentId":null,"name":"Using OCI containers for model storage","level":1,"index":0,"id":"using-oci-containers-for-model-storage_odh-user"},{"parentId":null,"name":"Storing a model in an OCI image","level":1,"index":1,"id":"storing-a-model-in-oci-image_odh-user"},{"parentId":null,"name":"Uploading model files to a Persistent Volume Claim (PVC)","level":1,"index":2,"id":"uploading-model-files-to-pvc_odh-user"},{"parentId":null,"name":"Deploying models on the single-model serving platform","level":0,"index":3,"id":"_deploying_models_on_the_single_model_serving_platform"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"About KServe deployment modes","level":1,"index":0,"id":"about-kserve-deployment-modes_odh-user"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"Deploying models on the single-model serving platform","level":1,"index":1,"id":"deploying-models-on-the-single-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"Deploying a model stored in an OCI image by using the CLI","level":1,"index":2,"id":"deploying-model-stored-in-oci-image_odh-user"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"Deploying models by using {llmd}","level":1,"index":3,"id":"deploying-models-using-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Example usage for {llmd}","level":2,"index":0,"id":"ref-example-distributed-inference_odh-user"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Single-node GPU deployment","level":3,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Multi-node deployment","level":3,"index":1,"id":"_multi_node_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Intelligent inference scheduler with KV cache routing","level":3,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"},{"parentId":"_deploying_models_on_the_single_model_serving_platform","name":"Monitoring models on the single-model serving platform","level":1,"index":4,"id":"_monitoring_models_on_the_single_model_serving_platform"},{"parentId":"_monitoring_models_on_the_single_model_serving_platform","name":"Viewing performance metrics for a deployed model","level":2,"index":0,"id":"viewing-performance-metrics-for-deployed-model_odh-user"},{"parentId":"_monitoring_models_on_the_single_model_serving_platform","name":"Viewing model-serving runtime metrics for the single-model serving platform","level":2,"index":1,"id":"viewing-metrics-for-the-single-model-serving-platform_odh-user"},{"parentId":null,"name":"Deploying models on the NVIDIA NIM model serving platform","level":0,"index":4,"id":"_deploying_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Deploying models on the NVIDIA NIM model serving platform","level":1,"index":0,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing NVIDIA NIM metrics for a NIM model","level":1,"index":1,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing performance metrics for a NIM model","level":1,"index":2,"id":"viewing-performance-metrics-for-a-nim-model_odh-user"},{"parentId":null,"name":"Deploying models on the multi-model serving platform","level":0,"index":5,"id":"_deploying_models_on_the_multi_model_serving_platform"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Adding a model server for the multi-model serving platform","level":1,"index":0,"id":"adding-a-model-server-for-the-multi-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Deleting a model server","level":1,"index":1,"id":"deleting-a-model-server_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Deploying a model by using the multi-model serving platform","level":1,"index":2,"id":"deploying-a-model-using-the-multi-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Viewing a deployed model","level":1,"index":3,"id":"viewing-a-deployed-model_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Updating the deployment properties of a deployed model","level":1,"index":4,"id":"updating-the-deployment-properties-of-a-deployed-model_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Deleting a deployed model","level":1,"index":5,"id":"deleting-a-deployed-model_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Configuring monitoring for the multi-model serving platform","level":1,"index":6,"id":"configuring-monitoring-for-the-multi-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Viewing model-serving runtime metrics for the multi-model serving platform","level":1,"index":7,"id":"viewing-metrics-for-the-multi-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Viewing performance metrics for all models on a model server","level":1,"index":8,"id":"viewing-performance-metrics-for-model-server_odh-user"},{"parentId":"_deploying_models_on_the_multi_model_serving_platform","name":"Viewing HTTP request metrics for a deployed model","level":1,"index":9,"id":"viewing-http-request-metrics-for-a-deployed-model_odh-user"},{"parentId":null,"name":"Making inference requests to deployed models","level":0,"index":6,"id":"_making_inference_requests_to_deployed_models"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the authentication token for a deployed model","level":1,"index":0,"id":"accessing-authentication-token-for-deployed-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the inference endpoint for a deployed model","level":1,"index":1,"id":"accessing-inference-endpoint-for-deployed-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Making inference requests to models deployed on the single-model serving platform","level":1,"index":2,"id":"making-inference-requests-to-models-deployed-on-single-model-serving-platform_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Inference endpoints","level":1,"index":3,"id":"inference-endpoints_odh-user"},{"parentId":"inference-endpoints_odh-user","name":"Caikit TGIS ServingRuntime for KServe","level":2,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"Caikit Standalone ServingRuntime for KServe","level":2,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"TGIS Standalone ServingRuntime for KServe","level":2,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"OpenVINO Model Server","level":2,"index":3,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_odh-user","name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":2,"index":4,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":2,"index":5,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM AMD GPU ServingRuntime for KServe","level":2,"index":6,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":2,"index":7,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"NVIDIA Triton Inference Server","level":2,"index":8,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_odh-user","name":"Seldon MLServer","level":2,"index":9,"id":"_seldon_mlserver"},{"parentId":"inference-endpoints_odh-user","name":"Additional resources","level":2,"index":10,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-accelerators/"},"sections":[{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":"intel-gaudi-ai-accelerator-integration_managing-odh","name":"Enabling Intel Gaudi AI accelerators","level":2,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_managing-odh"},{"parentId":null,"name":"AMD GPU Integration","level":1,"index":2,"id":"amd-gpu-integration_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Verifying AMD GPU availability on your cluster","level":2,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Enabling AMD GPUs","level":2,"index":1,"id":"enabling-amd-gpus_managing-odh"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-external-resource-access-for-lmeval-jobs/"},"sections":[{"parentId":null,"name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":1,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_{context}"},{"parentId":null,"name":"Updating LMEval job configuration using the web console","level":1,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enforcing-local-queues/"},"sections":[{"parentId":null,"name":"Enforcing the local-queue labeling policy for all projects","level":1,"index":0,"id":"enforcing-lqlabel-all_{context}"},{"parentId":null,"name":"Disabling the local-queue labeling policy for all projects","level":1,"index":1,"id":"disabling-lqlabel-all_{context}"},{"parentId":null,"name":"Enforcing the local-queue labeling policy for some projects only","level":1,"index":2,"id":"enforcing-lqlabel-some_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-large-language-models/"},"sections":[{"parentId":null,"name":"Setting up LM-Eval","level":1,"index":0,"id":"setting-up-lmeval_{context}"},{"parentId":null,"name":"Enabling external resource access for LMEval jobs","level":1,"index":1,"id":"enabling-external-resource-access-for-lmeval-jobs_{context}"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_{context}","name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":2,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_{context}"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_{context}","name":"Updating LMEval job configuration using the web console","level":2,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_{context}"},{"parentId":null,"name":"LM-Eval evaluation job","level":1,"index":2,"id":"lmeval-evaluation-job_{context}"},{"parentId":null,"name":"LM-Eval evaluation job properties","level":1,"index":3,"id":"lmeval-evaluation-job-properties_{context}"},{"parentId":"lmeval-evaluation-job-properties_{context}","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":2,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":null,"name":"Performing model evaluations in the dashboard","level":1,"index":4,"id":"performing-model-evaluations-in-the-dashboard_{context}"},{"parentId":null,"name":"LM-Eval scenarios","level":1,"index":5,"id":"lmeval-scenarios_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Accessing Hugging Face models with an environment variable token","level":2,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using a custom Unitxt card","level":2,"index":1,"id":"using-a-custom-unitxt-card_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using PVCs as storage","level":2,"index":2,"id":"using-pvcs-as-storage_{context}"},{"parentId":"using-pvcs-as-storage_{context}","name":"Managed PVCs","level":3,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_{context}","name":"Existing PVCs","level":3,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_{context}","name":"Using a KServe Inference Service","level":2,"index":3,"id":"using-a-kserve-inference-service_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Setting up LM-Eval S3 Support","level":2,"index":4,"id":"setting-up-lmeval-s3-support_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using LLM-as-a-Judge metrics with LM-Eval","level":2,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/example-kfto-pytorch-training-scripts/"},"sections":[{"parentId":null,"name":"Example Training Operator PyTorch training script: NCCL","level":1,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training script: DDP","level":1,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training script: FSDP","level":1,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/fine-tuning-a-model-by-using-kubeflow-training/"},"sections":[{"parentId":null,"name":"Configuring the fine-tuning job","level":1,"index":0,"id":"configuring-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Running the fine-tuning job","level":1,"index":1,"id":"running-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Deleting the fine-tuning job","level":1,"index":2,"id":"deleting-the-fine-tuning-job_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the Open Data Hub dashboard","level":1,"index":3,"id":"accessing-the-odh-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Configuring custom namespaces","level":1,"index":0,"id":"configuring-custom-namespaces"},{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":2,"id":"installing-odh-components_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/lmeval-scenarios/"},"sections":[{"parentId":null,"name":"Accessing Hugging Face models with an environment variable token","level":1,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_{context}"},{"parentId":null,"name":"Using a custom Unitxt card","level":1,"index":1,"id":"using-a-custom-unitxt-card_{context}"},{"parentId":null,"name":"Using PVCs as storage","level":1,"index":2,"id":"using-pvcs-as-storage_{context}"},{"parentId":"using-pvcs-as-storage_{context}","name":"Managed PVCs","level":2,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_{context}","name":"Existing PVCs","level":2,"index":1,"id":"_existing_pvcs"},{"parentId":null,"name":"Using a KServe Inference Service","level":1,"index":3,"id":"using-a-kserve-inference-service_{context}"},{"parentId":null,"name":"Setting up LM-Eval S3 Support","level":1,"index":4,"id":"setting-up-lmeval-s3-support_{context}"},{"parentId":null,"name":"Using LLM-as-a-Judge metrics with LM-Eval","level":1,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-access-to-data-science-projects/"},"sections":[{"parentId":null,"name":"Granting access to a data science project","level":1,"index":0,"id":"granting-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Updating access to a data science project","level":1,"index":1,"id":"updating-access-to-a-data-science-project_{context}"},{"parentId":null,"name":"Removing access to a data science project","level":1,"index":2,"id":"removing-access-to-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-and-monitoring-models/"},"sections":[{"parentId":null,"name":"Adding a custom model-serving runtime for the single-model serving platform","level":1,"index":0,"id":"adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models on the single-model serving platform","level":0,"index":1,"id":"_managing_and_monitoring_models_on_the_single_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Setting a timeout for KServe","level":1,"index":0,"id":"setting-timeout-for-kserve_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Deploying models by using multiple GPU nodes","level":1,"index":1,"id":"deploying-models-using-multiple-gpu-nodes_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Configuring an inference service for Kueue","level":1,"index":2,"id":"configuring-an-inference-service-for-kueue_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Configuring an inference service for Spyre","level":1,"index":3,"id":"configuring-inference-service-for-spyre_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Optimizing performance and tuning","level":1,"index":4,"id":"_optimizing_performance_and_tuning"},{"parentId":"_optimizing_performance_and_tuning","name":"Determining GPU requirements for LLM-powered applications","level":2,"index":0,"id":"determining-gpu-requirements-for-llm-powered-applications_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Performance considerations for text-summarization and retrieval-augmented generation (RAG) applications","level":2,"index":1,"id":"performance-considerations-for-document-based-apps_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Inference performance metrics","level":2,"index":2,"id":"inference-performance-metrics_cluster-admin"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Latency","level":3,"index":0,"id":"_latency"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Throughput","level":3,"index":1,"id":"_throughput"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Cost per million tokens","level":3,"index":2,"id":"_cost_per_million_tokens"},{"parentId":"_optimizing_performance_and_tuning","name":"Configuring metrics-based autoscaling","level":2,"index":3,"id":"configuring-metrics-based-autoscaling_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Guidelines for metrics-based autoscaling","level":2,"index":4,"id":"guidelines-for-metrics-based-autoscaling_cluster-admin"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing metrics for latency and throughput-optimized scaling","level":3,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing the right sliding window","level":3,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Optimizing HPA scale-down configuration","level":3,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Considering model size for optimal scaling","level":3,"index":3,"id":"_considering_model_size_for_optimal_scaling"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Monitoring models on the single-model serving platform","level":1,"index":5,"id":"_monitoring_models_on_the_single_model_serving_platform"},{"parentId":"_monitoring_models_on_the_single_model_serving_platform","name":"Configuring monitoring for the single-model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-the-single-model-serving-platform_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_single_model_serving_platform","name":"Using Grafana to monitor model performance","level":1,"index":6,"id":"_using_grafana_to_monitor_model_performance"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a Grafana metrics dashboard","level":2,"index":0,"id":"Deploying-a-grafana-metrics-dashboard_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a vLLM/GPU metrics dashboard on a Grafana instance","level":2,"index":1,"id":"deploying-vllm-gpu-metrics-dashboard-grafana_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Grafana metrics","level":2,"index":2,"id":"ref-grafana-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"Accelerator metrics","level":3,"index":0,"id":"ref-accelerator-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"CPU metrics","level":3,"index":1,"id":"ref-cpu-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"vLLM metrics","level":3,"index":2,"id":"ref-vllm-metrics_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models on the NVIDIA NIM model serving platform","level":0,"index":2,"id":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Customizing model selection options for the NVIDIA NIM model serving platform","level":1,"index":0,"id":"Customizing-model-selection-options_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":1,"index":1,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin","name":"Enabling graph generation for an existing NIM deployment","level":2,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin","name":"Enabling metrics collection for an existing NIM deployment","level":2,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"},{"parentId":null,"name":"Managing and monitoring models on the multi-model serving platform","level":0,"index":3,"id":"_managing_and_monitoring_models_on_the_multi_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_multi_model_serving_platform","name":"Configuring monitoring for the multi-model serving platform","level":1,"index":0,"id":"configuring-monitoring-for-the-multi-model-serving-platform_cluster-admin"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Disabling applications connected to {productname-short}","level":1,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":null,"name":"Showing or hiding information about available applications","level":1,"index":3,"id":"showing-hiding-information-about-available-applications_dashboard"},{"parentId":null,"name":"Hiding the default basic workbench application","level":1,"index":4,"id":"hiding-the-default-basic-workbench-application_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-basic-workbenches/"},"sections":[{"parentId":null,"name":"Accessing the administration interface for basic workbenches","level":1,"index":0,"id":"accessing-the-administration-interface-for-basic-workbenches_{context}"},{"parentId":null,"name":"Starting basic workbenches owned by other users","level":1,"index":1,"id":"starting-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing basic workbenches owned by other users","level":1,"index":2,"id":"accessing-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping basic workbenches owned by other users","level":1,"index":3,"id":"stopping-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle workbenches","level":1,"index":4,"id":"stopping-idle-workbenches_{context}"},{"parentId":null,"name":"Adding workbench pod tolerations","level":1,"index":5,"id":"adding-workbench-pod-tolerations_{context}"},{"parentId":null,"name":"Troubleshooting common problems in workbenches for administrators","level":1,"index":6,"id":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"A user&#8217;s workbench does not start","level":2,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-pvc-size/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-connection-types/"},"sections":[{"parentId":null,"name":"Viewing connection types","level":1,"index":0,"id":"viewing-connection-types_{context}"},{"parentId":null,"name":"Creating a connection type","level":1,"index":1,"id":"creating-a-connection-type_{context}"},{"parentId":null,"name":"Duplicating a connection type","level":1,"index":2,"id":"duplicating-a-connection-type_{context}"},{"parentId":null,"name":"Editing a connection type","level":1,"index":3,"id":"editing-a-connection-type_{context}"},{"parentId":null,"name":"Enabling a connection type","level":1,"index":4,"id":"enabling-a-connection-type_{context}"},{"parentId":null,"name":"Deleting a connection type","level":1,"index":5,"id":"deleting-a-connection-type_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-custom-training-images/"},"sections":[{"parentId":null,"name":"About base training images","level":1,"index":0,"id":"about-base-training-images_{context}"},{"parentId":null,"name":"Creating a custom training image","level":1,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":null,"name":"Pushing an image to the integrated OpenShift image registry","level":1,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Configuring a pipeline server","level":1,"index":0,"id":"configuring-a-pipeline-server_{context}"},{"parentId":"configuring-a-pipeline-server_{context}","name":"Configuring a pipeline server with an external Amazon RDS database","level":2,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_{context}"},{"parentId":null,"name":"Defining a pipeline","level":1,"index":1,"id":"defining-a-pipeline_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Compiling the pipeline YAML with the Kubeflow Pipelines SDK","level":2,"index":0,"id":"compiling-the-pipeline-yaml-with-kfp-sdk_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Compiling Kubernetes-native manifests with the Kubeflow Pipelines SDK","level":2,"index":1,"id":"compiling-kubernetes-native-manifests-with-kfp-sdk_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Defining a pipeline by using the Kubernetes API","level":2,"index":2,"id":"defining-a-pipeline-by-using-the-kubernetes-api_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Migrating pipelines from database to Kubernetes API storage","level":2,"index":3,"id":"migrating-pipelines-from-database-to-kubernetes-api_{context}"},{"parentId":null,"name":"Importing a data science pipeline","level":1,"index":2,"id":"importing-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a data science pipeline","level":1,"index":3,"id":"deleting-a-data-science-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline server","level":1,"index":4,"id":"deleting-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline server","level":1,"index":5,"id":"viewing-the-details-of-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing existing pipelines","level":1,"index":6,"id":"viewing-existing-pipelines_{context}"},{"parentId":null,"name":"Overview of pipeline versions","level":1,"index":7,"id":"overview-of-pipeline-versions_{context}"},{"parentId":null,"name":"Uploading a pipeline version","level":1,"index":8,"id":"uploading-a-pipeline-version_{context}"},{"parentId":null,"name":"Deleting a pipeline version","level":1,"index":9,"id":"deleting-a-pipeline-version_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline version","level":1,"index":10,"id":"viewing-the-details-of-a-pipeline-version_{context}"},{"parentId":null,"name":"Downloading a data science pipeline version","level":1,"index":11,"id":"downloading-a-data-science-pipeline-version_{context}"},{"parentId":null,"name":"Overview of data science pipelines caching","level":1,"index":12,"id":"overview-of-data-science-pipelines-caching_{context}"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Caching criteria","level":2,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Viewing cached steps in the {productname-short} user interface","level":2,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":"overview-of-data-science-pipelines-caching_{context}","name":"Controlling caching in data science pipelines","level":2,"index":2,"id":"controlling-caching-in-data-science-pipelines_{context}"},{"parentId":"controlling-caching-in-data-science-pipelines_{context}","name":"Disabling caching for individual tasks","level":3,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"controlling-caching-in-data-science-pipelines_{context}","name":"Disabling caching for a pipeline at submit time","level":3,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":"controlling-caching-in-data-science-pipelines_{context}","name":"Disabling caching for a pipeline at compile time","level":3,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":"controlling-caching-in-data-science-pipelines_{context}","name":"Disabling caching for all pipelines (pipeline server)","level":3,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-distributed-workloads/"},"sections":[{"parentId":null,"name":"Configuring quota management for distributed workloads","level":1,"index":0,"id":"configuring-quota-management-for-distributed-workloads_{context}"},{"parentId":null,"name":"Example Kueue resource configurations for distributed workloads","level":1,"index":1,"id":"ref-example-kueue-resource-configurations_{context}"},{"parentId":"ref-example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs without shared cohort","level":2,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":3,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":3,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":3,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":3,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"ref-example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":2,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":3,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":3,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":3,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":3,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":null,"name":"Configuring the CodeFlare Operator","level":1,"index":2,"id":"configuring-the-codeflare-operator_{context}"},{"parentId":null,"name":"Configuring a cluster for RDMA","level":1,"index":3,"id":"configuring-a-cluster-for-rdma_{context}"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for administrators","level":1,"index":4,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a suspended state","level":2,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a failed state","level":2,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user receives a \"failed to call webhook\" error message for the CodeFlare Operator","level":2,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster does not start","level":2,"index":3,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user cannot create a Ray cluster or submit jobs","level":2,"index":4,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"Additional resources","level":2,"index":5,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-observability/"},"sections":[{"parentId":null,"name":"Enabling the observability stack","level":1,"index":0,"id":"enabling-the-observability-stack_{context}"},{"parentId":null,"name":"Collecting metrics from user workloads","level":1,"index":1,"id":"collecting-metrics-from-user-workloads_{context}"},{"parentId":null,"name":"Exporting metrics to external observability tools","level":1,"index":2,"id":"exporting-metrics-to-external-observability-tools_{context}"},{"parentId":null,"name":"Viewing traces in external tracing platforms","level":1,"index":3,"id":"viewing-traces-in-external-tracing-platforms_{context}"},{"parentId":null,"name":"Accessing built-in alerts","level":1,"index":4,"id":"accessing-built-in-alerts_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-experiments/"},"sections":[{"parentId":null,"name":"Overview of pipeline experiments","level":1,"index":0,"id":"overview-of-pipeline-experiments_{context}"},{"parentId":null,"name":"Creating a pipeline experiment","level":1,"index":1,"id":"creating-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Archiving a pipeline experiment","level":1,"index":2,"id":"archiving-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Deleting an archived pipeline experiment","level":1,"index":3,"id":"deleting-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Restoring an archived pipeline experiment","level":1,"index":4,"id":"restoring-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Viewing pipeline task executions","level":1,"index":5,"id":"viewing-pipeline-task-executions_{context}"},{"parentId":null,"name":"Viewing pipeline artifacts","level":1,"index":6,"id":"viewing-pipeline-artifacts_{context}"},{"parentId":null,"name":"Comparing runs in an experiment","level":1,"index":7,"id":"comparing-runs-in-an-experiment_{context}"},{"parentId":null,"name":"Comparing runs in different experiments","level":1,"index":8,"id":"comparing-runs-in-different-experiments_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-runs/"},"sections":[{"parentId":null,"name":"Overview of pipeline runs","level":1,"index":0,"id":"overview-of-pipeline-runs_{context}"},{"parentId":null,"name":"Storing data with data science pipelines","level":1,"index":1,"id":"storing-data-with-data-science-pipelines_{context}"},{"parentId":null,"name":"Viewing active pipeline runs","level":1,"index":2,"id":"viewing-active-pipeline-runs_{context}"},{"parentId":null,"name":"Executing a pipeline run","level":1,"index":3,"id":"executing-a-pipeline-run_{context}"},{"parentId":null,"name":"Stopping an active pipeline run","level":1,"index":4,"id":"stopping-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an active pipeline run","level":1,"index":5,"id":"duplicating-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Viewing scheduled pipeline runs","level":1,"index":6,"id":"viewing-scheduled-pipeline-runs_{context}"},{"parentId":null,"name":"Scheduling a pipeline run using a cron job","level":1,"index":7,"id":"scheduling-a-pipeline-run-using-a-cron-job_{context}"},{"parentId":null,"name":"Scheduling a pipeline run","level":1,"index":8,"id":"scheduling-a-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating a scheduled pipeline run","level":1,"index":9,"id":"duplicating-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Deleting a scheduled pipeline run","level":1,"index":10,"id":"deleting-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline run","level":1,"index":11,"id":"viewing-the-details-of-a-pipeline-run_{context}"},{"parentId":null,"name":"Viewing archived pipeline runs","level":1,"index":12,"id":"viewing-archived-pipeline-runs_{context}"},{"parentId":null,"name":"Archiving a pipeline run","level":1,"index":13,"id":"archiving-a-pipeline-run_{context}"},{"parentId":null,"name":"Restoring an archived pipeline run","level":1,"index":14,"id":"restoring-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Deleting an archived pipeline run","level":1,"index":15,"id":"deleting-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an archived pipeline run","level":1,"index":16,"id":"duplicating-an-archived-pipeline-run_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages-in-code-server/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your code-server workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your code-server workbench","level":1,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your workbench","level":1,"index":1,"id":"installing-python-packages-on-your-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-storage-classes/"},"sections":[{"parentId":null,"name":"About persistent storage","level":1,"index":0,"id":"about-persistent-storage_{context}"},{"parentId":"about-persistent-storage_{context}","name":"Storage classes in {productname-short}","level":2,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":"about-persistent-storage_{context}","name":"Access modes","level":2,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":3,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":null,"name":"Configuring storage class settings","level":1,"index":1,"id":"configuring-storage-class-settings_{context}"},{"parentId":null,"name":"Configuring the default storage class for your cluster","level":1,"index":2,"id":"configuring-the-default-storage-class-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":3,"id":"overview-of-object-storage-endpoints_{context}"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-users-and-groups/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":1,"id":"viewing-data-science-users_{context}"},{"parentId":null,"name":"Adding users to {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-user-groups_{context}"},{"parentId":null,"name":"Selecting {productname-short} administrator and user groups","level":1,"index":3,"id":"selecting-admin-and-user-groups_{context}"},{"parentId":null,"name":"Deleting users","level":1,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":"_deleting_users","name":"Stopping basic workbenches owned by other users","level":2,"index":1,"id":"stopping-basic-workbenches-owned-by-other-users_{context}"},{"parentId":"_deleting_users","name":"Revoking user access to basic workbenches","level":2,"index":2,"id":"revoking-user-access-to-basic-workbenches_{context}"},{"parentId":"_deleting_users","name":"Backing up storage data","level":2,"index":3,"id":"backing-up-storage-data_{context}"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Overview of managing workloads with Kueue","level":1,"index":0,"id":"overview-of-managing-workloads-with-kueue_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue management states","level":2,"index":0,"id":"_kueue_management_states"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Queue enforcement for projects","level":2,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Restrictions for managing workloads with Kueue","level":2,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue workflow","level":2,"index":3,"id":"kueue-workflow_kueue"},{"parentId":null,"name":"Configuring workload management with Kueue","level":1,"index":1,"id":"configuring-workload-management-with-kueue_kueue"},{"parentId":"configuring-workload-management-with-kueue_kueue","name":"Enabling Kueue in the dashboard","level":2,"index":0,"id":"enabling-kueue-in-the-dashboard_kueue"},{"parentId":null,"name":"Troubleshooting common problems with Kueue","level":1,"index":2,"id":"troubleshooting-common-problems-with-Kueue_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"failed to call webhook\" error message for Kueue","level":2,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":2,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"local_queue provided does not exist\" error message","level":2,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"The pod provisioned by Kueue is terminated before the image is pulled","level":2,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"Additional resources","level":2,"index":4,"id":"_additional_resources"},{"parentId":null,"name":"Migrating to the {rhbok-productname} Operator","level":1,"index":3,"id":"migrating-to-the-rhbok-operator_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-data-drift/"},"sections":[{"parentId":null,"name":"Creating a drift metric","level":1,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":2,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Deleting a drift metric by using the CLI","level":1,"index":1,"id":"deleting-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Viewing data drift metrics for a model","level":1,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using drift metrics","level":1,"index":3,"id":"using-drift-metrics_drift-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Viewing project metrics for distributed workloads","level":1,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing the status of distributed workloads","level":1,"index":1,"id":"viewing-the-status-of-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing Kueue alerts for distributed workloads","level":1,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-bias/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Using bias metrics","level":1,"index":3,"id":"using-bias-metrics_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/overview-of-ml-features-and-feature-store/"},"sections":[{"parentId":null,"name":"Overview of machine learning features","level":1,"index":0,"id":"overview-of-machine-learning-features_{context}"},{"parentId":null,"name":"Overview of Feature Store","level":1,"index":1,"id":"overview-of-feature-store_{context}"},{"parentId":null,"name":"Audience for Feature Store","level":1,"index":2,"id":"audience-for-feature-store_{context}"},{"parentId":null,"name":"Feature Store workflow","level":1,"index":3,"id":"feature-store-workflow_{context}"},{"parentId":null,"name":"Setting up the Feature Store user interface for initial use","level":1,"index":4,"id":"setting-up-feature-store-UI_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":5,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/preparing-the-distributed-training-environment/"},"sections":[{"parentId":null,"name":"Creating a workbench for distributed training","level":1,"index":0,"id":"creating-a-workbench-for-distributed-training_{context}"},{"parentId":null,"name":"Using the cluster server and token to authenticate","level":1,"index":1,"id":"using-the-cluster-server-and-token-to-authenticate_{context}"},{"parentId":null,"name":"Managing custom training images","level":1,"index":2,"id":"managing-custom-training-images_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"About base training images","level":2,"index":0,"id":"about-base-training-images_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"Creating a custom training image","level":2,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"Pushing an image to the integrated OpenShift image registry","level":2,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/retrieving-features-for-model-training/"},"sections":[{"parentId":null,"name":"Making features available for real-time inference","level":1,"index":0,"id":"making-features-available-for-real-time-inference_{context}"},{"parentId":null,"name":"Retrieving online features for model inference","level":1,"index":1,"id":"retrieving-online-features-for-model-inference_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-kfto-based-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Using the Kubeflow Training Operator to run distributed training workloads","level":1,"index":0,"id":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":2,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorchJob resource","level":2,"index":1,"id":"creating-a-kfto-pytorchjob-resource_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":2,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Training Operator PyTorch training scripts","level":2,"index":3,"id":"example-kfto-pytorch-training-scripts_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: NCCL","level":3,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: DDP","level":3,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: FSDP","level":3,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Dockerfile for a Training Operator PyTorch training script","level":2,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Training Operator PyTorchJob resource for multi-node training","level":2,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_{context}"},{"parentId":null,"name":"Using the Training Operator SDK to run distributed training workloads","level":1,"index":1,"id":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"Configuring a training job by using the Training Operator SDK","level":2,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"Running a training job by using the Training Operator SDK","level":2,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"TrainingClient API: Job-related methods","level":2,"index":2,"id":"ref-trainingclient-api-job-related-methods_{context}"},{"parentId":null,"name":"Fine-tuning a model by using Kubeflow Training","level":1,"index":2,"id":"fine-tuning-a-model-by-using-kubeflow-training_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Configuring the fine-tuning job","level":2,"index":0,"id":"configuring-the-fine-tuning-job_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Running the fine-tuning job","level":2,"index":1,"id":"running-the-fine-tuning-job_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Deleting the fine-tuning job","level":2,"index":2,"id":"deleting-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Creating a multi-node PyTorch training job with RDMA","level":1,"index":3,"id":"creating-a-multi-node-pytorch-training-job-with-rdma_{context}"},{"parentId":null,"name":"Example Training Operator PyTorchJob resource configured to run with RDMA","level":1,"index":4,"id":"ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-ray-based-distributed-workloads/"},"sections":[{"parentId":null,"name":"Running distributed data science workloads from Jupyter notebooks","level":1,"index":0,"id":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Downloading the demo Jupyter notebooks from the CodeFlare SDK","level":2,"index":0,"id":"downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Running the demo Jupyter notebooks from the CodeFlare SDK","level":2,"index":1,"id":"running-the-demo-jupyter-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Managing Ray clusters from within a Jupyter notebook","level":2,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Running distributed data science workloads from data science pipelines","level":1,"index":1,"id":"running-distributed-data-science-workloads-from-ds-pipelines_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-feature-store/"},"sections":[{"parentId":null,"name":"Before you begin","level":1,"index":0,"id":"before-you-begin_{context}"},{"parentId":null,"name":"Enabling the Feature Store component","level":1,"index":1,"id":"enabling-the-feature-store-component_{context}"},{"parentId":null,"name":"Creating a Feature Store instance in a data science project","level":1,"index":2,"id":"creating-a-feature-store-instance-in-a-data-science-project_{context}"},{"parentId":null,"name":"Adding feature definitions and initializing your Feature Store instance","level":1,"index":3,"id":"adding-feature-definitions-and-initializing-your-feature-store-instance_{context}"},{"parentId":"adding-feature-definitions-and-initializing-your-feature-store-instance_{context}","name":"Specifying files to ignore","level":2,"index":0,"id":"specifying-files-to-ignore_{context}"},{"parentId":null,"name":"Viewing Feature Store objects in the web-based UI","level":1,"index":4,"id":"viewing-feature-store-objects-in-the-web-based-ui_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-trustyai-for-your-project/"},"sections":[{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":0,"id":"authenticating-trustyai-service_{context}"},{"parentId":null,"name":"Uploading training data to TrustyAI","level":1,"index":1,"id":"uploading-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Sending training data to TrustyAI","level":1,"index":2,"id":"sending-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Labeling data fields","level":1,"index":3,"id":"labeling-data-fields_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 1","level":1,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":null,"name":"Upgrading the Open Data Hub Operator","level":1,"index":1,"id":"upgrading-the-odh-operator_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 2","level":1,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":null,"name":"Upgrading the Open Data Hub Operator","level":1,"index":1,"id":"upgrading-the-odh-operator_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-basic-workbenches/"},"sections":[{"parentId":null,"name":"Starting a basic workbench","level":1,"index":0,"id":"starting-a-basic-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-connections/"},"sections":[{"parentId":null,"name":"Adding a connection to your data science project","level":1,"index":0,"id":"adding-a-connection-to-your-data-science-project_{context}"},{"parentId":null,"name":"Updating a connection","level":1,"index":1,"id":"updating-a-connection_{context}"},{"parentId":null,"name":"Deleting a connection","level":1,"index":2,"id":"deleting-a-connection_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-data-science-projects/"},"sections":[{"parentId":null,"name":"Creating a data science project","level":1,"index":0,"id":"creating-a-data-science-project_{context}"},{"parentId":null,"name":"Updating a data science project","level":1,"index":1,"id":"updating-a-data-science-project_{context}"},{"parentId":null,"name":"Deleting a data science project","level":1,"index":2,"id":"deleting-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-explainability/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation","level":1,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":2,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":null,"name":"Requesting a SHAP explanation","level":1,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":2,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":null,"name":"Using explainers","level":1,"index":2,"id":"using-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-llama-stack-with-trustyai/"},"sections":[{"parentId":null,"name":"Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI","level":1,"index":0,"id":"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_{context}"},{"parentId":null,"name":"Running custom evaluations with LM-Eval and Llama Stack","level":1,"index":1,"id":"running-custom-evaluations-with-LMEval-and-llama-stack_{context}"},{"parentId":null,"name":"Using Guardrails Orchestrator with Llama Stack","level":1,"index":2,"id":"using-guardrails-orchestrator-with-llama-stack_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-project-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":0,"id":"creating-a-workbench-select-ide_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Starting a workbench","level":1,"index":1,"id":"starting-a-workbench_{context}"},{"parentId":null,"name":"Updating a project workbench","level":1,"index":2,"id":"updating-a-project-workbench_{context}"},{"parentId":null,"name":"Deleting a workbench from a data science project","level":1,"index":3,"id":"deleting-a-workbench-from-a-data-science-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-the-kfto-sdk-to-run-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Configuring a training job by using the Training Operator SDK","level":1,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":null,"name":"Running a training job by using the Training Operator SDK","level":1,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":null,"name":"TrainingClient API: Job-related methods","level":1,"index":2,"id":"ref-trainingclient-api-job-related-methods_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-the-kubeflow-training-operator-to-run-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":1,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_{context}"},{"parentId":null,"name":"Creating a Training Operator PyTorchJob resource","level":1,"index":1,"id":"creating-a-kfto-pytorchjob-resource_{context}"},{"parentId":null,"name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":1,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training scripts","level":1,"index":3,"id":"example-kfto-pytorch-training-scripts_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: NCCL","level":2,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: DDP","level":2,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: FSDP","level":2,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"},{"parentId":null,"name":"Example Dockerfile for a Training Operator PyTorch training script","level":1,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_{context}"},{"parentId":null,"name":"Example Training Operator PyTorchJob resource for multi-node training","level":1,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/viewing-logs-and-audit-records/"},"sections":[{"parentId":null,"name":"Configuring the {productname-short} Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_{context}"},{"parentId":"configuring-the-operator-logger_{context}","name":"Viewing the {productname-short} Operator logs","level":2,"index":0,"id":"_viewing_the_productname_short_operator_logs"},{"parentId":null,"name":"Viewing audit records","level":1,"index":1,"id":"viewing-audit-records_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding how {productname-short} handles certificates","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":null,"name":"Adding certificates","level":1,"index":1,"id":"_adding_certificates"},{"parentId":null,"name":"Adding certificates to a cluster-wide CA bundle","level":1,"index":2,"id":"adding-certificates-to-a-cluster-ca-bundle_certs"},{"parentId":null,"name":"Adding certificates to a custom CA bundle","level":1,"index":3,"id":"adding-certificates-to-a-custom-ca-bundle_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":4,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Accessing S3-compatible object storage with self-signed certificates","level":2,"index":0,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Configuring a certificate for data science pipelines","level":2,"index":1,"id":"configuring-a-certificate-for-pipelines_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Configuring a certificate for workbenches","level":2,"index":2,"id":"configuring-a-certificate-for-workbenches_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using the cluster-wide CA bundle for the single-model serving platform","level":2,"index":3,"id":"using-the-cluster-CA-bundle-for-single-model-serving_certs"},{"parentId":null,"name":"Managing certificates without the {productname-long} Operator","level":1,"index":5,"id":"managing-certificates-without-the-operator_certs"},{"parentId":null,"name":"Removing the CA bundle","level":1,"index":6,"id":"_removing_the_ca_bundle"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from all namespaces","level":2,"index":0,"id":"removing-the-ca-bundle-from-all-namespaces_certs"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from a single namespace","level":2,"index":1,"id":"removing-the-ca-bundle-from-a-single-namespace_certs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":0,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Registering a model from the dashboard","level":1,"index":0,"id":"registering-a-model_model-registry"},{"parentId":null,"name":"Registering a model version","level":1,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":null,"name":"Viewing registered models","level":1,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":null,"name":"Viewing registered model versions","level":1,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":null,"name":"Editing model metadata in a model registry","level":1,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Editing model version metadata in a model registry","level":1,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Deploying a model version from a model registry","level":1,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Editing the deployment properties of a deployed model version from a model registry","level":1,"index":7,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the multi-model serving platform","level":2,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform_model-registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the single-model serving platform","level":2,"index":1,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform_model-registry"},{"parentId":null,"name":"Deleting a deployed model version from a model registry","level":1,"index":8,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Archiving a model","level":1,"index":9,"id":"archiving-a-model_model-registry"},{"parentId":null,"name":"Archiving a model version","level":1,"index":10,"id":"archiving-a-model-version_model-registry"},{"parentId":null,"name":"Restoring a model","level":1,"index":11,"id":"restoring-a-model_model-registry"},{"parentId":null,"name":"Restoring a model version","level":1,"index":12,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipeline-logs/"},"sections":[{"parentId":null,"name":"About pipeline logs","level":1,"index":0,"id":"about-pipeline-logs_{context}"},{"parentId":null,"name":"Viewing pipeline step logs","level":1,"index":1,"id":"viewing-pipeline-step-logs_{context}"},{"parentId":null,"name":"Downloading pipeline step logs","level":1,"index":2,"id":"downloading-pipeline-step-logs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipelines-in-jupyterlab/"},"sections":[{"parentId":null,"name":"Overview of pipelines in JupyterLab","level":1,"index":0,"id":"overview-of-pipelines-in-jupyterlab_{context}"},{"parentId":null,"name":"Accessing the pipeline editor","level":1,"index":1,"id":"accessing-the-pipeline-editor_{context}"},{"parentId":null,"name":"Disabling node caching in Elyra","level":1,"index":2,"id":"disabling-node-caching-in-elyra_{context}"},{"parentId":null,"name":"Creating a runtime configuration","level":1,"index":3,"id":"creating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Updating a runtime configuration","level":1,"index":4,"id":"updating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Deleting a runtime configuration","level":1,"index":5,"id":"deleting-a-runtime-configuration_{context}"},{"parentId":null,"name":"Duplicating a runtime configuration","level":1,"index":6,"id":"duplicating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Running a pipeline in JupyterLab","level":1,"index":7,"id":"running-a-pipeline-in-jupyterlab_{context}"},{"parentId":null,"name":"Exporting a pipeline in JupyterLab","level":1,"index":8,"id":"exporting-a-pipeline-in-jupyterlab_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-the-model-catalog/"},"sections":[{"parentId":null,"name":"Viewing models in the model catalog","level":1,"index":0,"id":"viewing-models-in-the-catalog_model-registry"},{"parentId":null,"name":"Registering a model from the model catalog","level":1,"index":1,"id":"registering-a-model-from-the-model-catalog_model-registry"},{"parentId":null,"name":"Deploying a model from the model catalog","level":1,"index":2,"id":"deploying-a-model-from-the-model-catalog_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-feature-definitions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-kserve-deployment-modes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-llama-stack-search-types/"},"sections":[{"parentId":null,"name":"Supported search modes","level":1,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":2,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":2,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":2,"index":2,"id":"_hybrid_search"},{"parentId":null,"name":"Retrieval database support","level":1,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Single-model serving platform","level":1,"index":0,"id":"_single_model_serving_platform"},{"parentId":null,"name":"Multi-model serving platform","level":1,"index":1,"id":"_multi_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":2,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-organizing-features-by-using-entities/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-persistent-storage/"},"sections":[{"parentId":null,"name":"Storage classes in {productname-short}","level":1,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":null,"name":"Access modes","level":1,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":2,"index":0,"id":"_using_shared_storage_rwx"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authentication-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-built-in-alerts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-hugging-face-models-with-an-environment-variable-token/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-administration-interface-for-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/activating-the-llama-stack-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-certificates-to-a-cluster-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-certificates-to-a-custom-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-feature-definitions-and-initializing-your-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-workbench-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/audience-for-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/auto-configuring-guardrails/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/before-you-begin/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/benchmarking-embedding-models-with-beir-datasets-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/collecting-metrics-from-user-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/compiling-kubernetes-native-manifests-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/compiling-the-pipeline-yaml-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-certificate-for-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-certificate-for-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-cluster-for-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-inference-service-for-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-inference-service-for-spyre/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-offline-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-online-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-metric-based-autoscaling/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-pipelines-with-your-own-argo-workflows-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-built-in-detector-and-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-guardrails-detector-hugging-face-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator logs","level":1,"index":0,"id":"_viewing_the_productname_short_operator_logs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-workload-management-with-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/controlling-caching-in-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Disabling caching for individual tasks","level":1,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":null,"name":"Disabling caching for a pipeline at submit time","level":1,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":null,"name":"Disabling caching for a pipeline at compile time","level":1,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":null,"name":"Disabling caching for all pipelines (pipeline server)","level":1,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-workbench-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-workbench-overview/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-custom-image-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-feature-store-instance-in-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorch-training-script-configmap-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorchjob-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-multi-node-pytorch-training-job-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-for-distributed-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-feature-views/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-project-scoped-resources-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-project-scoped-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-model-selection-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline-by-using-the-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-llama-model-with-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-llamastackdistribution-instance/"},"sections":[{"parentId":null,"name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":1,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":null,"name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":1,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-remote-milvus-vector-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-model-stored-in-oci-image-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-distributed-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-vllm-gpu-metrics-dashboard-grafana/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/determining-gpu-requirements-for-llm-powered-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-node-caching-in-elyra/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-an-existing-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/emptying-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-kueue-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":null,"name":"Enabling metrics collection for an existing NIM deployment","level":1,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-feature-store-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-observability-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-kserve-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-metrics-to-external-observability-tools/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/feature-store-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/glossary-of-common-terms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/granting-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-detectors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-hap-scenario/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guidelines-for-metrics-based-autoscaling/"},"sections":[{"parentId":null,"name":"Choosing metrics for latency and throughput-optimized scaling","level":1,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":null,"name":"Choosing the right sliding window","level":1,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":null,"name":"Optimizing HPA scale-down configuration","level":1,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":null,"name":"Considering model size for optimal scaling","level":1,"index":3,"id":"_considering_model_size_for_optimal_scaling"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-querying-using-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-basic-workbench-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ibm-spyre-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/inference-performance-metrics/"},"sections":[{"parentId":null,"name":"Latency","level":1,"index":0,"id":"_latency"},{"parentId":null,"name":"Throughput","level":1,"index":1,"id":"_throughput"},{"parentId":null,"name":"Cost per million tokens","level":1,"index":2,"id":"_cost_per_million_tokens"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ingesting-content-into-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-feature-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/kueue-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job-properties/"},"sections":[{"parentId":null,"name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":1,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-features-available-for-real-time-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates-without-the-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/migrating-pipelines-from-database-to-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/migrating-to-the-rhbok-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/model-serving-runtimes-for-accelerators/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"},{"parentId":null,"name":"IBM Spyre AI accelerators on x86","level":1,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/model-serving-runtimes/"},"sections":[{"parentId":null,"name":"ServingRuntime","level":1,"index":0,"id":"_servingruntime"},{"parentId":null,"name":"InferenceService","level":1,"index":1,"id":"_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/openai-compatibility-for-rag-apis-in-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/openai-compatible-apis-in-llama-stack/"},"sections":[{"parentId":null,"name":"Supported OpenAI-compatible APIs in {productname-short}","level":1,"index":0,"id":"_supported_openai_compatible_apis_in_productname_short"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Chat Completions API","level":2,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Completions API","level":2,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Embeddings API","level":2,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Files API","level":2,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Stores API","level":2,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Store Files API","level":2,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Models API","level":2,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Responses API","level":2,"index":7,"id":"_responses_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-data-science-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":[{"parentId":null,"name":"Distributed workloads infrastructure","level":1,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":null,"name":"Types of distributed workloads","level":1,"index":1,"id":"_types_of_distributed_workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-llama-stack/"},"sections":[{"parentId":null,"name":"The <code>LlamaStackDistribution</code> custom resource API providers","level":1,"index":0,"id":"_the_llamastackdistribution_custom_resource_api_providers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Kueue management states","level":1,"index":0,"id":"_kueue_management_states"},{"parentId":null,"name":"Queue enforcement for projects","level":1,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":null,"name":"Restrictions for managing workloads with Kueue","level":1,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-milvus-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-ml-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-registries/"},"sections":[{"parentId":null,"name":"Model registry","level":1,"index":0,"id":"_model_registry"},{"parentId":null,"name":"Model catalog","level":1,"index":1,"id":"_model_catalog"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-rag/"},"sections":[{"parentId":null,"name":"Audience for RAG","level":1,"index":0,"id":"_audience_for_rag"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/performance-considerations-for-doc-apps/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/performing-model-evaluations-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preparing-documents-with-docling-for-llama-stack-retrieval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/querying-ingested-content-in-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-accelerator-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-cpu-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-default-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-distributed-inference/"},"sections":[{"parentId":null,"name":"Single-node GPU deployment","level":1,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":null,"name":"Multi-node deployment","level":1,"index":1,"id":"_multi_node_deployment"},{"parentId":null,"name":"Intelligent inference scheduler with KV cache routing","level":1,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-ddp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-nccl/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-fsdp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kubernetes-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-oidc-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-pvc-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-grafana-metrics/"},"sections":[{"parentId":null,"name":"Accelerator metrics","level":1,"index":0,"id":"ref-accelerator-metrics_{context}"},{"parentId":null,"name":"CPU metrics","level":1,"index":1,"id":"ref-cpu-metrics_{context}"},{"parentId":null,"name":"vLLM metrics","level":1,"index":2,"id":"ref-vllm-metrics_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"Caikit Standalone ServingRuntime for KServe","level":1,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"TGIS Standalone ServingRuntime for KServe","level":1,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":3,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":1,"index":5,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM AMD GPU ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":1,"index":7,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":8,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Seldon MLServer","level":1,"index":9,"id":"_seldon_mlserver"},{"parentId":null,"name":"Additional resources","level":1,"index":10,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-trainingclient-api-job-related-methods/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-vllm-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-the-ca-bundle-from-a-single-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-the-ca-bundle-from-all-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/retrieving-online-features-for-model-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-jupyter-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-requests-to-the-regex-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-feature-store-UI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval-s3-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-your-working-environment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-available-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/specifying-files-to-ignore/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/specifying-the-data-source-for-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-basic-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-starting-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-your-vllm-model-endpoints/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-workbenches-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s workbench does not start","level":1,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-workbenches-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-Kueue/"},"sections":[{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for Kueue","level":1,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":1,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a \"local_queue provided does not exist\" error message","level":1,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"The pod provisioned by Kueue is terminated before the image is pulled","level":1,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":3,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":4,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"Additional resources","level":1,"index":5,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster does not start","level":1,"index":4,"id":"_my_ray_cluster_does_not_start"},{"parentId":null,"name":"I see a \"Default Local Queue not found\" error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a \"local_queue provided does not exist\" error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSP components","level":1,"index":0,"id":"_common_errors_across_dsp_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-lmeval-job-configuration-using-the-web-console/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-workbench-settings-by-restarting-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-model-files-to-pvc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-custom-unitxt-card/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-hugging-face-prompt-injection-detector-with-the-guardrails-orchestrator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-kserve-inference-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-guardrails-orchestrator-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-hugging-face-models-with-guardrails-orchestrator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-llm-as-a-judge-metrics-with-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-pvcs-as-storage/"},"sections":[{"parentId":null,"name":"Managed PVCs","level":1,"index":0,"id":"_managed_pvcs"},{"parentId":null,"name":"Existing PVCs","level":1,"index":1,"id":"_existing_pvcs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-the-cluster-CA-bundle-for-single-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-the-cluster-server-and-token-to-authenticate/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-feature-store-objects-in-the-web-based-ui/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-installed-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-models-in-the-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-traces-in-external-tracing-platforms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-hardware-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-custom-evaluations-with-LMEval-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-feature-definitions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-kserve-deployment-modes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-llama-stack-search-types/"},"sections":[{"parentId":null,"name":"Supported search modes","level":1,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":2,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":2,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":2,"index":2,"id":"_hybrid_search"},{"parentId":null,"name":"Retrieval database support","level":1,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Single-model serving platform","level":1,"index":0,"id":"_single_model_serving_platform"},{"parentId":null,"name":"Multi-model serving platform","level":1,"index":1,"id":"_multi_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":2,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-organizing-features-by-using-entities/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-persistent-storage/"},"sections":[{"parentId":null,"name":"Storage classes in {productname-short}","level":1,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":null,"name":"Access modes","level":1,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":2,"index":0,"id":"_using_shared_storage_rwx"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authentication-token-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-built-in-alerts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-hugging-face-models-with-an-environment-variable-token/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-administration-interface-for-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-odh-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/activating-the-llama-stack-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-connection-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-model-server-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-certificates-to-a-cluster-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-certificates-to-a-custom-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-feature-definitions-and-initializing-your-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-workbench-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-custom-image-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-workbench-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-workbench-overview/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/audience-for-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/auto-configuring-guardrails/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/before-you-begin/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/benchmarking-embedding-models-with-beir-datasets-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/collecting-metrics-from-user-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/compiling-kubernetes-native-manifests-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/compiling-the-pipeline-yaml-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-certificate-for-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-certificate-for-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-cluster-for-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-inference-service-for-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-inference-service-for-spyre/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-offline-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-online-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-metric-based-autoscaling/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-pipelines-with-your-own-argo-workflows-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-built-in-detector-and-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-codeflare-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-feature-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-guardrails-detector-hugging-face-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator logs","level":1,"index":0,"id":"_viewing_the_productname_short_operator_logs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-workload-management-with-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/controlling-caching-in-data-science-pipelines/"},"sections":[{"parentId":null,"name":"Disabling caching for individual tasks","level":1,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":null,"name":"Disabling caching for a pipeline at submit time","level":1,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":null,"name":"Disabling caching for a pipeline at compile time","level":1,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":null,"name":"Disabling caching for all pipelines (pipeline server)","level":1,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-feature-store-instance-in-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorch-training-script-configmap-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorchjob-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-multi-node-pytorch-training-job-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-for-distributed-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-feature-views/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-project-scoped-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-project-scoped-resources-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-model-selection-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline-by-using-the-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-llamastackdistribution-instance/"},"sections":[{"parentId":null,"name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":1,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":null,"name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":1,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-llama-model-with-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-remote-milvus-vector-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-model-stored-in-oci-image-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-distributed-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-vllm-gpu-metrics-dashboard-grafana/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/determining-gpu-requirements-for-llm-powered-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-node-caching-in-elyra/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-data-science-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-workload-with-a-kueue-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-an-existing-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/emptying-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-kueue-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"},{"parentId":null,"name":"Enabling metrics collection for an existing NIM deployment","level":1,"index":1,"id":"_enabling_metrics_collection_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-feature-store-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-observability-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-kserve-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-metrics-to-external-observability-tools/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/feature-store-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/glossary-of-common-terms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/granting-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-detectors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-querying-using-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-hap-scenario/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guidelines-for-metrics-based-autoscaling/"},"sections":[{"parentId":null,"name":"Choosing metrics for latency and throughput-optimized scaling","level":1,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":null,"name":"Choosing the right sliding window","level":1,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":null,"name":"Optimizing HPA scale-down configuration","level":1,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":null,"name":"Considering model size for optimal scaling","level":1,"index":3,"id":"_considering_model_size_for_optimal_scaling"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-basic-workbench-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ibm-spyre-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-data-science-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/inference-performance-metrics/"},"sections":[{"parentId":null,"name":"Latency","level":1,"index":0,"id":"_latency"},{"parentId":null,"name":"Throughput","level":1,"index":1,"id":"_throughput"},{"parentId":null,"name":"Cost per million tokens","level":1,"index":2,"id":"_cost_per_million_tokens"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ingesting-content-into-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/kueue-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job-properties/"},"sections":[{"parentId":null,"name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":1,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-features-available-for-real-time-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-single-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates-without-the-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/migrating-pipelines-from-database-to-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/model-serving-runtimes-for-accelerators/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"},{"parentId":null,"name":"IBM Spyre AI accelerators on x86","level":1,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/model-serving-runtimes/"},"sections":[{"parentId":null,"name":"ServingRuntime","level":1,"index":0,"id":"_servingruntime"},{"parentId":null,"name":"InferenceService","level":1,"index":1,"id":"_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/migrating-to-the-rhbok-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/openai-compatibility-for-rag-apis-in-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/openai-compatible-apis-in-llama-stack/"},"sections":[{"parentId":null,"name":"Supported OpenAI-compatible APIs in {productname-short}","level":1,"index":0,"id":"_supported_openai_compatible_apis_in_productname_short"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Chat Completions API","level":2,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Completions API","level":2,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Embeddings API","level":2,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Files API","level":2,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Stores API","level":2,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Store Files API","level":2,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Models API","level":2,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Responses API","level":2,"index":7,"id":"_responses_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-data-science-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":[{"parentId":null,"name":"Distributed workloads infrastructure","level":1,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":null,"name":"Types of distributed workloads","level":1,"index":1,"id":"_types_of_distributed_workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-llama-stack/"},"sections":[{"parentId":null,"name":"The <code>LlamaStackDistribution</code> custom resource API providers","level":1,"index":0,"id":"_the_llamastackdistribution_custom_resource_api_providers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Kueue management states","level":1,"index":0,"id":"_kueue_management_states"},{"parentId":null,"name":"Queue enforcement for projects","level":1,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":null,"name":"Restrictions for managing workloads with Kueue","level":1,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-milvus-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-ml-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-registries/"},"sections":[{"parentId":null,"name":"Model registry","level":1,"index":0,"id":"_model_registry"},{"parentId":null,"name":"Model catalog","level":1,"index":1,"id":"_model_catalog"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-rag/"},"sections":[{"parentId":null,"name":"Audience for RAG","level":1,"index":0,"id":"_audience_for_rag"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/performance-considerations-for-doc-apps/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/performing-model-evaluations-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preparing-documents-with-docling-for-llama-stack-retrieval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/querying-ingested-content-in-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-default-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-distributed-inference/"},"sections":[{"parentId":null,"name":"Single-node GPU deployment","level":1,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":null,"name":"Multi-node deployment","level":1,"index":1,"id":"_multi_node_deployment"},{"parentId":null,"name":"Intelligent inference scheduler with KV cache routing","level":1,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-ddp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-fsdp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-nccl/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kubernetes-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-pvc-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-grafana-metrics/"},"sections":[{"parentId":null,"name":"Accelerator metrics","level":1,"index":0,"id":"ref-accelerator-metrics_{context}"},{"parentId":null,"name":"CPU metrics","level":1,"index":1,"id":"ref-cpu-metrics_{context}"},{"parentId":null,"name":"vLLM metrics","level":1,"index":2,"id":"ref-vllm-metrics_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-oidc-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"Caikit Standalone ServingRuntime for KServe","level":1,"index":1,"id":"_caikit_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"TGIS Standalone ServingRuntime for KServe","level":1,"index":2,"id":"_tgis_standalone_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":3,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":1,"index":5,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM AMD GPU ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":1,"index":7,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":8,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Seldon MLServer","level":1,"index":9,"id":"_seldon_mlserver"},{"parentId":null,"name":"Additional resources","level":1,"index":10,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-trainingclient-api-job-related-methods/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-vllm-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-the-ca-bundle-from-a-single-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-the-ca-bundle-from-all-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/resolving-cuda-oom-errors/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/retrieving-online-features-for-model-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-custom-evaluations-with-LMEval-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-workload-with-a-kueue-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ds-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-jupyter-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-requests-to-the-regex-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-feature-store-UI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval-s3-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-your-working-environment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-available-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/specifying-files-to-ignore/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/specifying-the-data-source-for-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-basic-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-data-with-data-science-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-starting-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-your-vllm-model-endpoints/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-workbenches-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s workbench does not start","level":1,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-workbenches-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-Kueue/"},"sections":[{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for Kueue","level":1,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":1,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a \"local_queue provided does not exist\" error message","level":1,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"The pod provisioned by Kueue is terminated before the image is pulled","level":1,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for the CodeFlare Operator","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for Kueue","level":1,"index":3,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster does not start","level":1,"index":4,"id":"_my_ray_cluster_does_not_start"},{"parentId":null,"name":"I see a \"Default Local Queue not found\" error message","level":1,"index":5,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a \"local_queue provided does not exist\" error message","level":1,"index":6,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":7,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":8,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSP components","level":1,"index":0,"id":"_common_errors_across_dsp_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for the CodeFlare Operator","level":1,"index":2,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_the_codeflare_operator"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":3,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":4,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"Additional resources","level":1,"index":5,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-data-science-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-an-accelerator-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-lmeval-job-configuration-using-the-web-console/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-workbench-settings-by-restarting-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-model-files-to-pvc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-custom-unitxt-card/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-hugging-face-prompt-injection-detector-with-the-guardrails-orchestrator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-kserve-inference-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-guardrails-orchestrator-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-hugging-face-models-with-guardrails-orchestrator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-llm-as-a-judge-metrics-with-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-pvcs-as-storage/"},"sections":[{"parentId":null,"name":"Managed PVCs","level":1,"index":0,"id":"_managed_pvcs"},{"parentId":null,"name":"Existing PVCs","level":1,"index":1,"id":"_existing_pvcs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-the-cluster-CA-bundle-for-single-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-the-cluster-server-and-token-to-authenticate/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-feature-store-objects-in-the-web-based-ui/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-installed-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-multi-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-models-in-the-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-traces-in-external-tracing-platforms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-accelerator-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-hardware-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-cpu-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-accelerator-metrics/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#installing-odh-v2_installv2\">Installing Open Data Hub version 2</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#configuring-custom-namespaces\">Configuring custom namespaces</a></li>\n<li><a href=\"#installing-the-odh-operator-v2_installv2\">Installing the Open Data Hub Operator version 2</a></li>\n<li><a href=\"#installing-odh-components_installv2\">Installing Open Data Hub components</a></li>\n</ul>\n</li>\n<li><a href=\"#configuring-pipelines-with-your-own-argo-workflows-instance_install\">Configuring pipelines with your own Argo Workflows instance</a></li>\n<li><a href=\"#installing-the-distributed-workloads-components_install\">Installing the distributed workloads components</a></li>\n<li><a href=\"#accessing-the-odh-dashboard_install\">Accessing the Open Data Hub dashboard</a></li>\n<li><a href=\"#working-with-certificates_certs\">Working with certificates</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#understanding-certificates_certs\">Understanding how Open Data Hub handles certificates</a></li>\n<li><a href=\"#_adding_certificates\">Adding certificates</a></li>\n<li><a href=\"#adding-certificates-to-a-cluster-ca-bundle_certs\">Adding certificates to a cluster-wide CA bundle</a></li>\n<li><a href=\"#adding-certificates-to-a-custom-ca-bundle_certs\">Adding certificates to a custom CA bundle</a></li>\n<li><a href=\"#_using_self_signed_certificates_with_open_data_hub_components\">Using self-signed certificates with Open Data Hub components</a></li>\n<li><a href=\"#managing-certificates-without-the-operator_certs\">Managing certificates without the Open Data Hub Operator</a></li>\n<li><a href=\"#_removing_the_ca_bundle\">Removing the CA bundle</a></li>\n</ul>\n</li>\n<li><a href=\"#viewing-logs-and-audit-records_install\">Viewing logs and audit records</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#configuring-the-operator-logger_install\">Configuring the Open Data Hub Operator logger</a></li>\n<li><a href=\"#viewing-audit-records_install\">Viewing audit records</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"sect1\">\n<h2 id=\"installing-odh-v2_installv2\">Installing Open Data Hub version 2</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can install Open Data Hub version 2 on OpenShift Container Platform from the OpenShift web console.\nFor information about upgrading the Open Data Hub Operator, see <a href=\"https://opendatahub.io/docs/upgrading-open-data-hub\">Upgrading Open Data Hub</a>.</p>\n</div>\n<div class=\"paragraph\">\n<p>Installing Open Data Hub involves the following tasks:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Optional: Configuring custom namespaces.</p>\n</li>\n<li>\n<p>Installing the Open Data Hub Operator.</p>\n</li>\n<li>\n<p>Installing Open Data Hub components.</p>\n</li>\n<li>\n<p>Accessing the Open Data Hub dashboard.</p>\n</li>\n</ol>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-custom-namespaces\">Configuring custom namespaces</h3>\n<div class=\"paragraph _abstract\">\n<p>By default, Open Data Hub uses the following predefined namespaces:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>openshift-operators</code> contains the Open Data Hub Operator</p>\n</li>\n<li>\n<p><code>opendatahub</code> includes the dashboard and other required components of Open Data Hub</p>\n</li>\n<li>\n<p><code>opendatahub</code> is where basic workbenches are deployed by default</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>If needed, you can define custom namespaces to use instead of the predefined ones before installing Open Data Hub. This flexibility supports environments with naming policies or conventions and allows cluster administrators to control where components such as workbenches are deployed.</p>\n</div>\n<div class=\"paragraph\">\n<p>Namespaces created by Open Data Hub typically include <code>openshift</code> or <code>opendatahub</code> in their name. Do not rename these system namespaces because they are required for Open Data Hub to function properly.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have access to an Open Data Hub cluster with cluster administrator privileges.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have not yet installed the Open Data Hub Operator.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI (<code>oc</code>) as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Optional: To configure a custom operator namespace:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a namespace YAML file named <code>operator-namespace.yaml</code>.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-YAML\" data-lang=\"YAML\">apiVersion: v1\nkind: Namespace\nmetadata:\n  name: &lt;operator-namespace&gt; <b class=\"conum\">(1)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>Defines the operator namespace.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create the namespace in your OpenShift Container Platform cluster.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc create -f operator-namespace.yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>namespace/<em>&lt;operator-namespace&gt;</em> created</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>When you install the Open Data Hub Operator, use this namespace instead of <code>openshift-operators</code>.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: To configure a custom applications namespace:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a namespace YAML file named <code>applications-namespace.yaml</code>.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-YAML\" data-lang=\"YAML\">apiVersion: v1\nkind: Namespace\nmetadata:\n  name: &lt;applications-namespace&gt; <b class=\"conum\">(1)</b>\n  labels:\n    opendatahub.io/application-namespace: 'true' <b class=\"conum\">(2)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>Defines the applications namespace.</p>\n</li>\n<li>\n<p>Adds the required label.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create the namespace in your OpenShift Container Platform cluster.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc create -f applications-namespace.yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>namespace/<em>&lt;applications-namespace&gt;</em> created</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Optional: To configure a custom workbench namespace:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a namespace YAML file named <code>workbench-namespace.yaml</code>.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-YAML\" data-lang=\"YAML\">apiVersion: v1\nkind: Namespace\nmetadata:\n  name: &lt;workbench-namespace&gt; <b class=\"conum\">(1)</b></code></pre>\n</div>\n</div>\n<div class=\"colist arabic\">\n<ol>\n<li>\n<p>Defines the workbench namespace.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Create the namespace in your OpenShift Container Platform cluster.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc create -f workbench-namespace.yaml</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>You see output similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>namespace/<em>&lt;workbench-namespace&gt;</em> created</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>When you install the Open Data Hub components, specify this namespace for the <code>spec.workbenches.workbenchNamespace</code> field. You cannot change the default workbench namespace after you have installed the Open Data Hub Operator.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Next step</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/installing-open-data-hub/#installing-the-odh-operator-v2_installv2\">Installing the Open Data Hub Operator version 2</a>.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"installing-the-odh-operator-v2_installv2\">Installing the Open Data Hub Operator version 2</h3>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You are using OpenShift Container Platform 4.16 or later.</p>\n</li>\n<li>\n<p>Your OpenShift Container Platform cluster has a minimum of 16 CPUs and 32GB of memory across all OpenShift Container Platform worker nodes.</p>\n</li>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>If you are using custom namespaces, you have created and labeled them as required.</p>\n</li>\n<li>\n<p>If you are using Open Data Hub on a cluster running in FIPS mode, any custom container images for data science pipelines must be based on UBI 9 or RHEL 9. This ensures compatibility with FIPS-approved pipeline components and prevents errors related to mismatched OpenSSL or GNU C Library (glibc) versions.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to your OpenShift Container Platform as a user with <code>cluster-admin</code> privileges. If you are performing a developer installation on <a href=\"http://try.openshift.com\">try.openshift.com</a>, you can log in as the <code>kubeadmin</code> user.</p>\n</li>\n<li>\n<p>Select <strong>Operators</strong> &#8594; <strong>OperatorHub</strong>.</p>\n</li>\n<li>\n<p>On the <strong>OperatorHub</strong> page, in the <strong>Filter by keyword</strong> field, enter <code>Open Data Hub Operator</code>.</p>\n</li>\n<li>\n<p>Click the <strong>Open Data Hub Operator</strong> tile.</p>\n</li>\n<li>\n<p>If the <strong>Show community Operator</strong> window opens, read the information and then click <strong>Continue</strong>.</p>\n</li>\n<li>\n<p>Read the information about the Operator and then click <strong>Install</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Install Operator</strong> page, follow these steps:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>For <strong>Update</strong> channel, select <strong>fast</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Version 2 of the Open Data Hub Operator represents an alpha release, accessible only on the <strong>fast</strong> channel. Later releases will change to the <strong>rolling</strong> channel when the Operator is more stable.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>For <strong>Version</strong>, select the version of the Operator that you want to install.</p>\n</li>\n<li>\n<p>For <strong>Installation mode</strong>, leave <strong>All namespaces on the cluster (default)</strong> selected.</p>\n</li>\n<li>\n<p>For <strong>Installed Namespace</strong>, select the <strong>openshift-operators</strong> namespace or select your custom operator namespace.</p>\n</li>\n<li>\n<p>For <strong>Update approval</strong>, select automatic or manual updates.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Automatic</strong>: When a new version of the Operator is available, Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator.</p>\n</li>\n<li>\n<p><strong>Manual</strong>: When a new version of the Operator is available, OLM notifies you with an update request that you must manually approve to upgrade the running instance of your Operator.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Install</strong>. The installation might take a few minutes.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Select <strong>Operators</strong> &#8594; <strong>Installed Operators</strong> to verify that the <strong>Open Data Hub Operator</strong> is listed with <strong>Succeeded</strong> status.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Next step</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/installing-open-data-hub/#installing-odh-components_installv2\">Installing Open Data Hub components</a></p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/installing-open-data-hub/#configuring-the-operator-logger_install\">Configuring the Open Data Hub Operator logger</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"installing-odh-components_installv2\">Installing Open Data Hub components</h3>\n<div class=\"paragraph _abstract\">\n<p>You can use the OpenShift web console to install specific components of Open Data Hub on your cluster when the Open Data Hub Operator is already installed on the cluster.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the Open Data Hub Operator.</p>\n</li>\n<li>\n<p>You can log in as a user with <code>cluster-admin</code> privileges.</p>\n</li>\n<li>\n<p>If you want to use the <code>trustyai</code> component, you must configure workload monitoring as described in <a href=\"https://opendatahub.io/docs/managing-and-monitoring-models/#_monitoring_models_on_the_single_model_serving_platform\">Monitoring models on the single-model serving platform</a>.</p>\n</li>\n<li>\n<p>If you want to use the <code>RAG</code> component, your infrastructure supports GPU-enabled instance types, for example, <code>g4dn.xlarge</code> on AWS.</p>\n</li>\n<li>\n<p>If you want to use the <code>kserve</code> or <code>modelmesh</code> components, you must have already installed the following Operator or Operators for the component. For information about installing an Operator, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/operators/administrator-tasks#olm-adding-operators-to-a-cluster\">Adding Operators to a cluster</a>.</p>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The multi-model serving platform based on ModelMesh is deprecated.\nYou can continue to deploy models on the multi-model serving platform, but it is recommended that you migrate to the single-model serving platform.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>If you want to use <code>kserve</code>, you have selected a deployment mode. For more information, see <a href=\"https://opendatahub.io/docs/serving-models/#about-kserve-deployment-modes_serving-large-models\">About KServe deployment modes</a>.</p>\n</li>\n<li>\n<p>If you want to use the <code>kueue</code> component to manage workloads, you must install the Red Hat build of Kueue Operator before activating the Kueue integration. For more information, see <a href=\"https://opendatahub.io/docs/managing-odh/#configuring-workload-management-with-kueue_kueue\">Configuring workload management with Kueue</a>.</p>\n</li>\n</ul>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. Required Operators for components</caption>\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 33.3334%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Component</th>\n<th class=\"tableblock halign-left valign-top\">Required Operators</th>\n<th class=\"tableblock halign-left valign-top\">Catalog</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">kserve</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Red Hat OpenShift Serverless Operator, Red Hat OpenShift Service Mesh Operator, Red Hat Authorino Operator</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Red Hat</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">kueue</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Red Hat build of Kueue Operator</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Red Hat</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">[Deprecated] modelmesh</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Prometheus Operator</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Community</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">RAG (Llama Stack)</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Llama Stack Operator, Node Feature Discovery Operator, NVIDIA GPU Operator</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Red Hat</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to your OpenShift Container Platform cluster as a user with <code>cluster-admin</code> privileges. If you are performing a developer installation on <a href=\"http://try.openshift.com\">try.openshift.com</a>, you can log in as the <code>kubeadmin</code> user.</p>\n</li>\n<li>\n<p>Select <strong>Operators</strong> &#8594; <strong>Installed Operators</strong>, and then click the <strong>Open Data Hub Operator</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Operator details</strong> page, click the <strong>DSC Initialization</strong> tab, and then click <strong>Create DSCInitialization</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Create DSCInitialization</strong> page, configure by using the <strong>YAML</strong> view.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>If you are using a custom applications namespace, specify the namespace in the <code>spec.applicationsNamespace</code> field.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n</li>\n<li>\n<p>Wait until the status of the DSCInitialization is <strong>Ready</strong>.</p>\n</li>\n<li>\n<p>Click the <strong>Data Science Cluster</strong> tab, and then click <strong>Create DataScienceCluster</strong>.</p>\n</li>\n<li>\n<p>On the <strong>Create DataScienceCluster</strong> page, configure by using the <strong>YAML</strong> view.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <code>spec.components</code> section, for each component shown, set the value of the <code>managementState</code> field to <code>Managed</code>, <code>Removed</code>, or <code>Unmanaged</code>.</p>\n</li>\n<li>\n<p>If you are using a custom workbench namespace, specify the namespace in the <code>spec.workbenches.workbenchNamespace</code> field.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Create</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>Select <strong>Home</strong> &#8594; <strong>Projects</strong>, and then select the <strong>opendatahub</strong> project.</p>\n</li>\n<li>\n<p>On the <strong>Project details</strong> page, click the <strong>Workloads</strong> tab and confirm that the Open Data Hub core components are running.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p><strong>Note:</strong> In the Open Data Hub dashboard, users can view the list of the installed Open Data Hub components, their corresponding source (upstream) components, and the versions of the installed components, as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#viewing-installed-components_get-started\">Viewing installed Open Data Hub components</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Next steps</div>\n<ul>\n<li>\n<p>Optional: <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#configuring-pipelines-with-your-own-argo-workflows-instance_install\">Configuring pipelines with your own Argo Workflows instance</a></p>\n</li>\n<li>\n<p>Optional: <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#installing-the-distributed-workloads-components_install\">Installing the distributed workloads components</a></p>\n</li>\n<li>\n<p><a href=\"https://opendatahub.io/docs/installing-open-data-hub/#accessing-the-odh-dashboard_installv2\">Accessing the Open Data Hub dashboard</a></p>\n</li>\n</ul>\n</div>\n<div class=\"ulist _additional-resources\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/installing-open-data-hub/#configuring-the-operator-logger_install\">Configuring the Open Data Hub Operator logger</a></p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"configuring-pipelines-with-your-own-argo-workflows-instance_install\">Configuring pipelines with your own Argo Workflows instance</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can configure Open Data Hub to use an existing Argo Workflows instance instead of the embedded one included with Data Science Pipelines. This configuration is useful if your OpenShift Container Platform cluster already includes a managed Argo Workflows instance and you want to integrate it with Open Data Hub pipelines without conflicts. Disabling the embedded Argo Workflows controller allows cluster administrators to manage the lifecycles of Open Data Hub and Argo Workflows independently.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You cannot enable both the embedded Argo Workflows instance and your own Argo Workflows instance on the same cluster.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform web console as a cluster administrator.</p>\n</li>\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Operators</strong>  <strong>Installed Operators</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>Open Data Hub</strong> Operator, and then click the Operator name to open the Operator details page.</p>\n</li>\n<li>\n<p>Click the <strong>Data Science Cluster</strong> tab.</p>\n</li>\n<li>\n<p>Click the default instance name (for example, <strong>default-dsc</strong>) to open the instance details page.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab to show the instance specifications.</p>\n</li>\n<li>\n<p>Disable the embedded Argo Workflows controllers that are managed by the Open Data Hub Operator:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <code>spec.components</code> section, set the value of the <code>managementState</code> field for the <code>datasciencepipelines</code> component to <code>Managed</code>.</p>\n</li>\n<li>\n<p>In the <code>spec.components.datasciencepipelines</code> section, set the value of the <code>managementState</code> field for <code>argoWorkflowsControllers</code> to <code>Removed</code>, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example datasciencepipelines specification</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\"># ...\nspec:\n  components:\n    datasciencepipelines:\n      argoWorkflowsControllers:\n        managementState: Removed\n      managementState: Managed\n# ...</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong> to apply your changes.</p>\n</li>\n<li>\n<p>Install and configure a compatible version of Argo Workflows on your cluster. For compatible version information, see <a href=\"https://access.redhat.com/articles/rhoai-supported-configs\">Supported Configurations</a>. For installation information, see the <a href=\"https://argo-workflows.readthedocs.io/en/stable/installation/\" target=\"_blank\" rel=\"noopener\">Argo Workflows Installation documentation</a>.</p>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>On the <strong>Details</strong> tab of the <code>DataScienceCluster</code> instance (for example, <strong>default-dsc</strong>), verify that <code>DataSciencePipelinesReady</code> has a <strong>Status</strong> of <code>True</code>.</p>\n</li>\n<li>\n<p>Verify that the <code>ds-pipeline-workflow-controller</code> pod does not exist:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Go to <strong>Workloads</strong> &#8594; <strong>Pods</strong>.</p>\n</li>\n<li>\n<p>Search for the <code>ds-pipeline-workflow-controller</code> pod.</p>\n</li>\n<li>\n<p>Verify that this pod does not exist. The absence of this pod confirms that the embedded Argo Workflows controller is disabled.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"installing-the-distributed-workloads-components_install\">Installing the distributed workloads components</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To use the distributed workloads feature in Open Data Hub, you must install several components.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to OpenShift Container Platform with the <code>cluster-admin</code> role and you can access the data science cluster.</p>\n</li>\n<li>\n<p>You have installed Open Data Hub.</p>\n</li>\n<li>\n<p>You have installed the Red Hat build of Kueue Operator on your OpenShift Container Platform cluster, as described in the <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/ai_workloads/red-hat-build-of-kueue\">Red Hat build of Kueue documentation</a>.</p>\n</li>\n<li>\n<p>You have sufficient resources. In addition to the minimum Open Data Hub resources described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#installing-the-odh-operator-v2_installv2\">Installing the Open Data Hub Operator version 2</a>, you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.</p>\n</li>\n<li>\n<p>You have removed any previously installed instances of the CodeFlare Operator.</p>\n</li>\n<li>\n<p>If you want to use graphics processing units (GPUs), you have enabled GPU support.\nThis process includes installing the Node Feature Discovery Operator and the relevant GPU Operator.\nFor more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the NVIDIA documentation for NVIDIA GPUs and <a href=\"https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html\" target=\"_blank\" rel=\"noopener\">AMD GPU Operator on Red&#160;Hat OpenShift Container Platform</a> in the AMD documentation for AMD GPUs.</p>\n</li>\n<li>\n<p>If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#understanding-certificates_certs\">Understanding how Open Data Hub handles certificates</a>.\nNo additional configuration is necessary to use those certificates with distributed workloads.\nThe centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Cluster-wide CA bundle:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">/etc/pki/tls/certs/odh-trusted-ca-bundle.crt\n/etc/ssl/certs/odh-trusted-ca-bundle.crt</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Custom CA bundle:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">/etc/pki/tls/certs/odh-ca-bundle.crt\n/etc/ssl/certs/odh-ca-bundle.crt</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Operators</strong> &#8594; <strong>Installed Operators</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>Open Data Hub Operator</strong>, and then click the Operator name to open the Operator details page.</p>\n</li>\n<li>\n<p>Click the <strong>Data Science Cluster</strong> tab.</p>\n</li>\n<li>\n<p>Click the default instance name (for example, <strong>default-dsc</strong>) to open the instance details page.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab to show the instance specifications.</p>\n</li>\n<li>\n<p>Enable the required distributed workloads components.\nIn the <code>spec.components</code> section, set the <code>managementState</code> field correctly for the required components:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Set <code>kueue</code> to <code>Unmanaged</code> to allow the Red Hat build of Kueue Operator to manage Kueue.</p>\n</li>\n<li>\n<p>If you want to use the CodeFlare framework to tune models, set <code>codeflare</code> and <code>ray</code> to <code>Managed</code>.</p>\n</li>\n<li>\n<p>If you want to use the Kubeflow Training Operator to tune models, set <code>trainingoperator</code> to <code>Managed</code>.</p>\n</li>\n<li>\n<p>The list of required components depends on whether the distributed workload is run from a pipeline or workbench or both, as shown in the following table.</p>\n</li>\n</ul>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 2. Components required for distributed workloads</caption>\n<colgroup>\n<col style=\"width: 44%;\">\n<col style=\"width: 15%;\">\n<col style=\"width: 15%;\">\n<col style=\"width: 26%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Component</th>\n<th class=\"tableblock halign-left valign-top\">Pipelines only</th>\n<th class=\"tableblock halign-left valign-top\">Workbenches only</th>\n<th class=\"tableblock halign-left valign-top\">Pipelines and workbenches</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>codeflare</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>dashboard</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>datasciencepipelines</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Removed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>kueue</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Unmanaged</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Unmanaged</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Unmanaged</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>ray</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>trainingoperator</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>workbenches</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Removed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Managed</code></p></td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>\n<p>Click <strong>Save</strong>.\nAfter a short time, the components with a <code>Managed</code> state are ready.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Check the status of the <strong>codeflare-operator-manager</strong>, <strong>kubeflow-training-operator</strong>, <strong>kuberay-operator</strong>, <strong>kueue-controller-manager</strong>, and <strong>openshift-kueue-operator</strong> pods, as follows:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Workloads</strong> &#8594; <strong>Deployments</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Search by name</strong> field, enter the following search strings:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>In the <strong>opendatahub</strong> project, search for <strong>codeflare-operator-manager</strong>, <strong>kubeflow-training-operator</strong>, and <strong>kuberay-operator</strong>.</p>\n</li>\n<li>\n<p>In the <strong>openshift-kueue-operator</strong> project, search for <strong>kueue-controller-manager</strong> and <strong>openshift-kueue-operator</strong>.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In each case, check the status as follows:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click the deployment name to open the deployment details page.</p>\n</li>\n<li>\n<p>Click the <strong>Pods</strong> tab.</p>\n</li>\n<li>\n<p>Check the pod status.</p>\n<div class=\"paragraph\">\n<p>When the status of the pods is <strong>Running</strong>, the pods are ready to use.</p>\n</div>\n</li>\n<li>\n<p>To see more information about each pod, click the pod name to open the pod details page, and then click the <strong>Logs</strong> tab.</p>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Next Step</div>\n<p>Configure the distributed workloads feature as described in <a href=\"https://opendatahub.io/docs/managing-odh/#managing-distributed-workloads_managing-odh\">Managing distributed workloads</a>.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"accessing-the-odh-dashboard_install\">Accessing the Open Data Hub dashboard</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>After you have installed Open Data Hub, you can access and share the URL for your Open Data Hub dashboard with other users to let them log in and work on their models.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed the Open Data Hub Operator.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to OpenShift Container Platform web console.</p>\n</li>\n<li>\n<p>Click the application launcher (<span class=\"image\"><img src=\"/static/docs/images/osd-app-launcher.png\" alt=\"The application launcher\"></span>).</p>\n</li>\n<li>\n<p>Right-click <strong>Open Data Hub</strong> and copy the URL for your Open Data Hub instance.</p>\n</li>\n<li>\n<p>Give this URL to your users to let them log in to Open Data Hub dashboard.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Confirm that you and your users can log in to the Open Data Hub dashboard by using the URL.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p><strong>Note:</strong> In the Open Data Hub dashboard, users can view the list of the installed Open Data Hub components, their corresponding source (upstream) components, and the versions of the installed components, as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#viewing-installed-components_get-started\">Viewing installed Open Data Hub components</a>.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"working-with-certificates_certs\">Working with certificates</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>When you install Open Data Hub, OpenShift automatically applies a default Certificate Authority (CA) bundle to manage authentication for most Open Data Hub components, such as workbenches and model servers. These certificates are trusted self-signed certificates that help secure communication. However, as a cluster administrator, you might need to configure additional self-signed certificates to use some components, such as the data science pipeline server and object storage solutions. If an Open Data Hub component uses a self-signed certificate that is not part of the existing cluster-wide CA bundle, you have the following options for including the certificate:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Add it to the OpenShift cluster-wide CA bundle.</p>\n</li>\n<li>\n<p>Add it to a custom CA bundle, separate from the cluster-wide CA bundle.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>As a cluster administrator, you can also change how to manage authentication for Open Data Hub as follows:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Manually manage certificate changes, instead of relying on the Open Data Hub Operator to handle them automatically.</p>\n</li>\n<li>\n<p>Remove the cluster-wide CA bundle, either from all namespaces or specific ones. If you prefer to implement a different authentication approach, other than using CA bundles, you can override the default Open Data Hub behavior, as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#removing-the-ca-bundle-from-all-namespaces_certs\">Removing the CA bundle from all namespaces</a> and <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#removing-the-ca-bundle-from-a-single-namespace_certs\">Removing the CA bundle from a single namespace</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect2\">\n<h3 id=\"understanding-certificates_certs\">Understanding how Open Data Hub handles certificates</h3>\n<div class=\"paragraph _abstract\">\n<p>After installing Open Data Hub, the Open Data Hub Operator automatically creates an empty <code>odh-trusted-ca-bundle</code> configuration file (ConfigMap). The Cluster Network Operator (CNO) injects the cluster-wide CA bundle into the <code>odh-trusted-ca-bundle</code> configMap with the label <code>\"config.openshift.io/inject-trusted-cabundle\"</code>.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: opendatahub-operator\n    config.openshift.io/inject-trusted-cabundle: 'true'\n  name: odh-trusted-ca-bundle</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>After the CNO operator injects the bundle, it updates the ConfigMap with the contents of the <code>ca-bundle.crt</code> file.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: opendatahub-operator\n    config.openshift.io/inject-trusted-cabundle: 'true'\n  name: odh-trusted-ca-bundle\ndata:\n  ca-bundle.crt: |\n    &lt;BUNDLE OF CLUSTER-WIDE CERTIFICATES&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The management of CA bundles is configured through the Data Science Cluster Initialization (DSCI) object. Within this object, you can set the <code>spec.trustedCABundle.managementState</code> field to one of the following values:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>Managed</code>: (Default) The Open Data Hub Operator manages the <code>odh-trusted-ca-bundle</code> ConfigMap and adds it to all non-reserved existing and new namespaces. It does not add the ConfigMap to any reserved or system namespaces, such as <code>default</code>, <code>openshift-\\*</code> or <code>kube-*</code>. The Open Data Hub Operator automatically updates the ConfigMap to reflect any changes made to the <code>customCABundle</code> field.</p>\n</li>\n<li>\n<p><code>Unmanaged</code>: The Open Data Hub administrator manually manages the <code>odh-trusted-ca-bundle</code> ConfigMap, instead of allowing the Operator to manage it. Changing the <code>managementState</code> from <code>Managed</code> to <code>Unmanaged</code> does not remove the <code>odh-trusted-ca-bundle</code> ConfigMap. However, the ConfigMap is no longer automatically updated if changes are made to the <code>customCABundle</code> field.</p>\n<div class=\"paragraph\">\n<p>The <code>Unmanaged</code> setting is useful if your organization implements a different method for managing trusted CA bundles, such as Ansible automation, and does not want the Open Data Hub Operator to handle certificates automatically. This setting provides greater control, preventing the Operator from overwriting custom configurations.</p>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_adding_certificates\">Adding certificates</h3>\n<div class=\"paragraph\">\n<p>If you must use a self-signed certificate that is not part of the existing cluster-wide CA bundle, you have two options for configuring the certificate:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Add it to the cluster-wide CA bundle.</p>\n<div class=\"paragraph\">\n<p>This option is useful when the certificate is needed for secure communication across multiple services or when it&#8217;s required by security policies to be trusted cluster-wide. This option ensures that all services and components in the cluster trust the certificate automatically. It simplifies management because the certificate is trusted across the entire cluster, avoiding the need to configure the certificate separately for each service.</p>\n</div>\n</li>\n<li>\n<p>Add it to a custom CA bundle that is separate from the OpenShift cluster-wide bundle.</p>\n<div class=\"paragraph\">\n<p>Consider this option for the following scenarios:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Limit scope: Only specific services need the certificate, not the whole cluster.</p>\n</li>\n<li>\n<p>Isolation: Keeps custom certificates separate, preventing changes to the global configuration.</p>\n</li>\n<li>\n<p>Avoid global impact: Does not affect services that do not need the certificate.</p>\n</li>\n<li>\n<p>Easier management: Makes it simpler to manage certificates for specific services.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"adding-certificates-to-a-cluster-ca-bundle_certs\">Adding certificates to a cluster-wide CA bundle</h3>\n<div class=\"paragraph _abstract\">\n<p>You can add a self-signed certificate to a cluster-wide Certificate Authority (CA) bundle (<code>ca-bundle.crt</code>).</p>\n</div>\n<div class=\"paragraph\">\n<p>When the cluster-wide CA bundle is updated, the Cluster Network Operator (CNO) automatically detects the change and injects the updated bundle into the <code>odh-trusted-ca-bundle</code> ConfigMap, making the certificate available to Open Data Hub components.</p>\n</div>\n<div class=\"paragraph\">\n<p><strong>Note:</strong> By default, the management state for the Trusted CA bundle is <code>Managed</code> (that is, the <code>spec.trustedCABundle.managementState</code> field in the Open Data Hub Operator&#8217;s DSCI object is set to <code>Managed</code>). If you change this setting to <code>Unmanaged</code>, you must manually update the <code>odh-trusted-ca-bundle</code> ConfigMap to include the updated cluster-wide CA bundle.</p>\n</div>\n<div class=\"paragraph\">\n<p>Alternatively, you can add certificates to a custom CA bundle, as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#adding-certificates-to-a-custom-ca-bundle_certs\">Adding certificates to a custom CA bundle</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have created a self-signed certificate and saved the certificate to a file. For example, you have created a certificate using OpenSSL and saved it to a file named <code>example-ca.crt</code>.</p>\n</li>\n<li>\n<p>You have cluster administrator access for the OpenShift cluster where Open Data Hub is installed.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create a ConfigMap that includes the root CA certificate used to sign the certificate, where <code>&lt;/path/to/example-ca.crt&gt;</code> is the path to the CA certificate bundle on your local file system:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc create configmap custom-ca \\\n \t--from-file=ca-bundle.crt=&lt;/path/to/example-ca.crt&gt; \\\n \t-n openshift-config</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Update the cluster-wide proxy configuration with the newly-created ConfigMap:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc patch proxy/cluster \\\n    \t --type=merge \\\n   \t --patch='{\"spec\":{\"trustedCA\":{\"name\":\"custom-ca\"}}}'</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Run the following command to verify that all non-reserved namespaces contain the <code>odh-trusted-ca-bundle</code> ConfigMap:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get configmaps --all-namespaces -l app.kubernetes.io/part-of=opendatahub-operator | grep odh-trusted-ca-bundle</code></pre>\n</div>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Additional resources</div>\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/security_and_compliance/configuring-certificates\">Configuring certificates</a> in the OpenShift Container Platform documentation</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html-single/operators/index#olm-inject-custom-ca_olm-configuring-proxy-support\">Injecting a custom CA Bundle</a> in the Red&#160;Hat OpenShift Service on AWS documentation</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_dedicated/4/html/operators/administrator-tasks#olm-inject-custom-ca_olm-configuring-proxy-support\">Injecting a custom CA Bundle</a> in the OpenShift Dedicated documentation</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"adding-certificates-to-a-custom-ca-bundle_certs\">Adding certificates to a custom CA bundle</h3>\n<div class=\"paragraph _abstract\">\n<p>You can add self-signed certificates to a custom CA bundle that is separate from the OpenShift cluster-wide bundle.</p>\n</div>\n<div class=\"paragraph\">\n<p>This method is ideal for scenarios where components need access to external resources that require a self-signed certificate. For example, you may need to add self-signed certificates to grant data science pipelines access S3-compatible object storage.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have created a self-signed certificate and saved the certificate to a file. For example, you have created a certificate using OpenSSL and saved it to a file named <code>example-ca.crt</code>.</p>\n</li>\n<li>\n<p>You have cluster administrator access for the OpenShift cluster where Open Data Hub is installed.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to OpenShift Container Platform.</p>\n</li>\n<li>\n<p>Click <strong>Operators</strong>  <strong>Installed Operators</strong> and then click the Open Data Hub Operator.</p>\n</li>\n<li>\n<p>Click the <strong>DSC Initialization</strong> tab.</p>\n</li>\n<li>\n<p>Click the <strong>default-dsci</strong> object.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>In the <code>spec.trustedCABundle</code> section, add the custom certificate to the <code>customCABundle</code> field, as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>spec:\n  trustedCABundle:\n    managementState: Managed\n    customCABundle: |\n      -----BEGIN CERTIFICATE-----\n      examplebundle123\n      -----END CERTIFICATE-----</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>The Open Data Hub Operator automatically updates the ConfigMap to reflect any changes made to the <code>customCABundle</code> field. It adds the <code>odh-ca-bundle.crt</code> file containing the certificates to the <code>odh-trusted-ca-bundle</code> ConfigMap, as shown in the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: opendatahub-operator\n    config.openshift.io/inject-trusted-cabundle: 'true'\n  name: odh-trusted-ca-bundle\ndata:\n  ca-bundle.crt: |\n    &lt;BUNDLE OF CLUSTER-WIDE CERTIFICATES&gt;\n  odh-ca-bundle.crt: |\n    &lt;BUNDLE OF CUSTOM CERTIFICATES&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Run the following command to verify that a non-reserved namespace contains the <code>odh-trusted-ca-bundle</code> ConfigMap and that the ConfigMap contains your <code>customCABundle</code> value. In the following command, <em>example-namespace</em> is the non-reserved namespace and <em>examplebundle123</em> is the <code>customCABundle</code> value.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get configmap odh-trusted-ca-bundle -n example-namespace -o yaml | grep examplebundle123</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_using_self_signed_certificates_with_open_data_hub_components\">Using self-signed certificates with Open Data Hub components</h3>\n<div class=\"paragraph\">\n<p>Some Open Data Hub components have additional options or required configuration for self-signed certificates.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs\">Accessing S3-compatible object storage with self-signed certificates</h4>\n<div class=\"paragraph _abstract\">\n<p>To securely connect Open Data Hub components to object storage solutions or databases that are deployed within an OpenShift cluster that uses self-signed certificates, you must provide a certificate authority (CA) certificate. Each namespace includes a ConfigMap named <code>kube-root-ca.crt</code>, which contains the CA certificate of the internal API Server.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have deployed an object storage solution or database in your OpenShift cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, log in to the OpenShift CLI (<code>oc</code>) as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc login api.&lt;cluster_name&gt;.&lt;cluster_domain&gt;:6443 --web</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Retrieve the current Open Data Hub trusted CA configuration and store it in a new file:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get dscinitializations.dscinitialization.opendatahub.io default-dsci -o json | jq -r '.spec.trustedCABundle.customCABundle' &gt; /tmp/my-custom-ca-bundles.crt</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Add the cluster&#8217;s <code>kube-root-ca.crt</code> ConfigMap to the Open Data Hub trusted CA configuration:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get configmap kube-root-ca.crt -o jsonpath=\"{['data']['ca\\.crt']}\" &gt;&gt; /tmp/my-custom-ca-bundles.crt</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Update the Open Data Hub trusted CA configuration to trust certificates issued by the certificate authorities in <code>kube-root-ca.crt</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc patch dscinitialization default-dsci --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/trustedCABundle/customCABundle\",\"value\":\"'\"$(awk '{printf \"%s\\\\n\", $0}' /tmp/my-custom-ca-bundles.crt)\"'\"}]'</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>You can successfully deploy components that are configured to use object storage solutions or databases that are deployed in the OpenShift cluster. For example, a pipeline server that is configured to use a database deployed in the cluster starts successfully.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-certificate-for-pipelines_certs\">Configuring a certificate for data science pipelines</h4>\n<div class=\"paragraph\">\n<p>By default, Open Data Hub includes OpenShift cluster-wide certificates in the <code>odh-trusted-ca-bundle</code> ConfigMap. These cluster-wide certificates cover most components, such as workbenches and model servers. However, the pipeline server might require additional Certificate Authority (CA) configuration, especially when interacting with external systems that use self-signed or custom certificates.</p>\n</div>\n<div class=\"paragraph\">\n<p>You have the following options for adding the certificate for data science pipelines:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Add them to the cluster-wide CA bundle, as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#adding-certificates-to-a-cluster-ca-bundle_certs\">Adding certificates to a cluster-wide CA bundle</a>.</p>\n</li>\n<li>\n<p>Add them to a custom bundle as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#adding-certificates-to-a-custom-ca-bundle_certs\">Adding certificates to a custom CA bundle</a>.</p>\n</li>\n<li>\n<p>Provide a CA bundle that is only used for data science pipelines, as described in the following procedure.</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator access for the OpenShift cluster where Open Data Hub is installed.</p>\n</li>\n<li>\n<p>You have created a self-signed certificate and saved the certificate to a file. For example, you have created a certificate using OpenSSL and saved it to a file named <code>example-ca.crt</code>.</p>\n</li>\n<li>\n<p>You have configured a data science pipeline server.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift console.</p>\n</li>\n<li>\n<p>From <strong>Workloads</strong> &#8594; <strong>ConfigMaps</strong>, create a ConfigMap with the required bundle in the same data science project as the target data science pipeline:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n    name: custom-ca-bundle\ndata:\n    ca-bundle.crt: |\n    # contents of ca-bundle.crt</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Add the following snippet to the <code>.spec.apiserver.caBundle</code> field of the underlying Data Science Pipelines Application (DSPA):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: datasciencepipelinesapplications.opendatahub.io/v1\nkind: DataSciencePipelinesApplication\nmetadata:\n    name: data-science-dspa\nspec:\n    ...\n    apiServer:\n    ...\n    cABundle:\n        configMapName: custom-ca-bundle\n        configMapKey: ca-bundle.crt</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Save the ConfigMap. The pipeline server pod automatically redeploys with the updated bundle.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Confirm that your CA bundle was successfully mounted:</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift console.</p>\n</li>\n<li>\n<p>Go to the data science project that has the target data science pipeline.</p>\n</li>\n<li>\n<p>Click the <strong>Pods</strong> tab.</p>\n</li>\n<li>\n<p>Click the pipeline server pod with the <code>ds-pipeline-dspa-&lt;hash&gt;</code> prefix.</p>\n</li>\n<li>\n<p>Click <strong>Terminal</strong>.</p>\n</li>\n<li>\n<p>Enter <code>cat /dsp-custom-certs/dsp-ca.crt</code>.</p>\n</li>\n<li>\n<p>Verify that your CA bundle is present within this file.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"configuring-a-certificate-for-workbenches_certs\">Configuring a certificate for workbenches</h4>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default, self-signed certificates apply to workbenches that you create after configuring cluster-wide certificates. To apply cluster-wide certificates to an existing workbench, stop and then restart the workbench.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Self-signed certificates are stored in <code>/etc/pki/tls/custom-certs/ca-bundle.crt</code>. Workbenches use a preset environment variable that many popular HTTP client packages point to for certificates. For packages that are not included by default, you can provide this certificate path. For example, for the <code>kfp</code> package to connect to the data science pipeline server:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>from kfp.client import Client\n\nwith open(sa_token_file_path, 'r') as token_file:\n    bearer_token = token_file.read()\n\n    client = Client(\n        host='https://&lt;GO_TO_ROUTER_OF_DS_PROJECT&gt;/',\n        existing_token=bearer_token,\n        ssl_ca_cert='/etc/pki/tls/custom-certs/ca-bundle.crt'\n    )\n    print(client.list_experiments())</code></pre>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"using-the-cluster-CA-bundle-for-single-model-serving_certs\">Using the cluster-wide CA bundle for the single-model serving platform</h4>\n<div class=\"paragraph\">\n<p>By default, the single-model serving platform in Open Data Hub uses a self-signed certificate generated at installation for the endpoints that are created when deploying a server.</p>\n</div>\n<div class=\"paragraph\">\n<p>If you have configured cluster-wide certificates on your OpenShift cluster, they are used by default for other types of endpoints, such as endpoints for routes.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following procedure explains how to use the same certificate that you already have for your OpenShift cluster.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator access for the OpenShift cluster where Open Data Hub is installed.</p>\n</li>\n<li>\n<p>You have configured cluster-wide certificates in OpenShift.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift console.</p>\n</li>\n<li>\n<p>From the list of projects, open the <code>openshift-ingress</code> project.</p>\n</li>\n<li>\n<p>Click <strong>YAML</strong>.</p>\n</li>\n<li>\n<p>Search for \"cert\" to find a secret with a name that includes \"cert\". For example, <code>rhods-internal-primary-cert-bundle-secret</code>. The contents of the secret should contain two items that are used for all OpenShift Routes: <code>tls.cert</code> (the certificate) and <code>tls.key</code> (the key).</p>\n</li>\n<li>\n<p>Copy the reference to the secret.</p>\n</li>\n<li>\n<p>From the list of projects, open the <code>istio-system</code> project.</p>\n</li>\n<li>\n<p>Create a YAML file and paste the reference to the secret that you copied from the <code>openshift-ingress</code> YAML file.</p>\n</li>\n<li>\n<p>Edit the YAML code to keep only the relevant content, as shown in the following example. Replace <code>rhods-internal-primary-cert-bundle-secret</code> with the name of your secret:</p>\n<div class=\"listingblock lines_space\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">kind: Secret\napiVersion: v1\nmetadata:\nname: rhods-internal-primary-cert-bundle-secret\ndata:\ntls.crt: &gt;-\n    LS0tLS1CRUd...\ntls.key: &gt;-\n    LS0tLS1CRUd...\ntype: kubernetes.io/tls</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Save the YAML file in the <code>istio-system</code> project.</p>\n</li>\n<li>\n<p>Navigate to <strong>Operators</strong> &#8594; <strong>Installed Operators</strong> &#8594; <strong>Open Data Hub</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Data</strong> Science Cluster*, and then click <strong>default-dsc</strong> &#8594; <strong>YAML</strong>.</p>\n</li>\n<li>\n<p>Edit the <code>kserve</code> configuration section to refer to your secret as shown in the following example. Replace <code>rhods-internal-primary-cert-bundle-secret</code> with the name of the secret that you created in Step 8.</p>\n<div class=\"listingblock lines_space\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">kserve:\ndevFlags: {}\nmanagementState: Managed\nserving:\n    ingressGateway:\n    certificate:\n        secretName: rhods-internal-primary-cert-bundle-secret\n        type: Provided\n    managementState: Managed\n    name: knative-serving</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"managing-certificates-without-the-operator_certs\">Managing certificates without the Open Data Hub Operator</h3>\n<div class=\"paragraph _abstract\">\n<p>By default, the Open Data Hub Operator manages the <code>odh-trusted-ca-bundle</code> ConfigMap, which contains the trusted CA bundle and is applied to all non-reserved namespaces in the cluster. The Operator automatically updates this ConfigMap whenever changes are made to the CA bundle.</p>\n</div>\n<div class=\"paragraph\">\n<p>If your organization prefers to manage trusted CA bundles independently, for example, by using Ansible automation, you can disable this default behavior to prevent automatic updates by the Open Data Hub Operator.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform web console, click <strong>Operators</strong>  <strong>Installed Operators</strong> and then click the <strong>Open Data Hub Operator</strong>.</p>\n</li>\n<li>\n<p>Click the <strong>DSC Initialization</strong> tab.</p>\n</li>\n<li>\n<p>Click the <strong>default-dsci</strong> object.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>In the <code>spec</code> section, change the value of the <code>managementState</code> field for <code>trustedCABundle</code> to <code>Unmanaged</code>, as shown:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>spec:\n  trustedCABundle:\n    managementState: Unmanaged</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n<div class=\"paragraph\">\n<p>Changing the <code>managementState</code> from <code>Managed</code> to <code>Unmanaged</code> prevents automatic updates when the <code>customCABundle</code> field is modified, but does not remove the <code>odh-trusted-ca-bundle</code> ConfigMap.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the <code>spec</code> section, set the value of the <code>customCABundle</code> field for <code>trustedCABundle</code>, for example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>spec:\n  trustedCABundle:\n    managementState: Unmanaged\n    customCABundle: example123</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Workloads</strong> &#8594; <strong>ConfigMaps</strong>.</p>\n</li>\n<li>\n<p>Select a project from the project list.</p>\n</li>\n<li>\n<p>Click the <code>odh-trusted-ca-bundle</code> ConfigMap.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab and verify that the value of the <code>customCABundle</code> field did not update.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_removing_the_ca_bundle\">Removing the CA bundle</h3>\n<div class=\"paragraph\">\n<p>If you prefer to implement a different authentication approach for your Open Data Hub installation, you can override the default behavior by removing the CA bundle.</p>\n</div>\n<div class=\"paragraph\">\n<p>You have two options for removing the CA bundle:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Remove the CA bundle from all non-reserved projects in Open Data Hub.</p>\n</li>\n<li>\n<p>Remove the CA bundle from a specific project.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"removing-the-ca-bundle-from-all-namespaces_certs\">Removing the CA bundle from all namespaces</h4>\n<div class=\"paragraph _abstract\">\n<p>You can remove a Certificate Authority (CA) bundle from all non-reserved namespaces in Open Data Hub. This process changes the default configuration and disables the creation of the <code>odh-trusted-ca-bundle</code> configuration file (ConfigMap), as described in <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#understanding-certificates_certs\">Understanding certificates in Open Data Hub</a>.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nThe <code>odh-trusted-ca-bundle</code> ConfigMaps are only deleted from namespaces when you set the <code>managementState</code> of <code>trustedCABundle</code> to <code>Removed</code>; deleting the DSC Initialization does not delete the ConfigMaps.\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>To remove a CA bundle from a single namespace only, see <a href=\"https://opendatahub.io/docs/installing-open-data-hub/#removing-the-ca-bundle-from-a-single-namespace_certs\">Removing the CA bundle from a single namespace</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform web console, click <strong>Operators</strong>  <strong>Installed Operators</strong> and then click the Open Data Hub Operator.</p>\n</li>\n<li>\n<p>Click the <strong>DSC Initialization</strong> tab.</p>\n</li>\n<li>\n<p>Click the <strong>default-dsci</strong> object.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>In the <code>spec</code> section, change the value of the <code>managementState</code> field for <code>trustedCABundle</code> to <code>Removed</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>spec:\n  trustedCABundle:\n    managementState: Removed</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Run the following command to verify that the <code>odh-trusted-ca-bundle</code> ConfigMap has been removed from all namespaces:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get configmaps --all-namespaces | grep odh-trusted-ca-bundle</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The command should not return any ConfigMaps.</p>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"removing-the-ca-bundle-from-a-single-namespace_certs\">Removing the CA bundle from a single namespace</h4>\n<div class=\"paragraph _abstract\">\n<p>You can remove a custom Certificate Authority (CA) bundle from individual namespaces in Open Data Hub. This process disables the creation of the <code>odh-trusted-ca-bundle</code> configuration file (ConfigMap) for the specified namespace only.</p>\n</div>\n<div class=\"paragraph\">\n<p>To remove a certificate bundle from all namespaces, see\n<a href=\"https://opendatahub.io/docs/installing-open-data-hub/#removing-the-ca-bundle-from-all-namespaces_certs\">Removing the CA bundle from all namespaces</a>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Procedure</div>\n<ul>\n<li>\n<p>Run the following command to remove a CA bundle from a namespace. In the following command, <em>example-namespace</em> is the non-reserved namespace.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc annotate ns example-namespace security.opendatahub.io/inject-trusted-ca-bundle=false</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Run the following command to verify that the CA bundle has been removed from the namespace. In the following command, <em>example-namespace</em> is the non-reserved namespace.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get configmap odh-trusted-ca-bundle -n example-namespace</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The command should return <code>configmaps \"odh-trusted-ca-bundle\" not found</code>.</p>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"viewing-logs-and-audit-records_install\">Viewing logs and audit records</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As a cluster administrator, you can use the Open Data Hub Operator logger to monitor and troubleshoot issues. You can also use OpenShift Container Platform audit records to review a history of changes made to the Open Data Hub Operator configuration.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-the-operator-logger_install\">Configuring the Open Data Hub Operator logger</h3>\n<div class=\"paragraph _abstract\">\n<p>You can change the log level for Open Data Hub Operator components by setting the <code>.spec.devFlags.logmode</code> flag for the <strong>DSC Initialization</strong>/<code>DSCI</code> custom resource during runtime. If you do not set a <code>logmode</code> value, the logger uses the INFO log level by default.</p>\n</div>\n<div class=\"paragraph\">\n<p>The log level that you set with <code>.spec.devFlags.logmode</code> applies to all components, not just those in a <strong>Managed</strong> state.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following table shows the available log levels:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<colgroup>\n<col style=\"width: 20%;\">\n<col style=\"width: 20%;\">\n<col style=\"width: 20%;\">\n<col style=\"width: 20%;\">\n<col style=\"width: 20%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Log level</th>\n<th class=\"tableblock halign-left valign-top\">Stacktrace level</th>\n<th class=\"tableblock halign-left valign-top\">Verbosity</th>\n<th class=\"tableblock halign-left valign-top\">Output</th>\n<th class=\"tableblock halign-left valign-top\">Timestamp type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>devel</code> or <code>development</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">WARN</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">INFO</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Console</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Epoch timestamps</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>\"\"</code>  (or no <code>logmode</code> value set)</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">ERROR</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">INFO</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">JSON</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Human-readable timestamps</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>prod</code> or <code>production</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">ERROR</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">INFO</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">JSON</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Human-readable timestamps</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"paragraph\">\n<p>Logs that are set to <code>devel</code> or <code>development</code> generate in a plain text console format.\nLogs that are set to <code>prod</code>, <code>production</code>, or which do not have a level set generate in a JSON format.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have administrator access to the <code>DSCInitialization</code> resources in the OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift Container Platform as a cluster administrator.</p>\n</li>\n<li>\n<p>Click <strong>Operators</strong>  <strong>Installed Operators</strong> and then click the Open Data Hub Operator.</p>\n</li>\n<li>\n<p>Click the <strong>DSC Initialization</strong> tab.</p>\n</li>\n<li>\n<p>Click the <strong>default-dsci</strong> object.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab.</p>\n</li>\n<li>\n<p>In the <code>spec</code> section, update the <code>.spec.devFlags.logmode</code> flag with the log level that you want to set.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: dscinitialization.opendatahub.io/v1\nkind: DSCInitialization\nmetadata:\n  name: default-dsci\nspec:\n  devFlags:\n    logmode: development</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>You can also configure the log level from the OpenShift CLI (<code>oc</code>) by using the following command with the <code>logmode</code> value set to the log level that you want.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc patch dsci default-dsci -p '{\"spec\":{\"devFlags\":{\"logmode\":\"development\"}}}' --type=merge</code></pre>\n</div>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>If you set the component log level to <code>devel</code> or <code>development</code>, logs generate more frequently and include logs at <code>WARN</code> level and above.</p>\n</li>\n<li>\n<p>If you set the component log level to <code>prod</code> or <code>production</code>, or do not set a log level, logs generate less frequently and include logs at <code>ERROR</code> level or above.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_viewing_the_open_data_hub_operator_logs\">Viewing the Open Data Hub Operator logs</h4>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Log in to the OpenShift CLI (<code>oc</code>).</p>\n</li>\n<li>\n<p>Run the following command to stream logs from all Operator pods:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>for pod in $(oc get pods -l name=opendatahub-operator -n openshift-operators -o name); do\n  oc logs -f \"$pod\" -n openshift-operators &amp;\ndone</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The Operator pod logs open in your terminal.</p>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Tip</div>\n</td>\n<td class=\"content\">\nPress <code>Ctrl+C</code> to stop viewing. To fully stop all log streams, run <code>kill $(jobs -p)</code>.\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"viewing-audit-records_install\">Viewing audit records</h3>\n<div class=\"paragraph _abstract\">\n<p>Cluster administrators can use OpenShift Container Platform auditing to see changes made to the Open Data Hub Operator configuration by reviewing modifications to the DataScienceCluster (DSC) and DSCInitialization (DSCI) custom resources. Audit logging is enabled by default in standard OpenShift Container Platform cluster configurations.\nFor more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/security_and_compliance/audit-log-view#audit-log-view\" target=\"_blank\" rel=\"noopener\">Viewing audit logs</a> in the OpenShift Container Platform documentation.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following example shows how to use the OpenShift Container Platform audit logs to see the history of changes made (by users) to the DSC and DSCI custom resources.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift Container Platform cluster as a cluster administrator, log in to the OpenShift Container Platform CLI as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>To access the full content of the changed custom resources, set the OpenShift Container Platform audit log policy to <code>WriteRequestBodies</code> or a more comprehensive profile. For more information, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/security_and_compliance/audit-log-policy-config#configuring-audit-policy_audit-log-policy-config\" target=\"_blank\" rel=\"noopener\">Configuring the audit log policy</a>.</p>\n</li>\n<li>\n<p>Fetch the audit log files that are available for the relevant control plane nodes. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc adm node-logs --role=master --path=kube-apiserver/ \\\n  | awk '{ print $1 }' | sort -u \\\n  | while read node ; do\n      oc adm node-logs $node --path=kube-apiserver/audit.log &lt; /dev/null\n    done \\\n  | grep opendatahub &gt; /tmp/kube-apiserver-audit-opendatahub.log</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Search the files for the DSC and DSCI custom resources. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>jq 'select((.objectRef.apiGroup == \"dscinitialization.opendatahub.io\"\n                or .objectRef.apiGroup == \"datasciencecluster.opendatahub.io\")\n              and .user.username != \"system:serviceaccount:openshift-operators:redhat-ods-operator-controller-manager\"\n              and .verb != \"get\" and .verb != \"watch\" and .verb != \"list\")' &lt; /tmp/kube-apiserver-audit-opendatahub.log</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The commands return relevant log entries.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>","id":"1b0004e6-086a-56d8-9bd6-cbe13300fa80","document":{"title":"Installing Open Data Hub"}},"markdownRemark":null},"pageContext":{"id":"1b0004e6-086a-56d8-9bd6-cbe13300fa80"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}