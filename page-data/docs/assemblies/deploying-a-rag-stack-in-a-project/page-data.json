{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/assemblies/deploying-a-rag-stack-in-a-project/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/api-workbench/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"api-workbench-overview_api-workbench"},{"parentId":null,"name":"Creating a custom image by using the <code>ImageStream</code> CRD","level":1,"index":1,"id":"api-custom-image-creating_api-workbench"},{"parentId":null,"name":"Creating a workbench by using the <code>Notebook</code> CRD","level":1,"index":2,"id":"api-workbench-creating_api-workbench"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/configuring-your-model-serving-platform/"},"sections":[{"parentId":null,"name":"About model-serving platforms","level":1,"index":0,"id":"configuring-your-model-serving-platform_odh-admin"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"About model serving","level":2,"index":0,"id":"about-model-serving_odh-admin"},{"parentId":"about-model-serving_odh-admin","name":"Model serving platform","level":3,"index":0,"id":"_model_serving_platform"},{"parentId":"about-model-serving_odh-admin","name":"NVIDIA NIM model serving platform","level":3,"index":1,"id":"_nvidia_nim_model_serving_platform"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"Model-serving runtimes","level":2,"index":1,"id":"model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes_odh-admin","name":"ServingRuntime","level":3,"index":0,"id":"_servingruntime"},{"parentId":"model-serving-runtimes_odh-admin","name":"InferenceService","level":3,"index":1,"id":"_inferenceservice"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"Model-serving runtimes for accelerators","level":2,"index":2,"id":"model-serving-runtimes-for-accelerators_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"NVIDIA GPUs","level":3,"index":0,"id":"_nvidia_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Intel Gaudi accelerators","level":3,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"AMD GPUs","level":3,"index":2,"id":"_amd_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"IBM Spyre AI accelerators on x86 and IBM Z","level":3,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86_and_ibm_z"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Supported model-serving runtimes","level":3,"index":4,"id":"supported-model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Tested and verified model-serving runtimes","level":3,"index":5,"id":"tested-verified-runtimes_odh-admin"},{"parentId":null,"name":"Configuring model servers","level":1,"index":1,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the model serving platform","level":2,"index":0,"id":"enabling-the-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers","name":"Enabling speculative decoding and multi-modal inferencing","level":2,"index":1,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_odh-admin"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":2,"index":2,"id":"adding-a-custom-model-serving-runtime_odh-admin"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified runtime","level":2,"index":3,"id":"adding-a-tested-and-verified-runtime_odh-admin"},{"parentId":null,"name":"Configuring model servers on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform","name":"Enabling the NVIDIA NIM model serving platform","level":2,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_odh-admin"},{"parentId":null,"name":"Customizing model deployments","level":1,"index":3,"id":"_customizing_model_deployments"},{"parentId":"_customizing_model_deployments","name":"Customizing the parameters of a deployed model-serving runtime","level":2,"index":0,"id":"customizing-parameters-serving-runtime_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizable model serving runtime parameters","level":2,"index":1,"id":"customizable-model-serving-runtime-parameters_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizing the vLLM model-serving runtime","level":2,"index":2,"id":"Customizing-the-vllm-runtime_odh-admin"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/customize-models-to-build-gen-ai-applications/"},"sections":[{"parentId":null,"name":"Overview of the model customization workflow","level":1,"index":0,"id":"overview-of-the-model-customization-workflow_custom-models"},{"parentId":null,"name":"Set up your working environment","level":1,"index":1,"id":"set-up-your-working-environment_custom-models"},{"parentId":"set-up-your-working-environment_custom-models","name":"About the Red&#160;Hat Python Index","level":2,"index":0,"id":"about-the-python-index_custom-models"},{"parentId":"set-up-your-working-environment_custom-models","name":"Mirror the Python Index for your disconnected environment","level":2,"index":1,"id":"mirror-the-python-index_custom-models"},{"parentId":"set-up-your-working-environment_custom-models","name":"Install packages","level":2,"index":2,"id":"install-packages_custom-models"},{"parentId":"set-up-your-working-environment_custom-models","name":"Import example notebooks","level":2,"index":3,"id":"import-example-notebooks_custom-models"},{"parentId":"import-example-notebooks_custom-models","name":"Clone an example Git repository","level":3,"index":0,"id":"clone-an-example-git-repository_custom-models"},{"parentId":null,"name":"Prepare your data for AI consumption","level":1,"index":2,"id":"prepare-your-data-for-ai-consumption_custom-models"},{"parentId":"prepare-your-data-for-ai-consumption_custom-models","name":"Process data by using Docling","level":2,"index":0,"id":"_process_data_by_using_docling"},{"parentId":"prepare-your-data-for-ai-consumption_custom-models","name":"Explore the data processing examples","level":2,"index":1,"id":"explore-the-data-processing-examples_custom-models"},{"parentId":"prepare-your-data-for-ai-consumption_custom-models","name":"Automate data processing steps by building AI pipelines","level":2,"index":2,"id":"_automate_data_processing_steps_by_building_ai_pipelines"},{"parentId":"prepare-your-data-for-ai-consumption_custom-models","name":"Explore the kubeflow pipeline examples","level":2,"index":3,"id":"explore-the-kubeflow-pipeline-examples_custom-models"},{"parentId":null,"name":"Generate synthetic data","level":1,"index":3,"id":"generate-synthetic-data_custom-models"},{"parentId":"generate-synthetic-data_custom-models","name":"Explore the SDG Hub examples","level":2,"index":0,"id":"explore-the-sdg-hub-examples_custom-models"},{"parentId":"generate-synthetic-data_custom-models","name":"Guided example - Build a KFP pipeline for SDG","level":2,"index":1,"id":"guided-example-build-a-kfp-pipeline-for-sdg_custom-models"},{"parentId":null,"name":"Train the model by using your prepared data","level":1,"index":4,"id":"train-the-model-by-using-your-prepared-data_custom-models"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Explore the Training Hub examples","level":2,"index":0,"id":"explore-the-training-hub-examples_custom-models"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Estimate memory usage","level":2,"index":1,"id":"estimate-memory-usage_custom-models"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Compare the performance of OSFT and SFT training algorithms","level":2,"index":2,"id":"compare-the-performance-of-osft-and-sft_custom-models"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Distribute training jobs by using the KubeFlow Trainer Operator","level":2,"index":3,"id":"_distribute_training_jobs_by_using_the_kubeflow_trainer_operator"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Distributed fine-tuning with Training Hub and Kubeflow Trainer","level":2,"index":4,"id":"_distributed_fine_tuning_with_training_hub_and_kubeflow_trainer"},{"parentId":null,"name":"End-to-end model customization workflow","level":1,"index":5,"id":"end-to-end-model-customization-workflow_custom-models"},{"parentId":null,"name":"Support philosophy: A secure platform","level":1,"index":6,"id":"support-philosophy_custom-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/deploying-models/"},"sections":[{"parentId":null,"name":"Storing models","level":1,"index":0,"id":"deploying-models_odh-user"},{"parentId":"deploying-models_odh-user","name":"Using OCI containers for model storage","level":2,"index":0,"id":"using-oci-containers-for-model-storage_odh-user"},{"parentId":"deploying-models_odh-user","name":"Storing a model in an OCI image","level":2,"index":1,"id":"storing-a-model-in-oci-image_odh-user"},{"parentId":"deploying-models_odh-user","name":"Uploading model files to a Persistent Volume Claim (PVC)","level":2,"index":2,"id":"uploading-model-files-to-pvc_odh-user"},{"parentId":null,"name":"Deploying models","level":1,"index":1,"id":"_deploying_models"},{"parentId":"_deploying_models","name":"Deploying models on the model serving platform","level":2,"index":0,"id":"deploying-models-on-the-model-serving-platform_odh-user"},{"parentId":"_deploying_models","name":"Deploying a model stored in an OCI image by using the CLI","level":2,"index":1,"id":"deploying-model-stored-in-oci-image_odh-user"},{"parentId":"_deploying_models","name":"Deploying models by using Distributed Inference with llm-d","level":2,"index":2,"id":"deploying-models-using-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Configuring authentication for Distributed Inference with llm-d using Red&#160;Hat Connectivity Link","level":3,"index":0,"id":"configuring-authentication-for-llmd_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Enabling Distributed Inference with llm-d","level":3,"index":1,"id":"enabling-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Example usage for Distributed Inference with llm-d","level":3,"index":2,"id":"ref-example-distributed-inference_odh-user"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Single-node GPU deployment","level":4,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Multi-node deployment","level":4,"index":1,"id":"_multi_node_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Intelligent inference scheduler with KV cache routing","level":4,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Configuring authentication for Distributed Inference with llm-d using Red&#160;Hat Connectivity Link","level":3,"index":3,"id":"configuring-authentication-for-llmd_odh-user"},{"parentId":"_deploying_models","name":"Monitoring models","level":2,"index":3,"id":"_monitoring_models"},{"parentId":"_monitoring_models","name":"Viewing performance metrics for a deployed model","level":3,"index":0,"id":"viewing-performance-metrics-for-deployed-model_odh-user"},{"parentId":"_monitoring_models","name":"Viewing model-serving runtime metrics for the model serving platform","level":3,"index":1,"id":"viewing-metrics-for-the-model-serving-platform_odh-user"},{"parentId":null,"name":"Deploying models on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_deploying_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Deploying models on the NVIDIA NIM model serving platform","level":2,"index":0,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing NVIDIA NIM metrics for a NIM model","level":2,"index":1,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing performance metrics for a NIM model","level":2,"index":2,"id":"viewing-performance-metrics-for-a-nim-model_odh-user"},{"parentId":null,"name":"Making inference requests to deployed models","level":1,"index":3,"id":"_making_inference_requests_to_deployed_models"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the authentication token for a deployed model","level":2,"index":0,"id":"accessing-authentication-token-for-deployed-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":1,"id":"accessing-inference-endpoint-for-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Making inference requests to models deployed on the model serving platform","level":2,"index":2,"id":"making-inference-requests-to-models-deployed-on-model-serving-platform_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Inference endpoints","level":2,"index":3,"id":"inference-endpoints_odh-user"},{"parentId":"inference-endpoints_odh-user","name":"Caikit TGIS ServingRuntime for KServe","level":3,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"OpenVINO Model Server","level":3,"index":1,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_odh-user","name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":3,"index":2,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":3,"index":3,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM AMD GPU ServingRuntime for KServe","level":3,"index":4,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":3,"index":5,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre s390x ServingRuntime for KServe","level":3,"index":6,"id":"_vllm_spyre_s390x_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"NVIDIA Triton Inference Server","level":3,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_odh-user","name":"Seldon MLServer","level":3,"index":8,"id":"_seldon_mlserver"},{"parentId":"inference-endpoints_odh-user","name":"Additional resources","level":3,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/enabling-ai-safety/"},"sections":[{"parentId":null,"name":"Enabling AI safety with Guardrails","level":1,"index":0,"id":"enabling-ai-safety-with-guardrails_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Understanding detectors","level":2,"index":0,"id":"guardrails-detectors_safety"},{"parentId":"guardrails-detectors_safety","name":"Built-in Detector","level":3,"index":0,"id":"_built_in_detector"},{"parentId":"guardrails-detectors_safety","name":"The Hugging Face Detector serving runtime","level":3,"index":1,"id":"guardrails-configuring-the-hugging-face-detector-serving-runtime_safety"},{"parentId":"guardrails-configuring-the-hugging-face-detector-serving-runtime_safety","name":"Guardrails Detector Hugging Face serving runtime configuration values","level":4,"index":0,"id":"_guardrails_detector_hugging_face_serving_runtime_configuration_values"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Orchestrator Configuration Parameters","level":2,"index":1,"id":"guardrails-orchestrator-config-parameters_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Guardrails Gateway Config Parameters","level":2,"index":2,"id":"guardrails-gateway-config-parameters_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Deploying the Guardrails Orchestrator","level":2,"index":3,"id":"deploying-the-guardrails-orchestrator-service_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Auto-configuring Guardrails","level":2,"index":4,"id":"guardrails-auto-config_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Configuring the OpenTelemetry exporter","level":2,"index":5,"id":"configuring-the-opentelemetry-exporter_safety"},{"parentId":null,"name":"Using Guardrails for AI safety","level":1,"index":1,"id":"using-guardrails-for-ai-safety_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Detecting PII and sensitive data","level":2,"index":0,"id":"_detecting_pii_and_sensitive_data"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":2,"index":1,"id":"detecting-pii-by-using-guardrails-with-llama-stack_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Filtering flagged content by sending requests to the regex detector","level":2,"index":2,"id":"filtering-flagged-content-by-sending-requests-to-the-regex-detector_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Securing prompts","level":2,"index":3,"id":"_securing_prompts"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Mitigating Prompt Injection by using a Hugging Face Prompt Injection detector","level":2,"index":4,"id":"mitigating-prompt-injection-by-using-a-hugging-face-prompt-injection-detector_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Moderating and safeguarding content","level":2,"index":5,"id":"_moderating_and_safeguarding_content"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Detecting hateful and profane language","level":2,"index":6,"id":"detecting-hateful-and-profane-language_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Enforcing configured safety pipelines for LLM inference by using Guardrails Gateway","level":2,"index":7,"id":"enforcing-configured-safety-pipelines-for-llm-inference-using-guardrails-gateway_safety"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/evaluating-ai-systems/"},"sections":[{"parentId":null,"name":"Overview of evaluating AI systems","level":1,"index":0,"id":"overview-evaluating-ai-systems_evaluate"},{"parentId":null,"name":"Evaluating large language models","level":1,"index":1,"id":"evaluating-large-language-models_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"Setting up LM-Eval","level":2,"index":0,"id":"setting-up-lmeval_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"Enabling external resource access for LMEval jobs","level":2,"index":1,"id":"enabling-external-resource-access-for-lmeval-jobs_evaluate"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_evaluate","name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":3,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_evaluate"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_evaluate","name":"Updating LMEval job configuration using the web console","level":3,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"LM-Eval evaluation job","level":2,"index":2,"id":"lmeval-evaluation-job_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"LM-Eval evaluation job properties","level":2,"index":3,"id":"lmeval-evaluation-job-properties_evaluate"},{"parentId":"lmeval-evaluation-job-properties_evaluate","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":3,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":"evaluating-large-language-models_evaluate","name":"Performing model evaluations in the dashboard","level":2,"index":4,"id":"performing-model-evaluations-in-the-dashboard_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"LM-Eval scenarios","level":2,"index":5,"id":"lmeval-scenarios_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Accessing Hugging Face models with an environment variable token","level":3,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Using a custom Unitxt card","level":3,"index":1,"id":"using-a-custom-unitxt-card_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Using PVCs as storage","level":3,"index":2,"id":"using-pvcs-as-storage_evaluate"},{"parentId":"using-pvcs-as-storage_evaluate","name":"Managed PVCs","level":4,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_evaluate","name":"Existing PVCs","level":4,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_evaluate","name":"Using a KServe Inference Service","level":3,"index":3,"id":"using-a-kserve-inference-service_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Setting up LM-Eval S3 Support","level":3,"index":4,"id":"setting-up-lmeval-s3-support_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Using LLM-as-a-Judge metrics with LM-Eval","level":3,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_evaluate"},{"parentId":null,"name":"Using llama stack with TrustyAI","level":1,"index":2,"id":"using-llama-stack-with-trustyai_evaluate"},{"parentId":"using-llama-stack-with-trustyai_evaluate","name":"Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI","level":2,"index":0,"id":"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_evaluate"},{"parentId":"using-llama-stack-with-trustyai_evaluate","name":"Running custom evaluations with LM-Eval and Llama Stack","level":2,"index":1,"id":"running-custom-evaluations-with-LMEval-and-llama-stack_evaluate"},{"parentId":"using-llama-stack-with-trustyai_evaluate","name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":2,"index":2,"id":"detecting-pii-by-using-guardrails-with-llama-stack_evaluate"},{"parentId":null,"name":"Evaluating RAG systems with Ragas","level":1,"index":3,"id":"evaluating-rag-systems-with-ragas_evaluate"},{"parentId":"evaluating-rag-systems-with-ragas_evaluate","name":"About Ragas evaluation","level":2,"index":0,"id":"_about_ragas_evaluation"},{"parentId":"_about_ragas_evaluation","name":"Key Ragas metrics","level":3,"index":0,"id":"_key_ragas_metrics"},{"parentId":"_about_ragas_evaluation","name":"Use cases for Ragas in AI engineering workflows","level":3,"index":1,"id":"_use_cases_for_ragas_in_ai_engineering_workflows"},{"parentId":"_about_ragas_evaluation","name":"Ragas provider deployment modes","level":3,"index":2,"id":"_ragas_provider_deployment_modes"},{"parentId":"evaluating-rag-systems-with-ragas_evaluate","name":"Setting up the Ragas inline provider for development","level":2,"index":1,"id":"setting-up-ragas-inline-provider_evaluate"},{"parentId":"evaluating-rag-systems-with-ragas_evaluate","name":"Configuring the Ragas remote provider for production","level":2,"index":2,"id":"configuring-ragas-remote-provider-for-production_evaluate"},{"parentId":"evaluating-rag-systems-with-ragas_evaluate","name":"Evaluating RAG system quality with Ragas metrics","level":2,"index":3,"id":"evaluating-rag-system-quality-with-ragas_evaluate"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/experimenting-with-models-in-the-gen-ai-playground/"},"sections":[{"parentId":null,"name":"Experimenting with models in the gen AI playground","level":1,"index":0,"id":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Playground overview","level":2,"index":0,"id":"playground-overview_rhoai-user"},{"parentId":"playground-overview_rhoai-user","name":"Core capabilities","level":3,"index":0,"id":"_core_capabilities"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Playground prerequisites","level":2,"index":1,"id":"playground-prerequisites_rhoai-user"},{"parentId":"playground-prerequisites_rhoai-user","name":"Cluster administrator prerequisites","level":3,"index":0,"id":"_cluster_administrator_prerequisites"},{"parentId":"playground-prerequisites_rhoai-user","name":"User prerequisites","level":3,"index":1,"id":"_user_prerequisites"},{"parentId":"playground-prerequisites_rhoai-user","name":"Model and runtime requirements for the playground","level":3,"index":2,"id":"_model_and_runtime_requirements_for_the_playground"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Key model selection factors","level":4,"index":0,"id":"_key_model_selection_factors"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Example model configuration","level":4,"index":1,"id":"_example_model_configuration"},{"parentId":"playground-prerequisites_rhoai-user","name":"Configuring Model Control Protocol (MCP) servers","level":3,"index":3,"id":"configuring-model-control-protocol-servers_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"About the AI assets endpoint page","level":2,"index":2,"id":"About-the-ai-assets-endpoint-page_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Configuring a playground for your project","level":2,"index":3,"id":"configuring-a-playground-for-your-project_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Testing baseline model responses","level":2,"index":4,"id":"testing-baseline-model-responses_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Testing your model with retrieval augmented generation (RAG)","level":2,"index":5,"id":"testing-your-model-with-rag_rhoai-user"},{"parentId":"testing-your-model-with-rag_rhoai-user","name":"Understanding RAG settings","level":3,"index":0,"id":"understanding-rag-settings_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Testing with model control protocol (MCP) servers","level":2,"index":6,"id":"testing-with-model-control-protocol-servers_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Exporting your playground configuration","level":2,"index":7,"id":"exporting-your-playground-configuration_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Updating your playground configuration","level":2,"index":8,"id":"updating-your-playground-configuration_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Deleting a playground from your project","level":2,"index":9,"id":"Deleting-a-playground-from-your-project_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Next steps","level":2,"index":10,"id":"next-steps_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Troubleshooting playground issues","level":2,"index":11,"id":"troubleshooting-playground-issues_rhoai-user"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The chatbot thinks indefinitely","level":3,"index":0,"id":"_the_chatbot_thinks_indefinitely"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The model does not use RAG data","level":3,"index":1,"id":"_the_model_does_not_use_rag_data"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"MCP servers are missing from the UI","level":3,"index":2,"id":"_mcp_servers_are_missing_from_the_ui"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The model fails to call MCP tools","level":3,"index":3,"id":"_the_model_fails_to_call_mcp_tools"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"overview-for-getting-started_get-started"},{"parentId":"overview-for-getting-started_get-started","name":"Data science workflow","level":2,"index":0,"id":"_data_science_workflow"},{"parentId":"overview-for-getting-started_get-started","name":"About this guide","level":2,"index":1,"id":"_about_this_guide"},{"parentId":"overview-for-getting-started_get-started","name":"Glossary of common terms","level":2,"index":2,"id":"glossary-of-common-terms_get-started"},{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":1,"id":"logging-in_get-started"},{"parentId":"logging-in_get-started","name":"Viewing installed Open Data Hub components","level":2,"index":0,"id":"viewing-installed-components_get-started"},{"parentId":null,"name":"Creating a project","level":1,"index":2,"id":"creating-a-project_get-started"},{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":3,"id":"creating-a-workbench-select-ide_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_get-started"},{"parentId":null,"name":"Next steps","level":1,"index":4,"id":"next-steps_get-started"},{"parentId":"next-steps_get-started","name":"Additional resources","level":2,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":2,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Configuring pipelines with your own Argo Workflows instance","level":1,"index":1,"id":"configuring-pipelines-with-your-own-argo-workflows-instance_install"},{"parentId":null,"name":"Installing the distributed workloads components","level":1,"index":2,"id":"installing-the-distributed-workloads-components_install"},{"parentId":null,"name":"Accessing the dashboard","level":1,"index":3,"id":"accessing-the-dashboard_install"},{"parentId":null,"name":"Working with certificates","level":1,"index":4,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding how Open Data Hub handles certificates","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Adding certificates","level":2,"index":1,"id":"_adding_certificates"},{"parentId":"working-with-certificates_certs","name":"Adding certificates to a cluster-wide CA bundle","level":2,"index":2,"id":"adding-certificates-to-a-cluster-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Adding certificates to a custom CA bundle","level":2,"index":3,"id":"adding-certificates-to-a-custom-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":4,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Accessing S3-compatible object storage with self-signed certificates","level":3,"index":0,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Configuring a certificate for pipelines","level":3,"index":1,"id":"configuring-a-certificate-for-pipelines_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Configuring a certificate for workbenches","level":3,"index":2,"id":"configuring-a-certificate-for-workbenches_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using the cluster-wide CA bundle for the model serving platform","level":3,"index":3,"id":"using-the-cluster-CA-bundle-for-model-serving_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates without the Open Data Hub Operator","level":2,"index":5,"id":"managing-certificates-without-the-operator_certs"},{"parentId":"working-with-certificates_certs","name":"Removing the CA bundle","level":2,"index":6,"id":"_removing_the_ca_bundle"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from all namespaces","level":3,"index":0,"id":"removing-the-ca-bundle-from-all-namespaces_certs"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from a single namespace","level":3,"index":1,"id":"removing-the-ca-bundle-from-a-single-namespace_certs"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":5,"id":"viewing-logs-and-audit-records_install"},{"parentId":"viewing-logs-and-audit-records_install","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_install"},{"parentId":"configuring-the-operator-logger_install","name":"Viewing the Open Data Hub Operator logs","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_logs"},{"parentId":"viewing-logs-and-audit-records_install","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_install"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-and-monitoring-models/"},"sections":[{"parentId":null,"name":"Managing model-serving runtimes","level":1,"index":0,"id":"managing-and-monitoring-models_cluster-admin"},{"parentId":"managing-and-monitoring-models_cluster-admin","name":"Adding a custom model-serving runtime","level":2,"index":0,"id":"adding-a-custom-model-serving-runtime_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models","level":1,"index":1,"id":"_managing_and_monitoring_models"},{"parentId":"_managing_and_monitoring_models","name":"Setting a timeout for KServe","level":2,"index":0,"id":"setting-timeout-for-kserve_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Deploying models by using multiple GPU nodes","level":2,"index":1,"id":"deploying-models-using-multiple-gpu-nodes_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Configuring an inference service for Kueue","level":2,"index":2,"id":"configuring-an-inference-service-for-kueue_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Configuring an inference service for Spyre","level":2,"index":3,"id":"configuring-inference-service-for-spyre_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Optimizing performance and tuning","level":2,"index":4,"id":"_optimizing_performance_and_tuning"},{"parentId":"_optimizing_performance_and_tuning","name":"Determining GPU requirements for LLM-powered applications","level":3,"index":0,"id":"determining-gpu-requirements-for-llm-powered-applications_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Performance considerations for text-summarization and retrieval-augmented generation (RAG) applications","level":3,"index":1,"id":"performance-considerations-for-document-based-apps_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Inference performance metrics","level":3,"index":2,"id":"inference-performance-metrics_cluster-admin"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Latency","level":4,"index":0,"id":"_latency"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Throughput","level":4,"index":1,"id":"_throughput"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Cost per million tokens","level":4,"index":2,"id":"_cost_per_million_tokens"},{"parentId":"_optimizing_performance_and_tuning","name":"Configuring metrics-based autoscaling","level":3,"index":3,"id":"configuring-metrics-based-autoscaling_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Guidelines for metrics-based autoscaling","level":3,"index":4,"id":"guidelines-for-metrics-based-autoscaling_cluster-admin"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing metrics for latency and throughput-optimized scaling","level":4,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing the right sliding window","level":4,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Optimizing HPA scale-down configuration","level":4,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Considering model size for optimal scaling","level":4,"index":3,"id":"_considering_model_size_for_optimal_scaling"},{"parentId":"_managing_and_monitoring_models","name":"Monitoring models","level":2,"index":5,"id":"_monitoring_models"},{"parentId":"_monitoring_models","name":"Configuring monitoring for the model serving platform","level":3,"index":0,"id":"configuring-monitoring-for-the-model-serving-platform_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Using Grafana to monitor model performance","level":2,"index":6,"id":"_using_grafana_to_monitor_model_performance"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a Grafana metrics dashboard","level":3,"index":0,"id":"deploying-a-grafana-metrics-dashboard_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a vLLM/GPU metrics dashboard on a Grafana instance","level":3,"index":1,"id":"deploying-vllm-gpu-metrics-dashboard-grafana_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Grafana metrics","level":3,"index":2,"id":"ref-grafana-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"Accelerator metrics","level":4,"index":0,"id":"ref-accelerator-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"CPU metrics","level":4,"index":1,"id":"ref-cpu-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"vLLM metrics","level":4,"index":2,"id":"ref-vllm-metrics_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Customizing model selection options for the NVIDIA NIM model serving platform","level":2,"index":0,"id":"Customizing-model-selection-options_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":2,"index":1,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin","name":"Enabling graph generation for an existing NIM deployment","level":3,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-odh/"},"sections":[{"parentId":null,"name":"Managing users and groups","level":1,"index":0,"id":"managing-users-and-groups"},{"parentId":"managing-users-and-groups","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-odh"},{"parentId":"managing-users-and-groups","name":"Viewing Open Data Hub users","level":2,"index":1,"id":"viewing-data-science-users_managing-odh"},{"parentId":"managing-users-and-groups","name":"Adding users to Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Selecting Open Data Hub administrator and user groups","level":2,"index":3,"id":"selecting-admin-and-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Deleting users","level":2,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":3,"index":0,"id":"about-deleting-users-and-resources_managing-odh"},{"parentId":"_deleting_users","name":"Stopping basic workbenches owned by other users","level":3,"index":1,"id":"stopping-basic-workbenches-owned-by-other-users_managing-odh"},{"parentId":"_deleting_users","name":"Revoking user access to basic workbenches","level":3,"index":2,"id":"revoking-user-access-to-basic-workbenches_managing-odh"},{"parentId":"_deleting_users","name":"Backing up storage data","level":3,"index":3,"id":"backing-up-storage-data_managing-odh"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":3,"index":4,"id":"cleaning-up-after-deleting-users_managing-odh"},{"parentId":null,"name":"Creating custom workbench images","level":1,"index":1,"id":"creating-custom-workbench-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from a default Open Data Hub image","level":2,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from your own image","level":2,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":3,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":3,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-custom-workbench-images","name":"Enabling custom images in Open Data Hub","level":2,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Importing a custom workbench image","level":2,"index":3,"id":"importing-a-custom-workbench-image_custom-images"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":2,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Disabling applications connected to Open Data Hub","level":2,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about available applications","level":2,"index":3,"id":"showing-hiding-information-about-available-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default basic workbench application","level":2,"index":4,"id":"hiding-the-default-basic-workbench-application_dashboard"},{"parentId":null,"name":"Creating project-scoped resources","level":1,"index":3,"id":"creating-project-scoped-resources_managing-odh"},{"parentId":null,"name":"Allocating additional resources to Open Data Hub users","level":1,"index":4,"id":"allocating-additional-resources-to-users_managing-odh"},{"parentId":null,"name":"Customizing component deployment resources","level":1,"index":5,"id":"customizing-component-deployment-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Overview of component resource customization","level":2,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Customizing component resources","level":2,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Disabling component resource customization","level":2,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Re-enabling component resource customization","level":2,"index":3,"id":"reenabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":6,"id":"enabling-accelerators"},{"parentId":"enabling-accelerators","name":"Enabling NVIDIA GPUs","level":2,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":"enabling-accelerators","name":"Intel Gaudi AI Accelerator integration","level":2,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":"intel-gaudi-ai-accelerator-integration_managing-odh","name":"Enabling Intel Gaudi AI accelerators","level":3,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_managing-odh"},{"parentId":"enabling-accelerators","name":"AMD GPU Integration","level":2,"index":2,"id":"amd-gpu-integration_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Verifying AMD GPU availability on your cluster","level":3,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Enabling AMD GPUs","level":3,"index":1,"id":"enabling-amd-gpus_managing-odh"},{"parentId":null,"name":"Managing workloads with Kueue","level":1,"index":7,"id":"managing-workloads-with-kueue"},{"parentId":"managing-workloads-with-kueue","name":"Overview of managing workloads with Kueue","level":2,"index":0,"id":"overview-of-managing-workloads-with-kueue_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue management states","level":3,"index":0,"id":"_kueue_management_states"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Queue enforcement for projects","level":3,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Restrictions for managing workloads with Kueue","level":3,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue workflow","level":3,"index":3,"id":"kueue-workflow_kueue"},{"parentId":"managing-workloads-with-kueue","name":"Configuring workload management with Kueue","level":2,"index":1,"id":"configuring-workload-management-with-kueue_kueue"},{"parentId":"configuring-workload-management-with-kueue_kueue","name":"Enabling Kueue in the dashboard","level":3,"index":0,"id":"enabling-kueue-in-the-dashboard_kueue"},{"parentId":"managing-workloads-with-kueue","name":"Troubleshooting common problems with Kueue","level":2,"index":2,"id":"troubleshooting-common-problems-with-Kueue_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"failed to call webhook\" error message for Kueue","level":3,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":3,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"local_queue provided does not exist\" error message","level":3,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"The pod provisioned by Kueue is terminated before the image is pulled","level":3,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"Additional resources","level":3,"index":4,"id":"_additional_resources"},{"parentId":"managing-workloads-with-kueue","name":"Migrating to the Red Hat build of Kueue Operator","level":2,"index":3,"id":"migrating-to-the-rhbok-operator_kueue"},{"parentId":null,"name":"Managing distributed workloads","level":1,"index":8,"id":"managing-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring quota management for distributed workloads","level":2,"index":0,"id":"configuring-quota-management-for-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Example Kueue resource configurations for distributed workloads","level":2,"index":1,"id":"ref-example-kueue-resource-configurations_managing-odh"},{"parentId":"ref-example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs without shared cohort","level":3,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":4,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":4,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":4,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":4,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"ref-example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":3,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":4,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":4,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":4,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":4,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring a cluster for RDMA","level":2,"index":2,"id":"configuring-a-cluster-for-rdma_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Troubleshooting common problems with distributed workloads for administrators","level":2,"index":3,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a suspended state","level":3,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a failed state","level":3,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster does not start","level":3,"index":2,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user cannot create a Ray cluster or submit jobs","level":3,"index":3,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"Additional resources","level":3,"index":4,"id":"_additional_resources_2"},{"parentId":null,"name":"Configuring a central authentication service for an external OIDC identity provider","level":1,"index":9,"id":"configuring-external-oidc-provider_managing-odh"},{"parentId":"configuring-external-oidc-provider_managing-odh","name":"About centralized authentication Gateway API","level":2,"index":0,"id":"about-centralized-auth-oidc_managing-odh"},{"parentId":"configuring-external-oidc-provider_managing-odh","name":"Configuring OpenID Connect (OIDC) authentication for Gateway API","level":2,"index":1,"id":"configuring-oidc-auth-gateway-api_managing-odh"},{"parentId":"configuring-oidc-auth-gateway-api_managing-odh","name":"Security considerations","level":3,"index":0,"id":"_security_considerations"},{"parentId":"configuring-external-oidc-provider_managing-odh","name":"Troubleshooting common problems with Gateway API configuration","level":2,"index":2,"id":"troubleshooting-common-problems-gateway-api_managing-odh"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"The <code>GatewayConfig</code> status shows as not ready","level":3,"index":0,"id":"_the_gatewayconfig_status_shows_as_not_ready"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"Authentication proxy fails to start","level":3,"index":1,"id":"_authentication_proxy_fails_to_start"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"The Gateway is inaccessible","level":3,"index":2,"id":"_the_gateway_is_inaccessible"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"The OIDC authentication fails","level":3,"index":3,"id":"_the_oidc_authentication_fails"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"The dashboard is not accessible after authentication","level":3,"index":4,"id":"_the_dashboard_is_not_accessible_after_authentication"},{"parentId":null,"name":"Backing up data","level":1,"index":10,"id":"backing-up-data_data-mgmt"},{"parentId":"backing-up-data_data-mgmt","name":"Backing up storage data","level":2,"index":0,"id":"backing-up-storage-data_data-mgmt"},{"parentId":"backing-up-data_data-mgmt","name":"Backing up your cluster","level":2,"index":1,"id":"backing-up-your-cluster_data-mgmt"},{"parentId":null,"name":"Managing observability","level":1,"index":11,"id":"managing-observability_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Enabling the observability stack","level":2,"index":0,"id":"enabling-the-observability-stack_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Collecting metrics from user workloads","level":2,"index":1,"id":"collecting-metrics-from-user-workloads_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Exporting metrics to external observability tools","level":2,"index":2,"id":"exporting-metrics-to-external-observability-tools_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Viewing traces in external tracing platforms","level":2,"index":3,"id":"viewing-traces-in-external-tracing-platforms_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Accessing built-in alerts","level":2,"index":4,"id":"accessing-built-in-alerts_managing-odh"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":12,"id":"viewing-logs-and-audit-records_managing-odh"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_managing-odh"},{"parentId":"configuring-the-operator-logger_managing-odh","name":"Viewing the Open Data Hub Operator logs","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_logs"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_managing-odh"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Selecting Open Data Hub administrator and user groups","level":1,"index":0,"id":"selecting-admin-and-user-groups_managing-resources"},{"parentId":null,"name":"Customizing the dashboard","level":1,"index":1,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration","level":2,"index":0,"id":"editing-the-dashboard-configuration_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":2,"id":"importing-a-custom-workbench-image_managing-resources"},{"parentId":null,"name":"Managing cluster PVC size","level":1,"index":3,"id":"managing-cluster-pvc-size"},{"parentId":"managing-cluster-pvc-size","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-pvc-size","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":null,"name":"Managing connection types","level":1,"index":4,"id":"managing-connection-types"},{"parentId":"managing-connection-types","name":"Viewing connection types","level":2,"index":0,"id":"viewing-connection-types_managing-resources"},{"parentId":"managing-connection-types","name":"Creating a connection type","level":2,"index":1,"id":"creating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Duplicating a connection type","level":2,"index":2,"id":"duplicating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Editing a connection type","level":2,"index":3,"id":"editing-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Enabling a connection type","level":2,"index":4,"id":"enabling-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Deleting a connection type","level":2,"index":5,"id":"deleting-a-connection-type_managing-resources"},{"parentId":null,"name":"Managing storage classes","level":1,"index":5,"id":"managing-storage-classes"},{"parentId":"managing-storage-classes","name":"About persistent storage","level":2,"index":0,"id":"about-persistent-storage_managing-resources"},{"parentId":"about-persistent-storage_managing-resources","name":"Storage classes in Open Data Hub","level":3,"index":0,"id":"_storage_classes_in_open_data_hub"},{"parentId":"about-persistent-storage_managing-resources","name":"Access modes","level":3,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":4,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":"managing-storage-classes","name":"Configuring storage class settings","level":2,"index":1,"id":"configuring-storage-class-settings_managing-resources"},{"parentId":"managing-storage-classes","name":"Configuring the default storage class for your cluster","level":2,"index":2,"id":"configuring-the-default-storage-class-for-your-cluster_managing-resources"},{"parentId":"managing-storage-classes","name":"Overview of object storage endpoints","level":2,"index":3,"id":"overview-of-object-storage-endpoints_managing-resources"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"MinIO (On-Cluster)","level":3,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Amazon S3","level":3,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Other S3-Compatible Object Stores","level":3,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Verification and Troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing basic workbenches","level":1,"index":6,"id":"managing-basic-workbenches"},{"parentId":"managing-basic-workbenches","name":"Accessing the administration interface for basic workbenches","level":2,"index":0,"id":"accessing-the-administration-interface-for-basic-workbenches_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Starting basic workbenches owned by other users","level":2,"index":1,"id":"starting-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Accessing basic workbenches owned by other users","level":2,"index":2,"id":"accessing-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Stopping basic workbenches owned by other users","level":2,"index":3,"id":"stopping-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Stopping idle workbenches","level":2,"index":4,"id":"stopping-idle-workbenches_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Adding workbench pod tolerations","level":2,"index":5,"id":"adding-workbench-pod-tolerations_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Troubleshooting common problems in workbenches for administrators","level":2,"index":6,"id":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":3,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"A user&#8217;s workbench does not start","level":3,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":3,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Overview of model monitoring","level":1,"index":0,"id":"overview-of-model-monitoring_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for your model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling TrustyAI Integration with KServe RawDeployment","level":2,"index":4,"id":"enabling-trustyai-kserve-integration_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Uploading training data to TrustyAI","level":2,"index":1,"id":"uploading-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":2,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":3,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Using bias metrics","level":2,"index":3,"id":"using-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":4,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-by-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using drift metrics","level":2,"index":3,"id":"using-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using a drift metric in a credit card scenario","level":2,"index":4,"id":"using-a-drift-metric-in-a-credit-card-scenario_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":5,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Using explainers","level":2,"index":2,"id":"using-explainers_explainers"},{"parentId":null,"name":"Evaluating large language models","level":1,"index":6,"id":"evaluating-large-language-models_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"Setting up LM-Eval","level":2,"index":0,"id":"setting-up-lmeval_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"Enabling external resource access for LMEval jobs","level":2,"index":1,"id":"enabling-external-resource-access-for-lmeval-jobs_monitor"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_monitor","name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":3,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_monitor"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_monitor","name":"Updating LMEval job configuration using the web console","level":3,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job","level":2,"index":2,"id":"lmeval-evaluation-job_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job properties","level":2,"index":3,"id":"lmeval-evaluation-job-properties_monitor"},{"parentId":"lmeval-evaluation-job-properties_monitor","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":3,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":"evaluating-large-language-models_monitor","name":"Performing model evaluations in the dashboard","level":2,"index":4,"id":"performing-model-evaluations-in-the-dashboard_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval scenarios","level":2,"index":5,"id":"lmeval-scenarios_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Accessing Hugging Face models with an environment variable token","level":3,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using a custom Unitxt card","level":3,"index":1,"id":"using-a-custom-unitxt-card_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using PVCs as storage","level":3,"index":2,"id":"using-pvcs-as-storage_monitor"},{"parentId":"using-pvcs-as-storage_monitor","name":"Managed PVCs","level":4,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_monitor","name":"Existing PVCs","level":4,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_monitor","name":"Using a KServe Inference Service","level":3,"index":3,"id":"using-a-kserve-inference-service_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Setting up LM-Eval S3 Support","level":3,"index":4,"id":"setting-up-lmeval-s3-support_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using LLM-as-a-Judge metrics with LM-Eval","level":3,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_monitor"},{"parentId":null,"name":"Evaluating RAG systems with Ragas","level":1,"index":7,"id":"evaluating-rag-systems-with-ragas_monitor"},{"parentId":"evaluating-rag-systems-with-ragas_monitor","name":"About Ragas evaluation","level":2,"index":0,"id":"_about_ragas_evaluation"},{"parentId":"_about_ragas_evaluation","name":"Key Ragas metrics","level":3,"index":0,"id":"_key_ragas_metrics"},{"parentId":"_about_ragas_evaluation","name":"Use cases for Ragas in AI engineering workflows","level":3,"index":1,"id":"_use_cases_for_ragas_in_ai_engineering_workflows"},{"parentId":"_about_ragas_evaluation","name":"Ragas provider deployment modes","level":3,"index":2,"id":"_ragas_provider_deployment_modes"},{"parentId":"evaluating-rag-systems-with-ragas_monitor","name":"Setting up the Ragas inline provider for development","level":2,"index":1,"id":"setting-up-ragas-inline-provider_monitor"},{"parentId":"evaluating-rag-systems-with-ragas_monitor","name":"Configuring the Ragas remote provider for production","level":2,"index":2,"id":"configuring-ragas-remote-provider-for-production_monitor"},{"parentId":"evaluating-rag-systems-with-ragas_monitor","name":"Evaluating RAG system quality with Ragas metrics","level":2,"index":3,"id":"evaluating-rag-system-quality-with-ragas_monitor"},{"parentId":null,"name":"Using llama stack with TrustyAI","level":1,"index":8,"id":"using-llama-stack-with-trustyai_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI","level":2,"index":0,"id":"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Running custom evaluations with LM-Eval and Llama Stack","level":2,"index":1,"id":"running-custom-evaluations-with-LMEval-and-llama-stack_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":2,"index":2,"id":"detecting-pii-by-using-guardrails-with-llama-stack_monitor"},{"parentId":null,"name":"Bias monitoring tutorial - Gender bias example","level":1,"index":9,"id":"bias-monitoring-tutorial_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Introduction","level":2,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":3,"index":0,"id":"_about_the_example_models"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Setting up your environment","level":2,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":3,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":3,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":3,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":3,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":3,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":3,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Deploying models","level":2,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Sending training data to the models","level":2,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Labeling data fields","level":2,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Checking model fairness","level":2,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling a fairness metric request","level":2,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling an identity metric request","level":2,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Simulating real world data","level":2,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Reviewing the results","level":2,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":3,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":3,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-your-ai-systems/"},"sections":[{"parentId":null,"name":"Overview of monitoring your AI systems","level":1,"index":0,"id":"overview-of-monitoring-your-ai-systems_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for your model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling TrustyAI Integration with KServe RawDeployment","level":2,"index":4,"id":"enabling-trustyai-kserve-integration_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Uploading training data to TrustyAI","level":2,"index":1,"id":"uploading-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":2,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":3,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Using bias metrics","level":2,"index":3,"id":"using-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Bias monitoring tutorial - Gender bias example","level":1,"index":4,"id":"bias-monitoring-tutorial_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Introduction","level":2,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":3,"index":0,"id":"_about_the_example_models"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Setting up your environment","level":2,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":3,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":3,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":3,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":3,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":3,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":3,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Deploying models","level":2,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Sending training data to the models","level":2,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Labeling data fields","level":2,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Checking model fairness","level":2,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling a fairness metric request","level":2,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling an identity metric request","level":2,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Simulating real world data","level":2,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Reviewing the results","level":2,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":3,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":3,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":5,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-by-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using drift metrics","level":2,"index":3,"id":"using-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using a drift metric in a credit card scenario","level":2,"index":4,"id":"using-a-drift-metric-in-a-credit-card-scenario_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":6,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Using explainers","level":2,"index":2,"id":"using-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2 or later","level":1,"index":1,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Requirements for upgrading Open Data Hub version 2","level":2,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Upgrading the Open Data Hub Operator","level":2,"index":1,"id":"upgrading-the-odh-operator_upgradev2"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Requirements for upgrading Open Data Hub version 1","level":2,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator","level":2,"index":1,"id":"upgrading-the-odh-operator_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":3,"id":"installing-odh-components_upgrade"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-in-your-data-science-ide/"},"sections":[{"parentId":null,"name":"Accessing your workbench IDE","level":1,"index":0,"id":"accessing-your-workbench-ide_ide"},{"parentId":null,"name":"Working in JupyterLab","level":1,"index":1,"id":"_working_in_jupyterlab"},{"parentId":"_working_in_jupyterlab","name":"Creating and importing Jupyter notebooks","level":2,"index":0,"id":"creating-and-importing-jupyter-notebooks_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Creating a Jupyter notebook","level":3,"index":0,"id":"creating-a-jupyter-notebook_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":3,"index":2,"id":"deleting-files-in-trash-directory_ide"},{"parentId":"deleting-files-in-trash-directory_ide","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":4,"index":0,"id":"emptying-trash-directory_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Additional resources","level":3,"index":3,"id":"_additional_resources"},{"parentId":"_working_in_jupyterlab","name":"Collaborating on Jupyter notebooks by using Git","level":2,"index":1,"id":"collaborating-on-jupyter-notebooks-by-using-git_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_ide"},{"parentId":"_working_in_jupyterlab","name":"Managing Python packages","level":2,"index":2,"id":"managing-python-packages_ide"},{"parentId":"managing-python-packages_ide","name":"Viewing Python packages installed on your workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_ide"},{"parentId":"managing-python-packages_ide","name":"Installing Python packages on your workbench","level":3,"index":1,"id":"installing-python-packages-on-your-workbench_ide"},{"parentId":"_working_in_jupyterlab","name":"Troubleshooting common problems in workbenches for users","level":2,"index":3,"id":"troubleshooting-common-problems-in-workbenches-for-users_ide"},{"parentId":null,"name":"Working in code-server","level":1,"index":2,"id":"_working_in_code_server"},{"parentId":"_working_in_code_server","name":"Creating code-server workbenches","level":2,"index":0,"id":"creating-code-server-workbenches_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Creating a workbench","level":3,"index":0,"id":"creating-a-project-workbench_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Uploading an existing notebook file to code-server from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_ide"},{"parentId":"_working_in_code_server","name":"Collaborating on workbenches in code-server by using Git","level":2,"index":1,"id":"collaborating-on-workbenches-in-code-server-by-using-git_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using code-server","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Updating your project in code-server with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Pushing project changes in code-server to a Git repository","level":3,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_ide"},{"parentId":"_working_in_code_server","name":"Managing Python packages in code-server","level":2,"index":2,"id":"managing-python-packages-in-code-server_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Viewing Python packages installed on your code-server workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Installing Python packages on your code-server workbench","level":3,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_ide"},{"parentId":"_working_in_code_server","name":"Installing extensions with code-server","level":2,"index":3,"id":"installing-extensions-with-code-server_ide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-projects/"},"sections":[{"parentId":null,"name":"Using projects","level":1,"index":0,"id":"using-projects_projects"},{"parentId":"using-projects_projects","name":"Creating a project","level":2,"index":0,"id":"creating-a-project_projects"},{"parentId":"using-projects_projects","name":"Updating a project","level":2,"index":1,"id":"updating-a-project_projects"},{"parentId":"using-projects_projects","name":"Deleting a project","level":2,"index":2,"id":"deleting-a-project_projects"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"using-project-workbenches_projects"},{"parentId":"using-project-workbenches_projects","name":"Creating a workbench and selecting an IDE","level":2,"index":0,"id":"creating-a-workbench-select-ide_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"About workbench images","level":3,"index":0,"id":"about-workbench-images_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"Creating a workbench","level":3,"index":1,"id":"creating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Deleting a workbench from a project","level":2,"index":3,"id":"deleting-a-workbench-from-a-project_projects"},{"parentId":null,"name":"Using connections","level":1,"index":2,"id":"using-connections_projects"},{"parentId":"using-connections_projects","name":"Adding a connection to your project","level":2,"index":0,"id":"adding-a-connection-to-your-project_projects"},{"parentId":"using-connections_projects","name":"Updating a connection","level":2,"index":1,"id":"updating-a-connection_projects"},{"parentId":"using-connections_projects","name":"Deleting a connection","level":2,"index":2,"id":"deleting-a-connection_projects"},{"parentId":"using-connections_projects","name":"Using the connections API","level":2,"index":3,"id":"using-connections-api_projects"},{"parentId":"using-connections-api_projects","name":"Namespace isolation in connections API","level":3,"index":0,"id":"_namespace_isolation_in_connections_api"},{"parentId":"using-connections-api_projects","name":"Role-based access control (RBAC) requirements in connections API","level":3,"index":1,"id":"_role_based_access_control_rbac_requirements_in_connections_api"},{"parentId":"using-connections-api_projects","name":"Validation scope","level":3,"index":2,"id":"_validation_scope"},{"parentId":"using-connections-api_projects","name":"Using connection annotations based on workload type","level":3,"index":3,"id":"_using_connection_annotations_based_on_workload_type"},{"parentId":"using-connections-api_projects","name":"Creating an Amazon S3-compatible connection type using the connections API","level":3,"index":4,"id":"creating-s3-compatible-connection-type-api_projects"},{"parentId":"creating-s3-compatible-connection-type-api_projects","name":"Using an Amazon S3 connection with <code>InferenceService</code> custom resource","level":4,"index":0,"id":"_using_an_amazon_s3_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-s3-compatible-connection-type-api_projects","name":"Using an Amazon S3 connection with <code>LLMInferenceService</code> custom resource","level":4,"index":1,"id":"_using_an_amazon_s3_connection_with_llminferenceservice_custom_resource"},{"parentId":"using-connections-api_projects","name":"Creating a URI-compatible connection type using the connections API","level":3,"index":5,"id":"creating-uri-compatible-connection-type-api_projects"},{"parentId":"creating-uri-compatible-connection-type-api_projects","name":"Using a URI connection with <code>InferenceService</code> custom resource","level":4,"index":0,"id":"_using_a_uri_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-uri-compatible-connection-type-api_projects","name":"Using a URI connection with <code>LLMInferenceService</code> custom resource","level":4,"index":1,"id":"_using_a_uri_connection_with_llminferenceservice_custom_resource"},{"parentId":"using-connections-api_projects","name":"Creating an OCI-compatible connection type using the connections API","level":3,"index":6,"id":"creating-oci-compatible-connection-type-api_projects"},{"parentId":"creating-oci-compatible-connection-type-api_projects","name":"Using an OCI connection with <code>InferenceService</code> custom resource","level":4,"index":0,"id":"_using_an_oci_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-oci-compatible-connection-type-api_projects","name":"Using an OCI connection with <code>LLMInferenceService</code> custom resource","level":4,"index":1,"id":"_using_an_oci_connection_with_llminferenceservice_custom_resource"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"configuring-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"About persistent storage","level":2,"index":0,"id":"about-persistent-storage_projects"},{"parentId":"about-persistent-storage_projects","name":"Storage classes in Open Data Hub","level":3,"index":0,"id":"_storage_classes_in_open_data_hub"},{"parentId":"about-persistent-storage_projects","name":"Access modes","level":3,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":4,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":"configuring-cluster-storage_projects","name":"Adding cluster storage to your project","level":2,"index":1,"id":"adding-cluster-storage-to-your-project_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Updating cluster storage","level":2,"index":2,"id":"updating-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Changing the storage class for an existing cluster storage instance","level":2,"index":3,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Deleting cluster storage from a project","level":2,"index":4,"id":"deleting-cluster-storage-from-a-project_projects"},{"parentId":null,"name":"Managing access to projects","level":1,"index":4,"id":"managing-access-to-projects_projects"},{"parentId":"managing-access-to-projects_projects","name":"Granting access to a project","level":2,"index":0,"id":"granting-access-to-a-project_projects"},{"parentId":"managing-access-to-projects_projects","name":"Updating access to a project","level":2,"index":1,"id":"updating-access-to-a-project_projects"},{"parentId":"managing-access-to-projects_projects","name":"Removing access to a project","level":2,"index":2,"id":"removing-access-to-a-project_projects"},{"parentId":null,"name":"Creating project-scoped resources for your project","level":1,"index":5,"id":"creating-project-scoped-resources-for-your-project_projects"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":1,"id":"enabling-accelerators_accelerators"},{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":2,"id":"enabling-nvidia-gpus_accelerators"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":3,"id":"intel-gaudi-ai-accelerator-integration_accelerators"},{"parentId":null,"name":"AMD GPU Integration","level":1,"index":4,"id":"amd-gpu-integration_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Verifying AMD GPU availability on your cluster","level":2,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Enabling AMD GPUs","level":2,"index":1,"id":"enabling-amd-gpus_accelerators"},{"parentId":null,"name":"IBM Spyre integration","level":1,"index":5,"id":"ibm-spyre-integration_accelerators"},{"parentId":null,"name":"Working with hardware profiles","level":1,"index":6,"id":"working-with-hardware-profiles_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Creating a hardware profile","level":2,"index":0,"id":"creating-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Updating a hardware profile","level":2,"index":1,"id":"updating-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Deleting a hardware profile","level":2,"index":2,"id":"deleting-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Configuring a recommended accelerator for workbench images","level":2,"index":3,"id":"configuring-a-recommended-accelerator-for-workbench-images_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":null,"name":"About GPU time slicing","level":1,"index":7,"id":"about-gpu-time-slicing_accelerators"},{"parentId":null,"name":"Enabling GPU time slicing","level":1,"index":8,"id":"enabling-gpu-time-slicing_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-ai-pipelines/"},"sections":[{"parentId":null,"name":"Managing AI pipelines","level":1,"index":0,"id":"managing-ai-pipelines_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ai-pipelines"},{"parentId":"configuring-a-pipeline-server_ai-pipelines","name":"Configuring a pipeline server with an external Amazon RDS database","level":3,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Compiling the pipeline YAML with the Kubeflow Pipelines SDK","level":3,"index":0,"id":"compiling-the-pipeline-yaml-with-kfp-sdk_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Compiling Kubernetes-native manifests with the Kubeflow Pipelines SDK","level":3,"index":1,"id":"compiling-kubernetes-native-manifests-with-kfp-sdk_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Authenticating the Kubeflow Pipelines SDK with a pipeline server","level":3,"index":2,"id":"authenticating-kfp-sdk-with-pipeline-server_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Defining a pipeline by using the Kubernetes API","level":3,"index":3,"id":"defining-a-pipeline-by-using-the-kubernetes-api_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Migrating pipelines from database to Kubernetes API storage","level":3,"index":4,"id":"migrating-pipelines-from-database-to-kubernetes-api_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Importing a pipeline","level":2,"index":2,"id":"importing-a-pipeline_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Deleting a pipeline","level":2,"index":3,"id":"deleting-a-pipeline_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Deleting a pipeline server","level":2,"index":4,"id":"deleting-a-pipeline-server_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Viewing the details of a pipeline server","level":2,"index":5,"id":"viewing-the-details-of-a-pipeline-server_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Viewing existing pipelines","level":2,"index":6,"id":"viewing-existing-pipelines_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Overview of pipeline versions","level":2,"index":7,"id":"overview-of-pipeline-versions_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Uploading a pipeline version","level":2,"index":8,"id":"uploading-a-pipeline-version_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Deleting a pipeline version","level":2,"index":9,"id":"deleting-a-pipeline-version_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Viewing the details of a pipeline version","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-version_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Downloading a pipeline version","level":2,"index":11,"id":"downloading-a-pipeline-version_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Overview of pipelines caching","level":2,"index":12,"id":"overview-of-pipelines-caching_ai-pipelines"},{"parentId":"overview-of-pipelines-caching_ai-pipelines","name":"Caching criteria","level":3,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-pipelines-caching_ai-pipelines","name":"Viewing cached steps in the Open Data Hub user interface","level":3,"index":1,"id":"_viewing_cached_steps_in_the_open_data_hub_user_interface"},{"parentId":"overview-of-pipelines-caching_ai-pipelines","name":"Controlling caching in pipelines","level":3,"index":2,"id":"controlling-caching-in-pipelines_ai-pipelines"},{"parentId":"controlling-caching-in-pipelines_ai-pipelines","name":"Disabling caching for individual tasks","level":4,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"controlling-caching-in-pipelines_ai-pipelines","name":"Disabling caching for a pipeline at submit time","level":4,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":"controlling-caching-in-pipelines_ai-pipelines","name":"Disabling caching for a pipeline at compile time","level":4,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":"controlling-caching-in-pipelines_ai-pipelines","name":"Disabling caching for all pipelines (pipeline server)","level":4,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"},{"parentId":null,"name":"Managing pipeline experiments","level":1,"index":1,"id":"managing-pipeline-experiments_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Overview of pipeline experiments","level":2,"index":0,"id":"overview-of-pipeline-experiments_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Creating a pipeline experiment","level":2,"index":1,"id":"creating-a-pipeline-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Archiving a pipeline experiment","level":2,"index":2,"id":"archiving-a-pipeline-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Deleting an archived pipeline experiment","level":2,"index":3,"id":"deleting-an-archived-pipeline-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Restoring an archived pipeline experiment","level":2,"index":4,"id":"restoring-an-archived-pipeline-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Viewing pipeline task executions","level":2,"index":5,"id":"viewing-pipeline-task-executions_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Viewing pipeline artifacts","level":2,"index":6,"id":"viewing-pipeline-artifacts_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Comparing runs in an experiment","level":2,"index":7,"id":"comparing-runs-in-an-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Comparing runs in different experiments","level":2,"index":8,"id":"comparing-runs-in-different-experiments_ai-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":2,"id":"managing-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Storing data with pipelines","level":2,"index":1,"id":"storing-data-with-pipelines_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Understanding pipeline run workspaces","level":2,"index":2,"id":"configuring-pipeline-run-workspaces_ai-pipelines"},{"parentId":"configuring-pipeline-run-workspaces_ai-pipelines","name":"Configuring default workspace PVC settings in DSPA","level":3,"index":0,"id":"configuring-default-workspace-pvc-settings-in-dspa_ai-pipelines"},{"parentId":"configuring-pipeline-run-workspaces_ai-pipelines","name":"Adding external artifacts to pipeline run workspaces","level":3,"index":1,"id":"adding-external-artifacts-to-pipeline-run-workspaces_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Viewing active pipeline runs","level":2,"index":3,"id":"viewing-active-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Executing a pipeline run","level":2,"index":4,"id":"executing-a-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Stopping an active pipeline run","level":2,"index":5,"id":"stopping-an-active-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Duplicating an active pipeline run","level":2,"index":6,"id":"duplicating-an-active-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Viewing scheduled pipeline runs","level":2,"index":7,"id":"viewing-scheduled-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Scheduling a pipeline run using a cron job","level":2,"index":8,"id":"scheduling-a-pipeline-run-using-a-cron-job_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Scheduling a pipeline run","level":2,"index":9,"id":"scheduling-a-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Duplicating a scheduled pipeline run","level":2,"index":10,"id":"duplicating-a-scheduled-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Deleting a scheduled pipeline run","level":2,"index":11,"id":"deleting-a-scheduled-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Viewing the details of a pipeline run","level":2,"index":12,"id":"viewing-the-details-of-a-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Viewing archived pipeline runs","level":2,"index":13,"id":"viewing-archived-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Archiving a pipeline run","level":2,"index":14,"id":"archiving-a-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Restoring an archived pipeline run","level":2,"index":15,"id":"restoring-an-archived-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Deleting an archived pipeline run","level":2,"index":16,"id":"deleting-an-archived-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Duplicating an archived pipeline run","level":2,"index":17,"id":"duplicating-an-archived-pipeline-run_ai-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":3,"id":"working-with-pipeline-logs_ai-pipelines"},{"parentId":"working-with-pipeline-logs_ai-pipelines","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ai-pipelines"},{"parentId":"working-with-pipeline-logs_ai-pipelines","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ai-pipelines"},{"parentId":"working-with-pipeline-logs_ai-pipelines","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ai-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":4,"id":"working-with-pipelines-in-jupyterlab_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Disabling node caching in Elyra","level":2,"index":2,"id":"disabling-node-caching-in-elyra_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Creating a runtime configuration","level":2,"index":3,"id":"creating-a-runtime-configuration_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Updating a runtime configuration","level":2,"index":4,"id":"updating-a-runtime-configuration_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Deleting a runtime configuration","level":2,"index":5,"id":"deleting-a-runtime-configuration_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Duplicating a runtime configuration","level":2,"index":6,"id":"duplicating-a-runtime-configuration_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Running a pipeline in JupyterLab","level":2,"index":7,"id":"running-a-pipeline-in-jupyterlab_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Exporting a pipeline in JupyterLab","level":2,"index":8,"id":"exporting-a-pipeline-in-jupyterlab_ai-pipelines"},{"parentId":null,"name":"Troubleshooting DSPA component errors","level":1,"index":5,"id":"troubleshooting-dspa-component-errors_ai-pipelines"},{"parentId":"troubleshooting-dspa-component-errors_ai-pipelines","name":"Common errors across DSPA components","level":2,"index":0,"id":"_common_errors_across_dspa_components"},{"parentId":null,"name":"Additional resources","level":1,"index":6,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-connected-applications/"},"sections":[{"parentId":null,"name":"Viewing applications that are connected to Open Data Hub","level":1,"index":0,"id":"viewing-connected-applications_connected-apps"},{"parentId":null,"name":"Enabling applications that are connected to Open Data Hub","level":1,"index":1,"id":"enabling-applications-connected_connected-apps"},{"parentId":null,"name":"Removing disabled applications from the dashboard","level":1,"index":2,"id":"removing-disabled-applications_connected-apps"},{"parentId":null,"name":"Using basic workbenches","level":1,"index":3,"id":"using-basic-workbenches_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Starting a basic workbench","level":2,"index":0,"id":"starting-a-basic-workbench_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Creating and importing Jupyter notebooks","level":2,"index":1,"id":"creating-and-importing-jupyter-notebooks_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Creating a Jupyter notebook","level":3,"index":0,"id":"creating-a-jupyter-notebook_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":3,"index":2,"id":"deleting-files-in-trash-directory_connected-apps"},{"parentId":"deleting-files-in-trash-directory_connected-apps","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":4,"index":0,"id":"emptying-trash-directory_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Additional resources","level":3,"index":3,"id":"_additional_resources"},{"parentId":"using-basic-workbenches_connected-apps","name":"Collaborating on Jupyter notebooks by using Git","level":2,"index":2,"id":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Managing Python packages","level":2,"index":3,"id":"managing-python-packages_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Viewing Python packages installed on your workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Installing Python packages on your workbench","level":3,"index":1,"id":"installing-python-packages-on-your-workbench_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Updating workbench settings by restarting your workbench","level":2,"index":4,"id":"updating-workbench-settings-by-restarting-your-workbench_connected-apps"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":1,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Distributed workloads infrastructure","level":2,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Types of distributed workloads","level":2,"index":1,"id":"_types_of_distributed_workloads"},{"parentId":null,"name":"Preparing the distributed training environment","level":1,"index":1,"id":"preparing-the-distributed-training-environment_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Creating a workbench for distributed training","level":2,"index":0,"id":"creating-a-workbench-for-distributed-training_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Using the cluster server and token to authenticate","level":2,"index":1,"id":"using-the-cluster-server-and-token-to-authenticate_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Managing custom training images","level":2,"index":2,"id":"managing-custom-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"About base training images","level":3,"index":0,"id":"about-base-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Creating a custom training image","level":3,"index":1,"id":"creating-a-custom-training-image_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Pushing an image to the integrated OpenShift image registry","level":3,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_distributed-workloads"},{"parentId":null,"name":"Running Ray-based distributed workloads","level":1,"index":2,"id":"running-ray-based-distributed-workloads_distributed-workloads"},{"parentId":"running-ray-based-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from Jupyter notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Downloading the demo Jupyter notebooks from the CodeFlare SDK","level":3,"index":0,"id":"downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Running the demo Jupyter notebooks from the CodeFlare SDK","level":3,"index":1,"id":"running-the-demo-jupyter-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Managing Ray clusters from within a Jupyter notebook","level":3,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_distributed-workloads"},{"parentId":"running-ray-based-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from AI pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ai-pipelines_distributed-workloads"},{"parentId":null,"name":"Running Training Operator-based distributed training workloads","level":1,"index":3,"id":"running-kfto-based-distributed-training-workloads_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Using the Kubeflow Training Operator to run distributed training workloads","level":2,"index":0,"id":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":3,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorchJob resource","level":3,"index":1,"id":"creating-a-kfto-pytorchjob-resource_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":3,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorch training scripts","level":3,"index":3,"id":"example-kfto-pytorch-training-scripts_distributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: NCCL","level":4,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccldistributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: DDP","level":4,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_distributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: FSDP","level":4,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Dockerfile for a Training Operator PyTorch training script","level":3,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorchJob resource for multi-node training","level":3,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Using the Training Operator SDK to run distributed training workloads","level":2,"index":1,"id":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"Configuring a training job by using the Training Operator SDK","level":3,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"Running a training job by using the Training Operator SDK","level":3,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"TrainingClient API: Job-related methods","level":3,"index":2,"id":"ref-trainingclient-api-job-related-methods_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Fine-tuning a model by using Kubeflow Training","level":2,"index":2,"id":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Configuring the fine-tuning job","level":3,"index":0,"id":"configuring-the-fine-tuning-job_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Running the fine-tuning job","level":3,"index":1,"id":"running-the-fine-tuning-job_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Deleting the fine-tuning job","level":3,"index":2,"id":"deleting-the-fine-tuning-job_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Creating a multi-node PyTorch training job with RDMA","level":2,"index":3,"id":"creating-a-multi-node-pytorch-training-job-with-rdma_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorchJob resource configured to run with RDMA","level":2,"index":4,"id":"ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":4,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing Kueue alerts for distributed workloads","level":2,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for users","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a suspended state","level":2,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a failed state","level":2,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"failed to call webhook\" error message for Kueue","level":2,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster does not start","level":2,"index":3,"id":"_my_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"Default Local Queue not found\" error message","level":2,"index":4,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"local_queue provided does not exist\" error message","level":2,"index":5,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I cannot create a Ray cluster or submit jobs","level":2,"index":6,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My pod provisioned by Kueue is terminated before my image is pulled","level":2,"index":7,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"Additional resources","level":2,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-llama-stack/"},"sections":[{"parentId":null,"name":"Overview of Llama Stack","level":1,"index":0,"id":"overview-of-llama-stack_rag"},{"parentId":"overview-of-llama-stack_rag","name":"OpenAI compatibility for RAG APIs in Llama Stack","level":2,"index":0,"id":"openai-compatibility-for-rag-apis-in-llama-stack_rag"},{"parentId":"overview-of-llama-stack_rag","name":"OpenAI-compatible APIs in Llama Stack","level":2,"index":1,"id":"openai-compatible-apis-in-Llama-Stack_rag"},{"parentId":"openai-compatible-apis-in-Llama-Stack_rag","name":"Supported OpenAI-compatible APIs in Open Data Hub","level":3,"index":0,"id":"_supported_openai_compatible_apis_in_open_data_hub"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Chat Completions API","level":4,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Completions API","level":4,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Embeddings API","level":4,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Files API","level":4,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Vector Stores API","level":4,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Vector Store Files API","level":4,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Models API","level":4,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Responses API","level":4,"index":7,"id":"_responses_api"},{"parentId":null,"name":"Activating the Llama Stack Operator","level":1,"index":1,"id":"activating-the-llama-stack-operator_rag"},{"parentId":null,"name":"Deploying a RAG stack in a project","level":1,"index":2,"id":"deploying-a-rag-stack-in-a-project_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Overview of RAG","level":2,"index":0,"id":"overview-of-rag_rag"},{"parentId":"overview-of-rag_rag","name":"Audience for RAG","level":3,"index":0,"id":"_audience_for_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Overview of vector databases","level":2,"index":1,"id":"overview-of-vector-databases_rag"},{"parentId":"overview-of-vector-databases_rag","name":"Overview of Milvus vector databases","level":3,"index":0,"id":"overview-of-milvus-vector-databases_rag"},{"parentId":"overview-of-vector-databases_rag","name":"Overview of FAISS vector databases","level":3,"index":1,"id":"overview-of-faiss-vector-databases_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Deploying a Llama model with KServe","level":2,"index":2,"id":"Deploying-a-llama-model-with-kserve_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Testing your vLLM model endpoints","level":2,"index":3,"id":"testing-your-vllm-model-endpoints_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Deploying a remote Milvus vector database","level":2,"index":4,"id":"deploying-a-remote-milvus-vector-database_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Deploying a LlamaStackDistribution instance","level":2,"index":5,"id":"deploying-a-llamastackdistribution-instance_rag"},{"parentId":"deploying-a-llamastackdistribution-instance_rag","name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":3,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_rag","name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":3,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_rag","name":"Example C: LlamaStackDistribution with <strong>Inline FAISS</strong>","level":3,"index":2,"id":"_example_c_llamastackdistribution_with_inline_faiss"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Ingesting content into a Llama model","level":2,"index":6,"id":"ingesting-content-into-a-llama-model_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Querying ingested content in a Llama model","level":2,"index":7,"id":"querying-ingested-content-in-a-llama-model_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Preparing documents with Docling for Llama Stack retrieval","level":2,"index":8,"id":"preparing-documents-with-docling-for-llama-stack-retrieval_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"About Llama stack search types","level":2,"index":9,"id":"about-llama-stack-search-types_rag"},{"parentId":"about-llama-stack-search-types_rag","name":"Supported search modes","level":3,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":4,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":4,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":4,"index":2,"id":"_hybrid_search"},{"parentId":"about-llama-stack-search-types_rag","name":"Retrieval database support","level":3,"index":1,"id":"_retrieval_database_support"},{"parentId":null,"name":"Configuring Llama Stack with OAuth Authentication","level":1,"index":3,"id":"auth-on-llama-stack_rag"},{"parentId":null,"name":"Evaluating RAG systems with Llama Stack","level":1,"index":4,"id":"evaluating-rag-systems-with-llama-stack_rag"},{"parentId":"evaluating-rag-systems-with-llama-stack_rag","name":"Understanding RAG evaluation providers","level":2,"index":0,"id":"understanding-rag-evaluation-providers_rag"},{"parentId":"evaluating-rag-systems-with-llama-stack_rag","name":"Using Ragas with Llama Stack","level":2,"index":1,"id":"using-ragas-with-llama-stack_rag"},{"parentId":"evaluating-rag-systems-with-llama-stack_rag","name":"Benchmarking embedding models with BEIR datasets and Llama Stack","level":2,"index":2,"id":"benchmarking-embedding-models-with-BEIR-datasets-and-Llama-Stack_rag"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-machine-learning-features/"},"sections":[{"parentId":null,"name":"Overview of machine learning features and Feature Store","level":1,"index":0,"id":"overview-of-ml-features-and-feature-store.adoc_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Audience for Feature Store","level":2,"index":0,"id":"audience-for-feature-store_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Overview of machine learning features","level":2,"index":1,"id":"overview-of-machine-learning-features_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Overview of Feature Store","level":2,"index":2,"id":"overview-of-feature-store_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Feature Store workflow","level":2,"index":3,"id":"feature-store-workflow_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Setting up the Feature Store user interface for initial use","level":2,"index":4,"id":"setting-up-feature-store-UI_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Additional resources","level":2,"index":5,"id":"_additional_resources"},{"parentId":null,"name":"Configuring Feature Store","level":1,"index":1,"id":"_configuring_feature_store"},{"parentId":"_configuring_feature_store","name":"Setting up Feature Store","level":2,"index":0,"id":"setting-up-feature-store_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Before you begin","level":3,"index":0,"id":"before-you-begin_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Enabling the Feature Store component","level":3,"index":1,"id":"enabling-the-feature-store-component_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Creating a Feature Store instance in a project","level":3,"index":2,"id":"creating-a-feature-store-instance-in-a-project_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Configuring and managing Role Based Access Control","level":3,"index":3,"id":"configuring-and-managing-role-based-access-control_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Adding feature definitions and initializing your Feature Store instance","level":3,"index":4,"id":"adding-feature-definitions-and-initializing-your-feature-store-instance_featurestore"},{"parentId":"adding-feature-definitions-and-initializing-your-feature-store-instance_featurestore","name":"Specifying files to ignore","level":4,"index":0,"id":"specifying-files-to-ignore_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Viewing Feature Store objects in the web-based UI","level":3,"index":5,"id":"viewing-feature-store-objects-in-the-web-based-ui_featurestore"},{"parentId":"_configuring_feature_store","name":"Customizing your Feature Store configuration","level":2,"index":1,"id":"customizing-your-feature-store-configuration_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring an offline store","level":3,"index":0,"id":"configuring-an-offline-store_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring an online store","level":3,"index":1,"id":"configuring-an-online-store_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring the feature registry","level":3,"index":2,"id":"configuring-the-feature-registry_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Example PVC configuration","level":3,"index":3,"id":"ref-example-pvc-configuration_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Editing an existing Feature Store instance","level":3,"index":4,"id":"editing-an-existing-feature-store-instance_featurestore"},{"parentId":null,"name":"Defining machine learning features","level":1,"index":2,"id":"defining-ml-features_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Setting up your working environment","level":2,"index":0,"id":"setting-up-your-working-environment_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Enabling automatic authentication and publishing features","level":2,"index":1,"id":"enabling-automatic-authentication-and-publishing-features_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"About feature definitions","level":2,"index":2,"id":"about-feature-definitions_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Specifying the data source for features","level":2,"index":3,"id":"specifying-the-data-source-for-features_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"About organizing features by using entities","level":2,"index":4,"id":"about-organizing-features-by-using-entities_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Creating feature views","level":2,"index":5,"id":"creating-feature-views_featurestore"},{"parentId":null,"name":"Retrieving features for model training","level":1,"index":3,"id":"retrieving-features-for-model-training_featurestore"},{"parentId":"retrieving-features-for-model-training_featurestore","name":"Retrieving data science features","level":2,"index":0,"id":"retrieving-data-science-features_featurestore"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Overview of the model catalog and model registries","level":1,"index":0,"id":"overview-of-model-registries_model-registry"},{"parentId":"overview-of-model-registries_model-registry","name":"Model catalog","level":2,"index":0,"id":"_model_catalog"},{"parentId":"overview-of-model-registries_model-registry","name":"Model registry","level":2,"index":1,"id":"_model_registry"},{"parentId":null,"name":"Enabling the model registry component","level":0,"index":1,"id":"_enabling_the_model_registry_component"},{"parentId":"_enabling_the_model_registry_component","name":"Enabling the model registry component","level":1,"index":0,"id":"enabling-the-model-registry-component_model-registry"},{"parentId":null,"name":"Managing model registries","level":0,"index":2,"id":"_managing_model_registries"},{"parentId":"_managing_model_registries","name":"Creating a model registry","level":1,"index":0,"id":"creating-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Editing a model registry","level":1,"index":1,"id":"editing-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Managing model registry permissions","level":1,"index":2,"id":"managing-model-registry-permissions_model-registry"},{"parentId":"_managing_model_registries","name":"Deleting a model registry","level":1,"index":3,"id":"deleting-a-model-registry_model-registry"},{"parentId":null,"name":"Working with model registries","level":0,"index":3,"id":"_working_with_model_registries"},{"parentId":"_working_with_model_registries","name":"Working with model registries","level":1,"index":0,"id":"working-with-model-registries_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model from the dashboard","level":2,"index":0,"id":"registering-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model version","level":2,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered models","level":2,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered model versions","level":2,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model metadata in a model registry","level":2,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model version metadata in a model registry","level":2,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deploying a model version from a model registry","level":2,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing the deployment properties of a deployed model version from a model registry","level":2,"index":7,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the model serving platform","level":3,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-model-serving-platform_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deleting a deployed model version from a model registry","level":2,"index":8,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model","level":2,"index":9,"id":"archiving-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model version","level":2,"index":10,"id":"archiving-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model","level":2,"index":11,"id":"restoring-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model version","level":2,"index":12,"id":"restoring-a-model-version_model-registry"},{"parentId":null,"name":"Working with the model catalog","level":0,"index":4,"id":"_working_with_the_model_catalog"},{"parentId":"_working_with_the_model_catalog","name":"Working with the model catalog","level":1,"index":0,"id":"working-with-the-model-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Discovering and evaluating models in the model catalog","level":2,"index":0,"id":"viewing-models-in-the-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Registering a model from the model catalog","level":2,"index":1,"id":"registering-a-model-from-the-model-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Deploying a model from the model catalog","level":2,"index":2,"id":"deploying-a-model-from-the-model-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Configuring model catalog sources in OpenShift","level":2,"index":3,"id":"configuring-model-catalog-sources-in-openshift_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/backing-up-data/"},"sections":[{"parentId":null,"name":"Backing up storage data","level":1,"index":0,"id":"backing-up-storage-data_data-mgmt"},{"parentId":null,"name":"Backing up your cluster","level":1,"index":1,"id":"backing-up-your-cluster_data-mgmt"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/bias-monitoring-tutorial/"},"sections":[{"parentId":null,"name":"Introduction","level":1,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":2,"index":0,"id":"_about_the_example_models"},{"parentId":null,"name":"Setting up your environment","level":1,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":2,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":2,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":2,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":2,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":2,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":2,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":null,"name":"Deploying models","level":1,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":null,"name":"Sending training data to the models","level":1,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":null,"name":"Labeling data fields","level":1,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":null,"name":"Checking model fairness","level":1,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":null,"name":"Scheduling a fairness metric request","level":1,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":null,"name":"Scheduling an identity metric request","level":1,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":null,"name":"Simulating real world data","level":1,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":null,"name":"Reviewing the results","level":1,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":2,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":2,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-jupyter-notebooks-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-workbenches-in-code-server-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using code-server","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project in code-server with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes in code-server to a Git repository","level":1,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-cluster-storage/"},"sections":[{"parentId":null,"name":"About persistent storage","level":1,"index":0,"id":"about-persistent-storage_{context}"},{"parentId":"about-persistent-storage_{context}","name":"Storage classes in {productname-short}","level":2,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":"about-persistent-storage_{context}","name":"Access modes","level":2,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":3,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":null,"name":"Adding cluster storage to your project","level":1,"index":1,"id":"adding-cluster-storage-to-your-project_{context}"},{"parentId":null,"name":"Updating cluster storage","level":1,"index":2,"id":"updating-cluster-storage_{context}"},{"parentId":null,"name":"Changing the storage class for an existing cluster storage instance","level":1,"index":3,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_{context}"},{"parentId":null,"name":"Deleting cluster storage from a project","level":1,"index":4,"id":"deleting-cluster-storage-from-a-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-external-oidc-provider/"},"sections":[{"parentId":null,"name":"About centralized authentication Gateway API","level":1,"index":0,"id":"about-centralized-auth-oidc_{context}"},{"parentId":null,"name":"Configuring OpenID Connect (OIDC) authentication for Gateway API","level":1,"index":1,"id":"configuring-oidc-auth-gateway-api_{context}"},{"parentId":"configuring-oidc-auth-gateway-api_{context}","name":"Security considerations","level":2,"index":0,"id":"_security_considerations"},{"parentId":null,"name":"Troubleshooting common problems with Gateway API configuration","level":1,"index":2,"id":"troubleshooting-common-problems-gateway-api_{context}"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"The <code>GatewayConfig</code> status shows as not ready","level":2,"index":0,"id":"_the_gatewayconfig_status_shows_as_not_ready"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"Authentication proxy fails to start","level":2,"index":1,"id":"_authentication_proxy_fails_to_start"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"The Gateway is inaccessible","level":2,"index":2,"id":"_the_gateway_is_inaccessible"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"The OIDC authentication fails","level":2,"index":3,"id":"_the_oidc_authentication_fails"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"The dashboard is not accessible after authentication","level":2,"index":4,"id":"_the_dashboard_is_not_accessible_after_authentication"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-feature-store-role-based-access-control/"},"sections":[{"parentId":null,"name":"Configuring role-based access control","level":1,"index":0,"id":"configuring-role-based-access-control_{context}"},{"parentId":null,"name":"Default authorization configuration","level":1,"index":1,"id":"ref-default-authorization-configuration_{context}"},{"parentId":null,"name":"Example OIDC Authorization configuration","level":1,"index":2,"id":"ref-example-oidc-authorization-configuration_{context}"},{"parentId":null,"name":"Example Kubernetes Authorization configuration","level":1,"index":3,"id":"ref-example-kubernetes-authorization-configuration_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-trustyai/"},"sections":[{"parentId":null,"name":"Configuring monitoring for your model serving platform","level":1,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_{context}"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":1,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Configuring TrustyAI with a database","level":1,"index":2,"id":"configuring-trustyai-with-a-database_{context}"},{"parentId":null,"name":"Installing the TrustyAI service for a project","level":1,"index":3,"id":"installing-trustyai-service_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the dashboard","level":2,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the CLI","level":2,"index":1,"id":"installing-trustyai-service-using-cli_{context}"},{"parentId":null,"name":"Enabling TrustyAI Integration with KServe RawDeployment","level":1,"index":4,"id":"enabling-trustyai-kserve-integration_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-your-model-serving-platform/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_odh-admin"},{"parentId":"about-model-serving_odh-admin","name":"Model serving platform","level":2,"index":0,"id":"_model_serving_platform"},{"parentId":"about-model-serving_odh-admin","name":"NVIDIA NIM model serving platform","level":2,"index":1,"id":"_nvidia_nim_model_serving_platform"},{"parentId":null,"name":"Model-serving runtimes","level":1,"index":1,"id":"model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes_odh-admin","name":"ServingRuntime","level":2,"index":0,"id":"_servingruntime"},{"parentId":"model-serving-runtimes_odh-admin","name":"InferenceService","level":2,"index":1,"id":"_inferenceservice"},{"parentId":null,"name":"Model-serving runtimes for accelerators","level":1,"index":2,"id":"model-serving-runtimes-for-accelerators_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"NVIDIA GPUs","level":2,"index":0,"id":"_nvidia_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Intel Gaudi accelerators","level":2,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"AMD GPUs","level":2,"index":2,"id":"_amd_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"IBM Spyre AI accelerators on x86 and IBM Z","level":2,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86_and_ibm_z"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Supported model-serving runtimes","level":2,"index":4,"id":"supported-model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Tested and verified model-serving runtimes","level":2,"index":5,"id":"tested-verified-runtimes_odh-admin"},{"parentId":null,"name":"Configuring model servers","level":0,"index":3,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the model serving platform","level":1,"index":0,"id":"enabling-the-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers","name":"Enabling speculative decoding and multi-modal inferencing","level":1,"index":1,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_odh-admin"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":1,"index":2,"id":"adding-a-custom-model-serving-runtime_odh-admin"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified runtime","level":1,"index":3,"id":"adding-a-tested-and-verified-runtime_odh-admin"},{"parentId":null,"name":"Configuring model servers on the NVIDIA NIM model serving platform","level":0,"index":4,"id":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform","name":"Enabling the NVIDIA NIM model serving platform","level":1,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_odh-admin"},{"parentId":null,"name":"Customizing model deployments","level":0,"index":5,"id":"_customizing_model_deployments"},{"parentId":"_customizing_model_deployments","name":"Customizing the parameters of a deployed model-serving runtime","level":1,"index":0,"id":"customizing-parameters-serving-runtime_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizable model serving runtime parameters","level":1,"index":1,"id":"customizable-model-serving-runtime-parameters_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizing the vLLM model-serving runtime","level":1,"index":2,"id":"Customizing-the-vllm-runtime_odh-admin"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-jupyter-notebooks/"},"sections":[{"parentId":null,"name":"Creating a Jupyter notebook","level":1,"index":0,"id":"creating-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_{context}"},{"parentId":null,"name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":1,"index":2,"id":"deleting-files-in-trash-directory_{context}"},{"parentId":"deleting-files-in-trash-directory_{context}","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":2,"index":0,"id":"emptying-trash-directory_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":3,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-code-server-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench","level":1,"index":0,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-custom-workbench-images/"},"sections":[{"parentId":null,"name":"Creating a custom image from a default {productname-short} image","level":1,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":null,"name":"Creating a custom image from your own image","level":1,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":2,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":2,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Enabling custom images in {productname-short}","level":1,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":3,"id":"importing-a-custom-workbench-image_custom-images"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration","level":1,"index":0,"id":"editing-the-dashboard-configuration_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-component-deployment-resources/"},"sections":[{"parentId":null,"name":"Overview of component resource customization","level":1,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":null,"name":"Customizing component resources","level":1,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":null,"name":"Disabling component resource customization","level":1,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Re-enabling component resource customization","level":1,"index":3,"id":"reenabling-component-resource-customization_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-your-feature-store-configuration/"},"sections":[{"parentId":null,"name":"Configuring an offline store","level":1,"index":0,"id":"configuring-an-offline-store_{context}"},{"parentId":null,"name":"Configuring an online store","level":1,"index":1,"id":"configuring-an-online-store_{context}"},{"parentId":null,"name":"Configuring the feature registry","level":1,"index":2,"id":"configuring-the-feature-registry_{context}"},{"parentId":null,"name":"Example PVC configuration","level":1,"index":3,"id":"ref-example-pvc-configuration_{context}"},{"parentId":null,"name":"Editing an existing Feature Store instance","level":1,"index":4,"id":"editing-an-existing-feature-store-instance_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/defining-ml-features/"},"sections":[{"parentId":null,"name":"Setting up your working environment","level":1,"index":0,"id":"setting-up-your-working-environment_{context}"},{"parentId":null,"name":"Enabling automatic authentication and publishing features","level":1,"index":1,"id":"enabling-automatic-authentication-and-publishing-features_{context}"},{"parentId":null,"name":"About feature definitions","level":1,"index":2,"id":"about-feature-definitions_{context}"},{"parentId":null,"name":"Specifying the data source for features","level":1,"index":3,"id":"specifying-the-data-source-for-features_{context}"},{"parentId":null,"name":"About organizing features by using entities","level":1,"index":4,"id":"about-organizing-features-by-using-entities_{context}"},{"parentId":null,"name":"Creating feature views","level":1,"index":5,"id":"creating-feature-views_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deploying-a-rag-stack-in-a-project/"},"sections":[{"parentId":null,"name":"Overview of RAG","level":1,"index":0,"id":"overview-of-rag_{context}"},{"parentId":"overview-of-rag_{context}","name":"Audience for RAG","level":2,"index":0,"id":"_audience_for_rag"},{"parentId":null,"name":"Overview of vector databases","level":1,"index":1,"id":"overview-of-vector-databases_{context}"},{"parentId":"overview-of-vector-databases_{context}","name":"Overview of Milvus vector databases","level":2,"index":0,"id":"overview-of-milvus-vector-databases_{context}"},{"parentId":"overview-of-vector-databases_{context}","name":"Overview of FAISS vector databases","level":2,"index":1,"id":"overview-of-faiss-vector-databases_{context}"},{"parentId":null,"name":"Deploying a Llama model with KServe","level":1,"index":2,"id":"Deploying-a-llama-model-with-kserve_{context}"},{"parentId":null,"name":"Testing your vLLM model endpoints","level":1,"index":3,"id":"testing-your-vllm-model-endpoints_{context}"},{"parentId":null,"name":"Deploying a remote Milvus vector database","level":1,"index":4,"id":"deploying-a-remote-milvus-vector-database_{context}"},{"parentId":null,"name":"Deploying a LlamaStackDistribution instance","level":1,"index":5,"id":"deploying-a-llamastackdistribution-instance_{context}"},{"parentId":"deploying-a-llamastackdistribution-instance_{context}","name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":2,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_{context}","name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":2,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_{context}","name":"Example C: LlamaStackDistribution with <strong>Inline FAISS</strong>","level":2,"index":2,"id":"_example_c_llamastackdistribution_with_inline_faiss"},{"parentId":null,"name":"Ingesting content into a Llama model","level":1,"index":6,"id":"ingesting-content-into-a-llama-model_{context}"},{"parentId":null,"name":"Querying ingested content in a Llama model","level":1,"index":7,"id":"querying-ingested-content-in-a-llama-model_{context}"},{"parentId":null,"name":"Preparing documents with Docling for Llama Stack retrieval","level":1,"index":8,"id":"preparing-documents-with-docling-for-llama-stack-retrieval_{context}"},{"parentId":null,"name":"About Llama stack search types","level":1,"index":9,"id":"about-llama-stack-search-types_{context}"},{"parentId":"about-llama-stack-search-types_{context}","name":"Supported search modes","level":2,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":3,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":3,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":3,"index":2,"id":"_hybrid_search"},{"parentId":"about-llama-stack-search-types_{context}","name":"Retrieval database support","level":2,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deploying-models/"},"sections":[{"parentId":null,"name":"Using OCI containers for model storage","level":1,"index":0,"id":"using-oci-containers-for-model-storage_odh-user"},{"parentId":null,"name":"Storing a model in an OCI image","level":1,"index":1,"id":"storing-a-model-in-oci-image_odh-user"},{"parentId":null,"name":"Uploading model files to a Persistent Volume Claim (PVC)","level":1,"index":2,"id":"uploading-model-files-to-pvc_odh-user"},{"parentId":null,"name":"Deploying models","level":0,"index":3,"id":"_deploying_models"},{"parentId":"_deploying_models","name":"Deploying models on the model serving platform","level":1,"index":0,"id":"deploying-models-on-the-model-serving-platform_odh-user"},{"parentId":"_deploying_models","name":"Deploying a model stored in an OCI image by using the CLI","level":1,"index":1,"id":"deploying-model-stored-in-oci-image_odh-user"},{"parentId":"_deploying_models","name":"Deploying models by using {llmd}","level":1,"index":2,"id":"deploying-models-using-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Configuring authentication for {llmd} using {org-name} Connectivity Link","level":2,"index":0,"id":"configuring-authentication-for-llmd_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Enabling {llmd}","level":2,"index":1,"id":"enabling-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Example usage for {llmd}","level":2,"index":2,"id":"ref-example-distributed-inference_odh-user"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Single-node GPU deployment","level":3,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Multi-node deployment","level":3,"index":1,"id":"_multi_node_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Intelligent inference scheduler with KV cache routing","level":3,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Configuring authentication for {llmd} using {org-name} Connectivity Link","level":2,"index":3,"id":"configuring-authentication-for-llmd_odh-user"},{"parentId":"_deploying_models","name":"Monitoring models","level":1,"index":3,"id":"_monitoring_models"},{"parentId":"_monitoring_models","name":"Viewing performance metrics for a deployed model","level":2,"index":0,"id":"viewing-performance-metrics-for-deployed-model_odh-user"},{"parentId":"_monitoring_models","name":"Viewing model-serving runtime metrics for the model serving platform","level":2,"index":1,"id":"viewing-metrics-for-the-model-serving-platform_odh-user"},{"parentId":null,"name":"Deploying models on the NVIDIA NIM model serving platform","level":0,"index":4,"id":"_deploying_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Deploying models on the NVIDIA NIM model serving platform","level":1,"index":0,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing NVIDIA NIM metrics for a NIM model","level":1,"index":1,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing performance metrics for a NIM model","level":1,"index":2,"id":"viewing-performance-metrics-for-a-nim-model_odh-user"},{"parentId":null,"name":"Making inference requests to deployed models","level":0,"index":5,"id":"_making_inference_requests_to_deployed_models"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the authentication token for a deployed model","level":1,"index":0,"id":"accessing-authentication-token-for-deployed-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the inference endpoint for a deployed model","level":1,"index":1,"id":"accessing-inference-endpoint-for-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Making inference requests to models deployed on the model serving platform","level":1,"index":2,"id":"making-inference-requests-to-models-deployed-on-model-serving-platform_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Inference endpoints","level":1,"index":3,"id":"inference-endpoints_odh-user"},{"parentId":"inference-endpoints_odh-user","name":"Caikit TGIS ServingRuntime for KServe","level":2,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"OpenVINO Model Server","level":2,"index":1,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_odh-user","name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":2,"index":2,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":2,"index":3,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM AMD GPU ServingRuntime for KServe","level":2,"index":4,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":2,"index":5,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre s390x ServingRuntime for KServe","level":2,"index":6,"id":"_vllm_spyre_s390x_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"NVIDIA Triton Inference Server","level":2,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_odh-user","name":"Seldon MLServer","level":2,"index":8,"id":"_seldon_mlserver"},{"parentId":"inference-endpoints_odh-user","name":"Additional resources","level":2,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-ai-safety-with-guardrails/"},"sections":[{"parentId":null,"name":"Understanding detectors","level":1,"index":0,"id":"guardrails-detectors_{context}"},{"parentId":"guardrails-detectors_{context}","name":"Built-in Detector","level":2,"index":0,"id":"_built_in_detector"},{"parentId":"guardrails-detectors_{context}","name":"The Hugging Face Detector serving runtime","level":2,"index":1,"id":"guardrails-configuring-the-hugging-face-detector-serving-runtime_{context}"},{"parentId":"guardrails-configuring-the-hugging-face-detector-serving-runtime_{context}","name":"Guardrails Detector Hugging Face serving runtime configuration values","level":3,"index":0,"id":"_guardrails_detector_hugging_face_serving_runtime_configuration_values"},{"parentId":null,"name":"Orchestrator Configuration Parameters","level":1,"index":1,"id":"guardrails-orchestrator-config-parameters_{context}"},{"parentId":null,"name":"Guardrails Gateway Config Parameters","level":1,"index":2,"id":"guardrails-gateway-config-parameters_{context}"},{"parentId":null,"name":"Deploying the Guardrails Orchestrator","level":1,"index":3,"id":"deploying-the-guardrails-orchestrator-service_{context}"},{"parentId":null,"name":"Auto-configuring Guardrails","level":1,"index":4,"id":"guardrails-auto-config_{context}"},{"parentId":null,"name":"Configuring the OpenTelemetry exporter","level":1,"index":5,"id":"configuring-the-opentelemetry-exporter_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-accelerators/"},"sections":[{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":"intel-gaudi-ai-accelerator-integration_managing-odh","name":"Enabling Intel Gaudi AI accelerators","level":2,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_managing-odh"},{"parentId":null,"name":"AMD GPU Integration","level":1,"index":2,"id":"amd-gpu-integration_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Verifying AMD GPU availability on your cluster","level":2,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Enabling AMD GPUs","level":2,"index":1,"id":"enabling-amd-gpus_managing-odh"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enforcing-local-queues/"},"sections":[{"parentId":null,"name":"Enforcing the local-queue labeling policy for all projects","level":1,"index":0,"id":"enforcing-lqlabel-all_{context}"},{"parentId":null,"name":"Disabling the local-queue labeling policy for all projects","level":1,"index":1,"id":"disabling-lqlabel-all_{context}"},{"parentId":null,"name":"Enforcing the local-queue labeling policy for some projects only","level":1,"index":2,"id":"enforcing-lqlabel-some_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-external-resource-access-for-lmeval-jobs/"},"sections":[{"parentId":null,"name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":1,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_{context}"},{"parentId":null,"name":"Updating LMEval job configuration using the web console","level":1,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-large-language-models/"},"sections":[{"parentId":null,"name":"Setting up LM-Eval","level":1,"index":0,"id":"setting-up-lmeval_{context}"},{"parentId":null,"name":"Enabling external resource access for LMEval jobs","level":1,"index":1,"id":"enabling-external-resource-access-for-lmeval-jobs_{context}"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_{context}","name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":2,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_{context}"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_{context}","name":"Updating LMEval job configuration using the web console","level":2,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_{context}"},{"parentId":null,"name":"LM-Eval evaluation job","level":1,"index":2,"id":"lmeval-evaluation-job_{context}"},{"parentId":null,"name":"LM-Eval evaluation job properties","level":1,"index":3,"id":"lmeval-evaluation-job-properties_{context}"},{"parentId":"lmeval-evaluation-job-properties_{context}","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":2,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":null,"name":"Performing model evaluations in the dashboard","level":1,"index":4,"id":"performing-model-evaluations-in-the-dashboard_{context}"},{"parentId":null,"name":"LM-Eval scenarios","level":1,"index":5,"id":"lmeval-scenarios_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Accessing Hugging Face models with an environment variable token","level":2,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using a custom Unitxt card","level":2,"index":1,"id":"using-a-custom-unitxt-card_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using PVCs as storage","level":2,"index":2,"id":"using-pvcs-as-storage_{context}"},{"parentId":"using-pvcs-as-storage_{context}","name":"Managed PVCs","level":3,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_{context}","name":"Existing PVCs","level":3,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_{context}","name":"Using a KServe Inference Service","level":2,"index":3,"id":"using-a-kserve-inference-service_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Setting up LM-Eval S3 Support","level":2,"index":4,"id":"setting-up-lmeval-s3-support_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using LLM-as-a-Judge metrics with LM-Eval","level":2,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-rag-systems-with-llama-stack/"},"sections":[{"parentId":null,"name":"Understanding RAG evaluation providers","level":1,"index":0,"id":"understanding-rag-evaluation-providers_{context}"},{"parentId":null,"name":"Using Ragas with Llama Stack","level":1,"index":1,"id":"using-ragas-with-llama-stack_{context}"},{"parentId":null,"name":"Benchmarking embedding models with BEIR datasets and Llama Stack","level":1,"index":2,"id":"benchmarking-embedding-models-with-BEIR-datasets-and-Llama-Stack_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/example-kfto-pytorch-training-scripts/"},"sections":[{"parentId":null,"name":"Example Training Operator PyTorch training script: NCCL","level":1,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training script: DDP","level":1,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training script: FSDP","level":1,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-rag-systems/"},"sections":[{"parentId":null,"name":"About Ragas evaluation","level":1,"index":0,"id":"_about_ragas_evaluation"},{"parentId":"_about_ragas_evaluation","name":"Key Ragas metrics","level":2,"index":0,"id":"_key_ragas_metrics"},{"parentId":"_about_ragas_evaluation","name":"Use cases for Ragas in AI engineering workflows","level":2,"index":1,"id":"_use_cases_for_ragas_in_ai_engineering_workflows"},{"parentId":"_about_ragas_evaluation","name":"Ragas provider deployment modes","level":2,"index":2,"id":"_ragas_provider_deployment_modes"},{"parentId":null,"name":"Setting up the Ragas inline provider for development","level":1,"index":1,"id":"setting-up-ragas-inline-provider_{context}"},{"parentId":null,"name":"Configuring the Ragas remote provider for production","level":1,"index":2,"id":"configuring-ragas-remote-provider-for-production_{context}"},{"parentId":null,"name":"Evaluating RAG system quality with Ragas metrics","level":1,"index":3,"id":"evaluating-rag-system-quality-with-ragas_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/experimenting-with-models-in-the-gen-ai-playground/"},"sections":[{"parentId":null,"name":"Playground overview","level":1,"index":0,"id":"playground-overview_rhoai-user"},{"parentId":"playground-overview_rhoai-user","name":"Core capabilities","level":2,"index":0,"id":"_core_capabilities"},{"parentId":null,"name":"Playground prerequisites","level":1,"index":1,"id":"playground-prerequisites_rhoai-user"},{"parentId":"playground-prerequisites_rhoai-user","name":"Cluster administrator prerequisites","level":2,"index":0,"id":"_cluster_administrator_prerequisites"},{"parentId":"playground-prerequisites_rhoai-user","name":"User prerequisites","level":2,"index":1,"id":"_user_prerequisites"},{"parentId":"playground-prerequisites_rhoai-user","name":"Model and runtime requirements for the playground","level":2,"index":2,"id":"_model_and_runtime_requirements_for_the_playground"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Key model selection factors","level":3,"index":0,"id":"_key_model_selection_factors"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Example model configuration","level":3,"index":1,"id":"_example_model_configuration"},{"parentId":"playground-prerequisites_rhoai-user","name":"Configuring Model Control Protocol (MCP) servers","level":2,"index":3,"id":"configuring-model-control-protocol-servers_rhoai-user"},{"parentId":null,"name":"About the AI assets endpoint page","level":1,"index":2,"id":"About-the-ai-assets-endpoint-page_rhoai-user"},{"parentId":null,"name":"Configuring a playground for your project","level":1,"index":3,"id":"configuring-a-playground-for-your-project_rhoai-user"},{"parentId":null,"name":"Testing baseline model responses","level":1,"index":4,"id":"testing-baseline-model-responses_rhoai-user"},{"parentId":null,"name":"Testing your model with retrieval augmented generation (RAG)","level":1,"index":5,"id":"testing-your-model-with-rag_rhoai-user"},{"parentId":"testing-your-model-with-rag_rhoai-user","name":"Understanding RAG settings","level":2,"index":0,"id":"understanding-rag-settings_rhoai-user"},{"parentId":null,"name":"Testing with model control protocol (MCP) servers","level":1,"index":6,"id":"testing-with-model-control-protocol-servers_rhoai-user"},{"parentId":null,"name":"Exporting your playground configuration","level":1,"index":7,"id":"exporting-your-playground-configuration_rhoai-user"},{"parentId":null,"name":"Updating your playground configuration","level":1,"index":8,"id":"updating-your-playground-configuration_rhoai-user"},{"parentId":null,"name":"Deleting a playground from your project","level":1,"index":9,"id":"Deleting-a-playground-from-your-project_rhoai-user"},{"parentId":null,"name":"Next steps","level":1,"index":10,"id":"next-steps_rhoai-user"},{"parentId":null,"name":"Troubleshooting playground issues","level":1,"index":11,"id":"troubleshooting-playground-issues_rhoai-user"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The chatbot thinks indefinitely","level":2,"index":0,"id":"_the_chatbot_thinks_indefinitely"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The model does not use RAG data","level":2,"index":1,"id":"_the_model_does_not_use_rag_data"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"MCP servers are missing from the UI","level":2,"index":2,"id":"_mcp_servers_are_missing_from_the_ui"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The model fails to call MCP tools","level":2,"index":3,"id":"_the_model_fails_to_call_mcp_tools"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/fine-tuning-a-model-by-using-kubeflow-training/"},"sections":[{"parentId":null,"name":"Configuring the fine-tuning job","level":1,"index":0,"id":"configuring-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Running the fine-tuning job","level":1,"index":1,"id":"running-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Deleting the fine-tuning job","level":1,"index":2,"id":"deleting-the-fine-tuning-job_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/generate-synthetic-data/"},"sections":[{"parentId":null,"name":"Explore the SDG Hub examples","level":1,"index":0,"id":"explore-the-sdg-hub-examples_{context}"},{"parentId":null,"name":"Guided example - Build a KFP pipeline for SDG","level":1,"index":1,"id":"guided-example-build-a-kfp-pipeline-for-sdg_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the dashboard","level":1,"index":3,"id":"accessing-the-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Configuring custom namespaces","level":1,"index":0,"id":"configuring-custom-namespaces"},{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":2,"id":"installing-odh-components_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/lmeval-scenarios/"},"sections":[{"parentId":null,"name":"Accessing Hugging Face models with an environment variable token","level":1,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_{context}"},{"parentId":null,"name":"Using a custom Unitxt card","level":1,"index":1,"id":"using-a-custom-unitxt-card_{context}"},{"parentId":null,"name":"Using PVCs as storage","level":1,"index":2,"id":"using-pvcs-as-storage_{context}"},{"parentId":"using-pvcs-as-storage_{context}","name":"Managed PVCs","level":2,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_{context}","name":"Existing PVCs","level":2,"index":1,"id":"_existing_pvcs"},{"parentId":null,"name":"Using a KServe Inference Service","level":1,"index":3,"id":"using-a-kserve-inference-service_{context}"},{"parentId":null,"name":"Setting up LM-Eval S3 Support","level":1,"index":4,"id":"setting-up-lmeval-s3-support_{context}"},{"parentId":null,"name":"Using LLM-as-a-Judge metrics with LM-Eval","level":1,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-access-to-projects/"},"sections":[{"parentId":null,"name":"Granting access to a project","level":1,"index":0,"id":"granting-access-to-a-project_{context}"},{"parentId":null,"name":"Updating access to a project","level":1,"index":1,"id":"updating-access-to-a-project_{context}"},{"parentId":null,"name":"Removing access to a project","level":1,"index":2,"id":"removing-access-to-a-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-ai-pipelines/"},"sections":[{"parentId":null,"name":"Configuring a pipeline server","level":1,"index":0,"id":"configuring-a-pipeline-server_{context}"},{"parentId":"configuring-a-pipeline-server_{context}","name":"Configuring a pipeline server with an external Amazon RDS database","level":2,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_{context}"},{"parentId":null,"name":"Defining a pipeline","level":1,"index":1,"id":"defining-a-pipeline_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Compiling the pipeline YAML with the Kubeflow Pipelines SDK","level":2,"index":0,"id":"compiling-the-pipeline-yaml-with-kfp-sdk_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Compiling Kubernetes-native manifests with the Kubeflow Pipelines SDK","level":2,"index":1,"id":"compiling-kubernetes-native-manifests-with-kfp-sdk_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Authenticating the Kubeflow Pipelines SDK with a pipeline server","level":2,"index":2,"id":"authenticating-kfp-sdk-with-pipeline-server_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Defining a pipeline by using the Kubernetes API","level":2,"index":3,"id":"defining-a-pipeline-by-using-the-kubernetes-api_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Migrating pipelines from database to Kubernetes API storage","level":2,"index":4,"id":"migrating-pipelines-from-database-to-kubernetes-api_{context}"},{"parentId":null,"name":"Importing a pipeline","level":1,"index":2,"id":"importing-a-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline","level":1,"index":3,"id":"deleting-a-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline server","level":1,"index":4,"id":"deleting-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline server","level":1,"index":5,"id":"viewing-the-details-of-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing existing pipelines","level":1,"index":6,"id":"viewing-existing-pipelines_{context}"},{"parentId":null,"name":"Overview of pipeline versions","level":1,"index":7,"id":"overview-of-pipeline-versions_{context}"},{"parentId":null,"name":"Uploading a pipeline version","level":1,"index":8,"id":"uploading-a-pipeline-version_{context}"},{"parentId":null,"name":"Deleting a pipeline version","level":1,"index":9,"id":"deleting-a-pipeline-version_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline version","level":1,"index":10,"id":"viewing-the-details-of-a-pipeline-version_{context}"},{"parentId":null,"name":"Downloading a pipeline version","level":1,"index":11,"id":"downloading-a-pipeline-version_{context}"},{"parentId":null,"name":"Overview of pipelines caching","level":1,"index":12,"id":"overview-of-pipelines-caching_{context}"},{"parentId":"overview-of-pipelines-caching_{context}","name":"Caching criteria","level":2,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-pipelines-caching_{context}","name":"Viewing cached steps in the {productname-short} user interface","level":2,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":"overview-of-pipelines-caching_{context}","name":"Controlling caching in pipelines","level":2,"index":2,"id":"controlling-caching-in-pipelines_{context}"},{"parentId":"controlling-caching-in-pipelines_{context}","name":"Disabling caching for individual tasks","level":3,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"controlling-caching-in-pipelines_{context}","name":"Disabling caching for a pipeline at submit time","level":3,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":"controlling-caching-in-pipelines_{context}","name":"Disabling caching for a pipeline at compile time","level":3,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":"controlling-caching-in-pipelines_{context}","name":"Disabling caching for all pipelines (pipeline server)","level":3,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Disabling applications connected to {productname-short}","level":1,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":null,"name":"Showing or hiding information about available applications","level":1,"index":3,"id":"showing-hiding-information-about-available-applications_dashboard"},{"parentId":null,"name":"Hiding the default basic workbench application","level":1,"index":4,"id":"hiding-the-default-basic-workbench-application_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-and-monitoring-models/"},"sections":[{"parentId":null,"name":"Adding a custom model-serving runtime","level":1,"index":0,"id":"adding-a-custom-model-serving-runtime_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models","level":0,"index":1,"id":"_managing_and_monitoring_models"},{"parentId":"_managing_and_monitoring_models","name":"Setting a timeout for KServe","level":1,"index":0,"id":"setting-timeout-for-kserve_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Deploying models by using multiple GPU nodes","level":1,"index":1,"id":"deploying-models-using-multiple-gpu-nodes_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Configuring an inference service for Kueue","level":1,"index":2,"id":"configuring-an-inference-service-for-kueue_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Configuring an inference service for Spyre","level":1,"index":3,"id":"configuring-inference-service-for-spyre_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Optimizing performance and tuning","level":1,"index":4,"id":"_optimizing_performance_and_tuning"},{"parentId":"_optimizing_performance_and_tuning","name":"Determining GPU requirements for LLM-powered applications","level":2,"index":0,"id":"determining-gpu-requirements-for-llm-powered-applications_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Performance considerations for text-summarization and retrieval-augmented generation (RAG) applications","level":2,"index":1,"id":"performance-considerations-for-document-based-apps_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Inference performance metrics","level":2,"index":2,"id":"inference-performance-metrics_cluster-admin"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Latency","level":3,"index":0,"id":"_latency"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Throughput","level":3,"index":1,"id":"_throughput"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Cost per million tokens","level":3,"index":2,"id":"_cost_per_million_tokens"},{"parentId":"_optimizing_performance_and_tuning","name":"Configuring metrics-based autoscaling","level":2,"index":3,"id":"configuring-metrics-based-autoscaling_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Guidelines for metrics-based autoscaling","level":2,"index":4,"id":"guidelines-for-metrics-based-autoscaling_cluster-admin"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing metrics for latency and throughput-optimized scaling","level":3,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing the right sliding window","level":3,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Optimizing HPA scale-down configuration","level":3,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Considering model size for optimal scaling","level":3,"index":3,"id":"_considering_model_size_for_optimal_scaling"},{"parentId":"_managing_and_monitoring_models","name":"Monitoring models","level":1,"index":5,"id":"_monitoring_models"},{"parentId":"_monitoring_models","name":"Configuring monitoring for the model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-the-model-serving-platform_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Using Grafana to monitor model performance","level":1,"index":6,"id":"_using_grafana_to_monitor_model_performance"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a Grafana metrics dashboard","level":2,"index":0,"id":"deploying-a-grafana-metrics-dashboard_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a vLLM/GPU metrics dashboard on a Grafana instance","level":2,"index":1,"id":"deploying-vllm-gpu-metrics-dashboard-grafana_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Grafana metrics","level":2,"index":2,"id":"ref-grafana-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"Accelerator metrics","level":3,"index":0,"id":"ref-accelerator-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"CPU metrics","level":3,"index":1,"id":"ref-cpu-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"vLLM metrics","level":3,"index":2,"id":"ref-vllm-metrics_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models on the NVIDIA NIM model serving platform","level":0,"index":2,"id":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Customizing model selection options for the NVIDIA NIM model serving platform","level":1,"index":0,"id":"Customizing-model-selection-options_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":1,"index":1,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin","name":"Enabling graph generation for an existing NIM deployment","level":2,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-basic-workbenches/"},"sections":[{"parentId":null,"name":"Accessing the administration interface for basic workbenches","level":1,"index":0,"id":"accessing-the-administration-interface-for-basic-workbenches_{context}"},{"parentId":null,"name":"Starting basic workbenches owned by other users","level":1,"index":1,"id":"starting-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing basic workbenches owned by other users","level":1,"index":2,"id":"accessing-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping basic workbenches owned by other users","level":1,"index":3,"id":"stopping-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle workbenches","level":1,"index":4,"id":"stopping-idle-workbenches_{context}"},{"parentId":null,"name":"Adding workbench pod tolerations","level":1,"index":5,"id":"adding-workbench-pod-tolerations_{context}"},{"parentId":null,"name":"Troubleshooting common problems in workbenches for administrators","level":1,"index":6,"id":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"A user&#8217;s workbench does not start","level":2,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-pvc-size/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-connection-types/"},"sections":[{"parentId":null,"name":"Viewing connection types","level":1,"index":0,"id":"viewing-connection-types_{context}"},{"parentId":null,"name":"Creating a connection type","level":1,"index":1,"id":"creating-a-connection-type_{context}"},{"parentId":null,"name":"Duplicating a connection type","level":1,"index":2,"id":"duplicating-a-connection-type_{context}"},{"parentId":null,"name":"Editing a connection type","level":1,"index":3,"id":"editing-a-connection-type_{context}"},{"parentId":null,"name":"Enabling a connection type","level":1,"index":4,"id":"enabling-a-connection-type_{context}"},{"parentId":null,"name":"Deleting a connection type","level":1,"index":5,"id":"deleting-a-connection-type_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-custom-training-images/"},"sections":[{"parentId":null,"name":"About base training images","level":1,"index":0,"id":"about-base-training-images_{context}"},{"parentId":null,"name":"Creating a custom training image","level":1,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":null,"name":"Pushing an image to the integrated OpenShift image registry","level":1,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-distributed-workloads/"},"sections":[{"parentId":null,"name":"Configuring quota management for distributed workloads","level":1,"index":0,"id":"configuring-quota-management-for-distributed-workloads_{context}"},{"parentId":null,"name":"Example Kueue resource configurations for distributed workloads","level":1,"index":1,"id":"ref-example-kueue-resource-configurations_{context}"},{"parentId":"ref-example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs without shared cohort","level":2,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":3,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":3,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":3,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":3,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"ref-example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":2,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":3,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":3,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":3,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":3,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":null,"name":"Configuring a cluster for RDMA","level":1,"index":2,"id":"configuring-a-cluster-for-rdma_{context}"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for administrators","level":1,"index":3,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a suspended state","level":2,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a failed state","level":2,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster does not start","level":2,"index":2,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user cannot create a Ray cluster or submit jobs","level":2,"index":3,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"Additional resources","level":2,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-observability/"},"sections":[{"parentId":null,"name":"Enabling the observability stack","level":1,"index":0,"id":"enabling-the-observability-stack_{context}"},{"parentId":null,"name":"Collecting metrics from user workloads","level":1,"index":1,"id":"collecting-metrics-from-user-workloads_{context}"},{"parentId":null,"name":"Exporting metrics to external observability tools","level":1,"index":2,"id":"exporting-metrics-to-external-observability-tools_{context}"},{"parentId":null,"name":"Viewing traces in external tracing platforms","level":1,"index":3,"id":"viewing-traces-in-external-tracing-platforms_{context}"},{"parentId":null,"name":"Accessing built-in alerts","level":1,"index":4,"id":"accessing-built-in-alerts_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-experiments/"},"sections":[{"parentId":null,"name":"Overview of pipeline experiments","level":1,"index":0,"id":"overview-of-pipeline-experiments_{context}"},{"parentId":null,"name":"Creating a pipeline experiment","level":1,"index":1,"id":"creating-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Archiving a pipeline experiment","level":1,"index":2,"id":"archiving-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Deleting an archived pipeline experiment","level":1,"index":3,"id":"deleting-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Restoring an archived pipeline experiment","level":1,"index":4,"id":"restoring-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Viewing pipeline task executions","level":1,"index":5,"id":"viewing-pipeline-task-executions_{context}"},{"parentId":null,"name":"Viewing pipeline artifacts","level":1,"index":6,"id":"viewing-pipeline-artifacts_{context}"},{"parentId":null,"name":"Comparing runs in an experiment","level":1,"index":7,"id":"comparing-runs-in-an-experiment_{context}"},{"parentId":null,"name":"Comparing runs in different experiments","level":1,"index":8,"id":"comparing-runs-in-different-experiments_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-runs/"},"sections":[{"parentId":null,"name":"Overview of pipeline runs","level":1,"index":0,"id":"overview-of-pipeline-runs_{context}"},{"parentId":null,"name":"Storing data with pipelines","level":1,"index":1,"id":"storing-data-with-pipelines_{context}"},{"parentId":null,"name":"Understanding pipeline run workspaces","level":1,"index":2,"id":"configuring-pipeline-run-workspaces_{context}"},{"parentId":"configuring-pipeline-run-workspaces_{context}","name":"Configuring default workspace PVC settings in DSPA","level":2,"index":0,"id":"configuring-default-workspace-pvc-settings-in-dspa_{context}"},{"parentId":"configuring-pipeline-run-workspaces_{context}","name":"Adding external artifacts to pipeline run workspaces","level":2,"index":1,"id":"adding-external-artifacts-to-pipeline-run-workspaces_{context}"},{"parentId":null,"name":"Viewing active pipeline runs","level":1,"index":3,"id":"viewing-active-pipeline-runs_{context}"},{"parentId":null,"name":"Executing a pipeline run","level":1,"index":4,"id":"executing-a-pipeline-run_{context}"},{"parentId":null,"name":"Stopping an active pipeline run","level":1,"index":5,"id":"stopping-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an active pipeline run","level":1,"index":6,"id":"duplicating-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Viewing scheduled pipeline runs","level":1,"index":7,"id":"viewing-scheduled-pipeline-runs_{context}"},{"parentId":null,"name":"Scheduling a pipeline run using a cron job","level":1,"index":8,"id":"scheduling-a-pipeline-run-using-a-cron-job_{context}"},{"parentId":null,"name":"Scheduling a pipeline run","level":1,"index":9,"id":"scheduling-a-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating a scheduled pipeline run","level":1,"index":10,"id":"duplicating-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Deleting a scheduled pipeline run","level":1,"index":11,"id":"deleting-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline run","level":1,"index":12,"id":"viewing-the-details-of-a-pipeline-run_{context}"},{"parentId":null,"name":"Viewing archived pipeline runs","level":1,"index":13,"id":"viewing-archived-pipeline-runs_{context}"},{"parentId":null,"name":"Archiving a pipeline run","level":1,"index":14,"id":"archiving-a-pipeline-run_{context}"},{"parentId":null,"name":"Restoring an archived pipeline run","level":1,"index":15,"id":"restoring-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Deleting an archived pipeline run","level":1,"index":16,"id":"deleting-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an archived pipeline run","level":1,"index":17,"id":"duplicating-an-archived-pipeline-run_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages-in-code-server/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your code-server workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your code-server workbench","level":1,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your workbench","level":1,"index":1,"id":"installing-python-packages-on-your-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-storage-classes/"},"sections":[{"parentId":null,"name":"About persistent storage","level":1,"index":0,"id":"about-persistent-storage_{context}"},{"parentId":"about-persistent-storage_{context}","name":"Storage classes in {productname-short}","level":2,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":"about-persistent-storage_{context}","name":"Access modes","level":2,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":3,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":null,"name":"Configuring storage class settings","level":1,"index":1,"id":"configuring-storage-class-settings_{context}"},{"parentId":null,"name":"Configuring the default storage class for your cluster","level":1,"index":2,"id":"configuring-the-default-storage-class-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":3,"id":"overview-of-object-storage-endpoints_{context}"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-users-and-groups/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":1,"id":"viewing-data-science-users_{context}"},{"parentId":null,"name":"Adding users to {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-user-groups_{context}"},{"parentId":null,"name":"Selecting {productname-short} administrator and user groups","level":1,"index":3,"id":"selecting-admin-and-user-groups_{context}"},{"parentId":null,"name":"Deleting users","level":1,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":"_deleting_users","name":"Stopping basic workbenches owned by other users","level":2,"index":1,"id":"stopping-basic-workbenches-owned-by-other-users_{context}"},{"parentId":"_deleting_users","name":"Revoking user access to basic workbenches","level":2,"index":2,"id":"revoking-user-access-to-basic-workbenches_{context}"},{"parentId":"_deleting_users","name":"Backing up storage data","level":2,"index":3,"id":"backing-up-storage-data_{context}"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Overview of managing workloads with Kueue","level":1,"index":0,"id":"overview-of-managing-workloads-with-kueue_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue management states","level":2,"index":0,"id":"_kueue_management_states"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Queue enforcement for projects","level":2,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Restrictions for managing workloads with Kueue","level":2,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue workflow","level":2,"index":3,"id":"kueue-workflow_kueue"},{"parentId":null,"name":"Configuring workload management with Kueue","level":1,"index":1,"id":"configuring-workload-management-with-kueue_kueue"},{"parentId":"configuring-workload-management-with-kueue_kueue","name":"Enabling Kueue in the dashboard","level":2,"index":0,"id":"enabling-kueue-in-the-dashboard_kueue"},{"parentId":null,"name":"Troubleshooting common problems with Kueue","level":1,"index":2,"id":"troubleshooting-common-problems-with-Kueue_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"failed to call webhook\" error message for Kueue","level":2,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":2,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"local_queue provided does not exist\" error message","level":2,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"The pod provisioned by Kueue is terminated before the image is pulled","level":2,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"Additional resources","level":2,"index":4,"id":"_additional_resources"},{"parentId":null,"name":"Migrating to the {rhbok-productname} Operator","level":1,"index":3,"id":"migrating-to-the-rhbok-operator_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-data-drift/"},"sections":[{"parentId":null,"name":"Creating a drift metric","level":1,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":2,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Deleting a drift metric by using the CLI","level":1,"index":1,"id":"deleting-a-drift-metric-by-using-cli_drift-monitoring"},{"parentId":null,"name":"Viewing drift metrics for a model","level":1,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using drift metrics","level":1,"index":3,"id":"using-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using a drift metric in a credit card scenario","level":1,"index":4,"id":"using-a-drift-metric-in-a-credit-card-scenario_drift-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Viewing project metrics for distributed workloads","level":1,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing the status of distributed workloads","level":1,"index":1,"id":"viewing-the-status-of-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing Kueue alerts for distributed workloads","level":1,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-bias/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Using bias metrics","level":1,"index":3,"id":"using-bias-metrics_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/overview-of-ml-features-and-feature-store/"},"sections":[{"parentId":null,"name":"Audience for Feature Store","level":1,"index":0,"id":"audience-for-feature-store_{context}"},{"parentId":null,"name":"Overview of machine learning features","level":1,"index":1,"id":"overview-of-machine-learning-features_{context}"},{"parentId":null,"name":"Overview of Feature Store","level":1,"index":2,"id":"overview-of-feature-store_{context}"},{"parentId":null,"name":"Feature Store workflow","level":1,"index":3,"id":"feature-store-workflow_{context}"},{"parentId":null,"name":"Setting up the Feature Store user interface for initial use","level":1,"index":4,"id":"setting-up-feature-store-UI_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":5,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/prepare-your-data-for-ai-consumption/"},"sections":[{"parentId":null,"name":"Process data by using Docling","level":1,"index":0,"id":"_process_data_by_using_docling"},{"parentId":null,"name":"Explore the data processing examples","level":1,"index":1,"id":"explore-the-data-processing-examples_{context}"},{"parentId":null,"name":"Automate data processing steps by building AI pipelines","level":1,"index":2,"id":"_automate_data_processing_steps_by_building_ai_pipelines"},{"parentId":null,"name":"Explore the kubeflow pipeline examples","level":1,"index":3,"id":"explore-the-kubeflow-pipeline-examples_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/preparing-the-distributed-training-environment/"},"sections":[{"parentId":null,"name":"Creating a workbench for distributed training","level":1,"index":0,"id":"creating-a-workbench-for-distributed-training_{context}"},{"parentId":null,"name":"Using the cluster server and token to authenticate","level":1,"index":1,"id":"using-the-cluster-server-and-token-to-authenticate_{context}"},{"parentId":null,"name":"Managing custom training images","level":1,"index":2,"id":"managing-custom-training-images_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"About base training images","level":2,"index":0,"id":"about-base-training-images_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"Creating a custom training image","level":2,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"Pushing an image to the integrated OpenShift image registry","level":2,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/retrieving-features-for-model-training/"},"sections":[{"parentId":null,"name":"Retrieving data science features","level":1,"index":0,"id":"retrieving-data-science-features_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-kfto-based-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Using the Kubeflow Training Operator to run distributed training workloads","level":1,"index":0,"id":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":2,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorchJob resource","level":2,"index":1,"id":"creating-a-kfto-pytorchjob-resource_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":2,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Training Operator PyTorch training scripts","level":2,"index":3,"id":"example-kfto-pytorch-training-scripts_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: NCCL","level":3,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: DDP","level":3,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: FSDP","level":3,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Dockerfile for a Training Operator PyTorch training script","level":2,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Training Operator PyTorchJob resource for multi-node training","level":2,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_{context}"},{"parentId":null,"name":"Using the Training Operator SDK to run distributed training workloads","level":1,"index":1,"id":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"Configuring a training job by using the Training Operator SDK","level":2,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"Running a training job by using the Training Operator SDK","level":2,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"TrainingClient API: Job-related methods","level":2,"index":2,"id":"ref-trainingclient-api-job-related-methods_{context}"},{"parentId":null,"name":"Fine-tuning a model by using Kubeflow Training","level":1,"index":2,"id":"fine-tuning-a-model-by-using-kubeflow-training_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Configuring the fine-tuning job","level":2,"index":0,"id":"configuring-the-fine-tuning-job_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Running the fine-tuning job","level":2,"index":1,"id":"running-the-fine-tuning-job_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Deleting the fine-tuning job","level":2,"index":2,"id":"deleting-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Creating a multi-node PyTorch training job with RDMA","level":1,"index":3,"id":"creating-a-multi-node-pytorch-training-job-with-rdma_{context}"},{"parentId":null,"name":"Example Training Operator PyTorchJob resource configured to run with RDMA","level":1,"index":4,"id":"ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-ray-based-distributed-workloads/"},"sections":[{"parentId":null,"name":"Running distributed data science workloads from Jupyter notebooks","level":1,"index":0,"id":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Downloading the demo Jupyter notebooks from the CodeFlare SDK","level":2,"index":0,"id":"downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Running the demo Jupyter notebooks from the CodeFlare SDK","level":2,"index":1,"id":"running-the-demo-jupyter-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Managing Ray clusters from within a Jupyter notebook","level":2,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Running distributed data science workloads from AI pipelines","level":1,"index":1,"id":"running-distributed-data-science-workloads-from-ai-pipelines_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/set-up-your-working-environment/"},"sections":[{"parentId":null,"name":"About the {org-name} Python Index","level":1,"index":0,"id":"about-the-python-index_{context}"},{"parentId":null,"name":"Mirror the Python Index for your disconnected environment","level":1,"index":1,"id":"mirror-the-python-index_{context}"},{"parentId":null,"name":"Install packages","level":1,"index":2,"id":"install-packages_{context}"},{"parentId":null,"name":"Import example notebooks","level":1,"index":3,"id":"import-example-notebooks_{context}"},{"parentId":"import-example-notebooks_{context}","name":"Clone an example Git repository","level":2,"index":0,"id":"clone-an-example-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-feature-store/"},"sections":[{"parentId":null,"name":"Before you begin","level":1,"index":0,"id":"before-you-begin_{context}"},{"parentId":null,"name":"Enabling the Feature Store component","level":1,"index":1,"id":"enabling-the-feature-store-component_{context}"},{"parentId":null,"name":"Creating a Feature Store instance in a project","level":1,"index":2,"id":"creating-a-feature-store-instance-in-a-project_{context}"},{"parentId":null,"name":"Configuring and managing Role Based Access Control","level":1,"index":3,"id":"configuring-and-managing-role-based-access-control_{context}"},{"parentId":null,"name":"Adding feature definitions and initializing your Feature Store instance","level":1,"index":4,"id":"adding-feature-definitions-and-initializing-your-feature-store-instance_{context}"},{"parentId":"adding-feature-definitions-and-initializing-your-feature-store-instance_{context}","name":"Specifying files to ignore","level":2,"index":0,"id":"specifying-files-to-ignore_{context}"},{"parentId":null,"name":"Viewing Feature Store objects in the web-based UI","level":1,"index":5,"id":"viewing-feature-store-objects-in-the-web-based-ui_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-trustyai-for-your-project/"},"sections":[{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":0,"id":"authenticating-trustyai-service_{context}"},{"parentId":null,"name":"Uploading training data to TrustyAI","level":1,"index":1,"id":"uploading-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Sending training data to TrustyAI","level":1,"index":2,"id":"sending-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Labeling data fields","level":1,"index":3,"id":"labeling-data-fields_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/train-the-model-by-using-your-prepared-data/"},"sections":[{"parentId":null,"name":"Explore the Training Hub examples","level":1,"index":0,"id":"explore-the-training-hub-examples_{context}"},{"parentId":null,"name":"Estimate memory usage","level":1,"index":1,"id":"estimate-memory-usage_{context}"},{"parentId":null,"name":"Compare the performance of OSFT and SFT training algorithms","level":1,"index":2,"id":"compare-the-performance-of-osft-and-sft_{context}"},{"parentId":null,"name":"Distribute training jobs by using the KubeFlow Trainer Operator","level":1,"index":3,"id":"_distribute_training_jobs_by_using_the_kubeflow_trainer_operator"},{"parentId":null,"name":"Distributed fine-tuning with Training Hub and Kubeflow Trainer","level":1,"index":4,"id":"_distributed_fine_tuning_with_training_hub_and_kubeflow_trainer"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 1","level":1,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":null,"name":"Upgrading the Open Data Hub Operator","level":1,"index":1,"id":"upgrading-the-odh-operator_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 2","level":1,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":null,"name":"Upgrading the Open Data Hub Operator","level":1,"index":1,"id":"upgrading-the-odh-operator_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-basic-workbenches/"},"sections":[{"parentId":null,"name":"Starting a basic workbench","level":1,"index":0,"id":"starting-a-basic-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-connections/"},"sections":[{"parentId":null,"name":"Adding a connection to your project","level":1,"index":0,"id":"adding-a-connection-to-your-project_{context}"},{"parentId":null,"name":"Updating a connection","level":1,"index":1,"id":"updating-a-connection_{context}"},{"parentId":null,"name":"Deleting a connection","level":1,"index":2,"id":"deleting-a-connection_{context}"},{"parentId":null,"name":"Using the connections API","level":1,"index":3,"id":"using-connections-api_{context}"},{"parentId":"using-connections-api_{context}","name":"Namespace isolation in connections API","level":2,"index":0,"id":"_namespace_isolation_in_connections_api"},{"parentId":"using-connections-api_{context}","name":"Role-based access control (RBAC) requirements in connections API","level":2,"index":1,"id":"_role_based_access_control_rbac_requirements_in_connections_api"},{"parentId":"using-connections-api_{context}","name":"Validation scope","level":2,"index":2,"id":"_validation_scope"},{"parentId":"using-connections-api_{context}","name":"Using connection annotations based on workload type","level":2,"index":3,"id":"_using_connection_annotations_based_on_workload_type"},{"parentId":"using-connections-api_{context}","name":"Creating an Amazon S3-compatible connection type using the connections API","level":2,"index":4,"id":"creating-s3-compatible-connection-type-api_{context}"},{"parentId":"creating-s3-compatible-connection-type-api_{context}","name":"Using an Amazon S3 connection with <code>InferenceService</code> custom resource","level":3,"index":0,"id":"_using_an_amazon_s3_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-s3-compatible-connection-type-api_{context}","name":"Using an Amazon S3 connection with <code>LLMInferenceService</code> custom resource","level":3,"index":1,"id":"_using_an_amazon_s3_connection_with_llminferenceservice_custom_resource"},{"parentId":"using-connections-api_{context}","name":"Creating a URI-compatible connection type using the connections API","level":2,"index":5,"id":"creating-uri-compatible-connection-type-api_{context}"},{"parentId":"creating-uri-compatible-connection-type-api_{context}","name":"Using a URI connection with <code>InferenceService</code> custom resource","level":3,"index":0,"id":"_using_a_uri_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-uri-compatible-connection-type-api_{context}","name":"Using a URI connection with <code>LLMInferenceService</code> custom resource","level":3,"index":1,"id":"_using_a_uri_connection_with_llminferenceservice_custom_resource"},{"parentId":"using-connections-api_{context}","name":"Creating an OCI-compatible connection type using the connections API","level":2,"index":6,"id":"creating-oci-compatible-connection-type-api_{context}"},{"parentId":"creating-oci-compatible-connection-type-api_{context}","name":"Using an OCI connection with <code>InferenceService</code> custom resource","level":3,"index":0,"id":"_using_an_oci_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-oci-compatible-connection-type-api_{context}","name":"Using an OCI connection with <code>LLMInferenceService</code> custom resource","level":3,"index":1,"id":"_using_an_oci_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-explainability/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation","level":1,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":2,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":null,"name":"Requesting a SHAP explanation","level":1,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":2,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":null,"name":"Using explainers","level":1,"index":2,"id":"using-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-guardrails-for-ai-safety/"},"sections":[{"parentId":null,"name":"Detecting PII and sensitive data","level":1,"index":0,"id":"_detecting_pii_and_sensitive_data"},{"parentId":null,"name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":1,"index":1,"id":"detecting-pii-by-using-guardrails-with-llama-stack_{context}"},{"parentId":null,"name":"Filtering flagged content by sending requests to the regex detector","level":1,"index":2,"id":"filtering-flagged-content-by-sending-requests-to-the-regex-detector_{context}"},{"parentId":null,"name":"Securing prompts","level":1,"index":3,"id":"_securing_prompts"},{"parentId":null,"name":"Mitigating Prompt Injection by using a Hugging Face Prompt Injection detector","level":1,"index":4,"id":"mitigating-prompt-injection-by-using-a-hugging-face-prompt-injection-detector_{context}"},{"parentId":null,"name":"Moderating and safeguarding content","level":1,"index":5,"id":"_moderating_and_safeguarding_content"},{"parentId":null,"name":"Detecting hateful and profane language","level":1,"index":6,"id":"detecting-hateful-and-profane-language_{context}"},{"parentId":null,"name":"Enforcing configured safety pipelines for LLM inference by using Guardrails Gateway","level":1,"index":7,"id":"enforcing-configured-safety-pipelines-for-llm-inference-using-guardrails-gateway_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-llama-stack-with-trustyai/"},"sections":[{"parentId":null,"name":"Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI","level":1,"index":0,"id":"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_{context}"},{"parentId":null,"name":"Running custom evaluations with LM-Eval and Llama Stack","level":1,"index":1,"id":"running-custom-evaluations-with-LMEval-and-llama-stack_{context}"},{"parentId":null,"name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":1,"index":2,"id":"detecting-pii-by-using-guardrails-with-llama-stack_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-project-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":0,"id":"creating-a-workbench-select-ide_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Starting a workbench","level":1,"index":1,"id":"starting-a-workbench_{context}"},{"parentId":null,"name":"Updating a project workbench","level":1,"index":2,"id":"updating-a-project-workbench_{context}"},{"parentId":null,"name":"Deleting a workbench from a project","level":1,"index":3,"id":"deleting-a-workbench-from-a-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-projects/"},"sections":[{"parentId":null,"name":"Creating a project","level":1,"index":0,"id":"creating-a-project_{context}"},{"parentId":null,"name":"Updating a project","level":1,"index":1,"id":"updating-a-project_{context}"},{"parentId":null,"name":"Deleting a project","level":1,"index":2,"id":"deleting-a-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-the-kfto-sdk-to-run-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Configuring a training job by using the Training Operator SDK","level":1,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":null,"name":"Running a training job by using the Training Operator SDK","level":1,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":null,"name":"TrainingClient API: Job-related methods","level":1,"index":2,"id":"ref-trainingclient-api-job-related-methods_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-the-kubeflow-training-operator-to-run-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":1,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_{context}"},{"parentId":null,"name":"Creating a Training Operator PyTorchJob resource","level":1,"index":1,"id":"creating-a-kfto-pytorchjob-resource_{context}"},{"parentId":null,"name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":1,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training scripts","level":1,"index":3,"id":"example-kfto-pytorch-training-scripts_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: NCCL","level":2,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: DDP","level":2,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: FSDP","level":2,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"},{"parentId":null,"name":"Example Dockerfile for a Training Operator PyTorch training script","level":1,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_{context}"},{"parentId":null,"name":"Example Training Operator PyTorchJob resource for multi-node training","level":1,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/viewing-logs-and-audit-records/"},"sections":[{"parentId":null,"name":"Configuring the {productname-short} Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_{context}"},{"parentId":"configuring-the-operator-logger_{context}","name":"Viewing the {productname-short} Operator logs","level":2,"index":0,"id":"_viewing_the_productname_short_operator_logs"},{"parentId":null,"name":"Viewing audit records","level":1,"index":1,"id":"viewing-audit-records_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding how {productname-short} handles certificates","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":null,"name":"Adding certificates","level":1,"index":1,"id":"_adding_certificates"},{"parentId":null,"name":"Adding certificates to a cluster-wide CA bundle","level":1,"index":2,"id":"adding-certificates-to-a-cluster-ca-bundle_certs"},{"parentId":null,"name":"Adding certificates to a custom CA bundle","level":1,"index":3,"id":"adding-certificates-to-a-custom-ca-bundle_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":4,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Accessing S3-compatible object storage with self-signed certificates","level":2,"index":0,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Configuring a certificate for pipelines","level":2,"index":1,"id":"configuring-a-certificate-for-pipelines_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Configuring a certificate for workbenches","level":2,"index":2,"id":"configuring-a-certificate-for-workbenches_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using the cluster-wide CA bundle for the model serving platform","level":2,"index":3,"id":"using-the-cluster-CA-bundle-for-model-serving_certs"},{"parentId":null,"name":"Managing certificates without the {productname-long} Operator","level":1,"index":5,"id":"managing-certificates-without-the-operator_certs"},{"parentId":null,"name":"Removing the CA bundle","level":1,"index":6,"id":"_removing_the_ca_bundle"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from all namespaces","level":2,"index":0,"id":"removing-the-ca-bundle-from-all-namespaces_certs"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from a single namespace","level":2,"index":1,"id":"removing-the-ca-bundle-from-a-single-namespace_certs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":0,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Registering a model from the dashboard","level":1,"index":0,"id":"registering-a-model_model-registry"},{"parentId":null,"name":"Registering a model version","level":1,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":null,"name":"Viewing registered models","level":1,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":null,"name":"Viewing registered model versions","level":1,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":null,"name":"Editing model metadata in a model registry","level":1,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Editing model version metadata in a model registry","level":1,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Deploying a model version from a model registry","level":1,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Editing the deployment properties of a deployed model version from a model registry","level":1,"index":7,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the model serving platform","level":2,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-model-serving-platform_model-registry"},{"parentId":null,"name":"Deleting a deployed model version from a model registry","level":1,"index":8,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Archiving a model","level":1,"index":9,"id":"archiving-a-model_model-registry"},{"parentId":null,"name":"Archiving a model version","level":1,"index":10,"id":"archiving-a-model-version_model-registry"},{"parentId":null,"name":"Restoring a model","level":1,"index":11,"id":"restoring-a-model_model-registry"},{"parentId":null,"name":"Restoring a model version","level":1,"index":12,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipeline-logs/"},"sections":[{"parentId":null,"name":"About pipeline logs","level":1,"index":0,"id":"about-pipeline-logs_{context}"},{"parentId":null,"name":"Viewing pipeline step logs","level":1,"index":1,"id":"viewing-pipeline-step-logs_{context}"},{"parentId":null,"name":"Downloading pipeline step logs","level":1,"index":2,"id":"downloading-pipeline-step-logs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipelines-in-jupyterlab/"},"sections":[{"parentId":null,"name":"Overview of pipelines in JupyterLab","level":1,"index":0,"id":"overview-of-pipelines-in-jupyterlab_{context}"},{"parentId":null,"name":"Accessing the pipeline editor","level":1,"index":1,"id":"accessing-the-pipeline-editor_{context}"},{"parentId":null,"name":"Disabling node caching in Elyra","level":1,"index":2,"id":"disabling-node-caching-in-elyra_{context}"},{"parentId":null,"name":"Creating a runtime configuration","level":1,"index":3,"id":"creating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Updating a runtime configuration","level":1,"index":4,"id":"updating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Deleting a runtime configuration","level":1,"index":5,"id":"deleting-a-runtime-configuration_{context}"},{"parentId":null,"name":"Duplicating a runtime configuration","level":1,"index":6,"id":"duplicating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Running a pipeline in JupyterLab","level":1,"index":7,"id":"running-a-pipeline-in-jupyterlab_{context}"},{"parentId":null,"name":"Exporting a pipeline in JupyterLab","level":1,"index":8,"id":"exporting-a-pipeline-in-jupyterlab_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-the-model-catalog/"},"sections":[{"parentId":null,"name":"Discovering and evaluating models in the model catalog","level":1,"index":0,"id":"viewing-models-in-the-catalog_model-registry"},{"parentId":null,"name":"Registering a model from the model catalog","level":1,"index":1,"id":"registering-a-model-from-the-model-catalog_model-registry"},{"parentId":null,"name":"Deploying a model from the model catalog","level":1,"index":2,"id":"deploying-a-model-from-the-model-catalog_model-registry"},{"parentId":null,"name":"Configuring model catalog sources in OpenShift","level":1,"index":3,"id":"configuring-model-catalog-sources-in-openshift_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-centralized-auth-oidc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-feature-definitions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-kserve-deployment-modes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-llama-stack-search-types/"},"sections":[{"parentId":null,"name":"Supported search modes","level":1,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":2,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":2,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":2,"index":2,"id":"_hybrid_search"},{"parentId":null,"name":"Retrieval database support","level":1,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Model serving platform","level":1,"index":0,"id":"_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":1,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-organizing-features-by-using-entities/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-persistent-storage/"},"sections":[{"parentId":null,"name":"Storage classes in {productname-short}","level":1,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":null,"name":"Access modes","level":1,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":2,"index":0,"id":"_using_shared_storage_rwx"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-ai-assets-endpoints-page/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-python-index/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authentication-token-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-built-in-alerts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-hugging-face-models-with-an-environment-variable-token/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-administration-interface-for-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/activating-the-llama-stack-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-connection-to-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-certificates-to-a-cluster-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-certificates-to-a-custom-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-external-artifacts-to-pipeline-run-workspaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-feature-definitions-and-initializing-your-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-workbench-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-custom-image-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-workbench-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-workbench-overview/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/audience-for-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/auth-on-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-kfp-sdk-with-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/before-you-begin/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/benchmarking-embedding-models-with-beir-datasets-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/clone-an-example-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/collecting-metrics-from-user-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/compare-the-performance-of-osft-and-sft/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/compiling-kubernetes-native-manifests-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/compiling-the-pipeline-yaml-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-certificate-for-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-certificate-for-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-cluster-for-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-playground-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-offline-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-online-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-and-managing-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-authentication-for-llmd/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-default-workspace-pvc-settings-in-dspa/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-mcp-servers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-metric-based-autoscaling/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-model-catalog-sources-in-openshift/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-oidc-auth-gateway-api/"},"sections":[{"parentId":null,"name":"Security considerations","level":1,"index":0,"id":"_security_considerations"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-pipelines-with-your-own-argo-workflows-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-ragas-remote-provider-for-production/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-built-in-detector-and-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-feature-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator logs","level":1,"index":0,"id":"_viewing_the_productname_short_operator_logs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-workload-management-with-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/controlling-caching-in-pipelines/"},"sections":[{"parentId":null,"name":"Disabling caching for individual tasks","level":1,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":null,"name":"Disabling caching for a pipeline at submit time","level":1,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":null,"name":"Disabling caching for a pipeline at compile time","level":1,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":null,"name":"Disabling caching for all pipelines (pipeline server)","level":1,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-feature-store-instance-in-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorch-training-script-configmap-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorchjob-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-multi-node-pytorch-training-job-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-for-distributed-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-feature-views/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-oci-compatible-connection-types-api/"},"sections":[{"parentId":null,"name":"Using an OCI connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_an_oci_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using an OCI connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_an_oci_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-project-scoped-resources-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-project-scoped-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-s3-compatible-connection-type-api/"},"sections":[{"parentId":null,"name":"Using an Amazon S3 connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_an_amazon_s3_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using an Amazon S3 connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_an_amazon_s3_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-uri-compatible-connection-type-api/"},"sections":[{"parentId":null,"name":"Using a URI connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_a_uri_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using a URI connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_a_uri_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-model-selection-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline-by-using-the-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-drift-metric-by-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-inference-service-for-spyre/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-inference-service-for-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-playground-from-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-llama-model-with-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-llamastackdistribution-instance/"},"sections":[{"parentId":null,"name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":1,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":null,"name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":1,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":null,"name":"Example C: LlamaStackDistribution with <strong>Inline FAISS</strong>","level":1,"index":2,"id":"_example_c_llamastackdistribution_with_inline_faiss"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-remote-milvus-vector-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-model-stored-in-oci-image-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-distributed-inference/"},"sections":[{"parentId":null,"name":"Configuring authentication for {llmd} using {org-name} Connectivity Link","level":1,"index":0,"id":"configuring-authentication-for-llmd_{context}"},{"parentId":null,"name":"Enabling {llmd}","level":1,"index":1,"id":"enabling-distributed-inference_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-vllm-gpu-metrics-dashboard-grafana/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/detecting-hateful-and-profane-language/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/determining-gpu-requirements-for-llm-powered-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/detecting-pii-by-using-guardrails-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-node-caching-in-elyra/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-an-existing-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/emptying-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-automatic-authentication-and-publishing-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-distributed-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-kueue-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-feature-store-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-observability-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-kserve-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/end-to-end-model-customization-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/estimate-memory-usage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/evaluating-rag-system-quality-with-ragas/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/explore-the-data-processing-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/explore-the-kubeflow-pipeline-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/explore-the-sdg-hub-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/explore-the-training-hub-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-metrics-to-external-observability-tools/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-your-playground-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/feature-store-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/glossary-of-common-terms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/granting-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-auto-config/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-configuring-the-hugging-face-detector-serving-runtime/"},"sections":[{"parentId":null,"name":"Guardrails Detector Hugging Face serving runtime configuration values","level":1,"index":0,"id":"_guardrails_detector_hugging_face_serving_runtime_configuration_values"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-enforcing-configured-safety-pipelines-for-llm-inference-using-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-filtering-flagged-content-by-sending-requests-to-the-regex-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-gateway-config-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-configmap-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-detectors/"},"sections":[{"parentId":null,"name":"Built-in Detector","level":1,"index":0,"id":"_built_in_detector"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-parameters/"},"sections":[{"parentId":null,"name":"Gateway Pa","level":0,"index":0,"id":"_gateway_pa"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guided-example-build-a-kfp-pipeline-for-sdg/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guidelines-for-metrics-based-autoscaling/"},"sections":[{"parentId":null,"name":"Choosing metrics for latency and throughput-optimized scaling","level":1,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":null,"name":"Choosing the right sliding window","level":1,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":null,"name":"Optimizing HPA scale-down configuration","level":1,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":null,"name":"Considering model size for optimal scaling","level":1,"index":3,"id":"_considering_model_size_for_optimal_scaling"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-basic-workbench-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ibm-spyre-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/import-example-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/inference-performance-metrics/"},"sections":[{"parentId":null,"name":"Latency","level":1,"index":0,"id":"_latency"},{"parentId":null,"name":"Throughput","level":1,"index":1,"id":"_throughput"},{"parentId":null,"name":"Cost per million tokens","level":1,"index":2,"id":"_cost_per_million_tokens"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ingesting-content-into-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/install-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/kueue-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/llama-stack-apis/"},"sections":[{"parentId":null,"name":"Supported Llama Stack APIs in {productname-short}","level":1,"index":0,"id":"_supported_llama_stack_apis_in_productname_short"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Agents API","level":2,"index":0,"id":"_agents_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Datasets_IO API","level":2,"index":1,"id":"_datasets_io_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Evaluation API","level":2,"index":2,"id":"_evaluation_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Inference API","level":2,"index":3,"id":"_inference_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Safety API","level":2,"index":4,"id":"_safety_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Tool Runtime API","level":2,"index":5,"id":"_tool_runtime_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Vector_IO API","level":2,"index":6,"id":"_vector_io_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/llama-stack-providers-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job-properties/"},"sections":[{"parentId":null,"name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":1,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-features-available-for-real-time-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates-without-the-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/migrating-pipelines-from-database-to-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/migrating-to-the-rhbok-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/mirror-the-python-index/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/mitigating-prompt-injection-by-using-a-hugging-face-prompt-injection-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/model-serving-runtimes-for-accelerators/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"},{"parentId":null,"name":"IBM Spyre AI accelerators on x86 and IBM Z","level":1,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86_and_ibm_z"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/model-serving-runtimes/"},"sections":[{"parentId":null,"name":"ServingRuntime","level":1,"index":0,"id":"_servingruntime"},{"parentId":null,"name":"InferenceService","level":1,"index":1,"id":"_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-playground/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/openai-compatibility-for-rag-apis-in-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/openai-compatible-apis-in-llama-stack/"},"sections":[{"parentId":null,"name":"Supported OpenAI-compatible APIs in {productname-short}","level":1,"index":0,"id":"_supported_openai_compatible_apis_in_productname_short"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Chat Completions API","level":2,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Completions API","level":2,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Embeddings API","level":2,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Files API","level":2,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Stores API","level":2,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Store Files API","level":2,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Models API","level":2,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Responses API","level":2,"index":7,"id":"_responses_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":[{"parentId":null,"name":"Distributed workloads infrastructure","level":1,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":null,"name":"Types of distributed workloads","level":1,"index":1,"id":"_types_of_distributed_workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-evaluating-ai-systems/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-faiss-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Kueue management states","level":1,"index":0,"id":"_kueue_management_states"},{"parentId":null,"name":"Queue enforcement for projects","level":1,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":null,"name":"Restrictions for managing workloads with Kueue","level":1,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-milvus-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-ml-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-registries/"},"sections":[{"parentId":null,"name":"Model catalog","level":1,"index":0,"id":"_model_catalog"},{"parentId":null,"name":"Model registry","level":1,"index":1,"id":"_model_registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-monitoring-your-ai-systems/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-rag/"},"sections":[{"parentId":null,"name":"Audience for RAG","level":1,"index":0,"id":"_audience_for_rag"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-the-model-customization-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/performance-considerations-for-doc-apps/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/performing-model-evaluations-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/querying-ingested-content-in-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-accelerator-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-cpu-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-default-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-distributed-inference/"},"sections":[{"parentId":null,"name":"Single-node GPU deployment","level":1,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":null,"name":"Multi-node deployment","level":1,"index":1,"id":"_multi_node_deployment"},{"parentId":null,"name":"Intelligent inference scheduler with KV cache routing","level":1,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-ddp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-fsdp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-nccl/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kubernetes-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-oidc-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-pvc-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-grafana-metrics/"},"sections":[{"parentId":null,"name":"Accelerator metrics","level":1,"index":0,"id":"ref-accelerator-metrics_{context}"},{"parentId":null,"name":"CPU metrics","level":1,"index":1,"id":"ref-cpu-metrics_{context}"},{"parentId":null,"name":"vLLM metrics","level":1,"index":2,"id":"ref-vllm-metrics_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":1,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":1,"index":2,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":1,"index":3,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM AMD GPU ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":1,"index":5,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre s390x ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_spyre_s390x_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Seldon MLServer","level":1,"index":8,"id":"_seldon_mlserver"},{"parentId":null,"name":"Additional resources","level":1,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-trainingclient-api-job-related-methods/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-vllm-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-the-ca-bundle-from-a-single-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-the-ca-bundle-from-all-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/retrieving-data-science-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-workload-with-a-kueue-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-custom-evaluations-with-LMEval-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ai-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-jupyter-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-feature-store-UI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval-s3-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-ragas-inline-provider/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-your-working-environment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preparing-documents-with-docling-for-llama-stack-retrieval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-available-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/specifying-files-to-ignore/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/specifying-the-data-source-for-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-basic-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-starting-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-data-with-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/support-philosophy/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-baseline-model-responses/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-with-model-control-protocol-servers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-your-model-with-rag/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-your-vllm-model-endpoints/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-gateway-api/"},"sections":[{"parentId":null,"name":"The <code>GatewayConfig</code> status shows as not ready","level":1,"index":0,"id":"_the_gatewayconfig_status_shows_as_not_ready"},{"parentId":null,"name":"Authentication proxy fails to start","level":1,"index":1,"id":"_authentication_proxy_fails_to_start"},{"parentId":null,"name":"The Gateway is inaccessible","level":1,"index":2,"id":"_the_gateway_is_inaccessible"},{"parentId":null,"name":"The OIDC authentication fails","level":1,"index":3,"id":"_the_oidc_authentication_fails"},{"parentId":null,"name":"The dashboard is not accessible after authentication","level":1,"index":4,"id":"_the_dashboard_is_not_accessible_after_authentication"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-workbenches-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s workbench does not start","level":1,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-Kueue/"},"sections":[{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for Kueue","level":1,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":1,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a \"local_queue provided does not exist\" error message","level":1,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"The pod provisioned by Kueue is terminated before the image is pulled","level":1,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-workbenches-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for Kueue","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster does not start","level":1,"index":3,"id":"_my_ray_cluster_does_not_start"},{"parentId":null,"name":"I see a \"Default Local Queue not found\" error message","level":1,"index":4,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a \"local_queue provided does not exist\" error message","level":1,"index":5,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":6,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":7,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":2,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":3,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-playground-issues/"},"sections":[{"parentId":null,"name":"The chatbot thinks indefinitely","level":1,"index":0,"id":"_the_chatbot_thinks_indefinitely"},{"parentId":null,"name":"The model does not use RAG data","level":1,"index":1,"id":"_the_model_does_not_use_rag_data"},{"parentId":null,"name":"MCP servers are missing from the UI","level":1,"index":2,"id":"_mcp_servers_are_missing_from_the_ui"},{"parentId":null,"name":"The model fails to call MCP tools","level":1,"index":3,"id":"_the_model_fails_to_call_mcp_tools"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSPA components","level":1,"index":0,"id":"_common_errors_across_dspa_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-pipeline-run-workspaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-rag-evaluation-providers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-rag-settings/"},"sections":[{"parentId":null,"name":"Understanding RAG settings","level":1,"index":0,"id":"understanding-rag-settings_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-lmeval-job-configuration-using-the-web-console/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-workbench-settings-by-restarting-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-playground-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-model-files-to-pvc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-custom-unitxt-card/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-kserve-inference-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-drift-metric-in-a-credit-card-scenario/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-connections-api/"},"sections":[{"parentId":null,"name":"Namespace isolation in connections API","level":1,"index":0,"id":"_namespace_isolation_in_connections_api"},{"parentId":null,"name":"Role-based access control (RBAC) requirements in connections API","level":1,"index":1,"id":"_role_based_access_control_rbac_requirements_in_connections_api"},{"parentId":null,"name":"Validation scope","level":1,"index":2,"id":"_validation_scope"},{"parentId":null,"name":"Using connection annotations based on workload type","level":1,"index":3,"id":"_using_connection_annotations_based_on_workload_type"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-llm-as-a-judge-metrics-with-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-pvcs-as-storage/"},"sections":[{"parentId":null,"name":"Managed PVCs","level":1,"index":0,"id":"_managed_pvcs"},{"parentId":null,"name":"Existing PVCs","level":1,"index":1,"id":"_existing_pvcs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-ragas-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-the-cluster-CA-bundle-for-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-the-cluster-server-and-token-to-authenticate/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-feature-store-objects-in-the-web-based-ui/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-installed-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-models-in-the-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/playground-prerequisites/"},"sections":[{"parentId":null,"name":"Cluster administrator prerequisites","level":1,"index":0,"id":"_cluster_administrator_prerequisites"},{"parentId":null,"name":"User prerequisites","level":1,"index":1,"id":"_user_prerequisites"},{"parentId":null,"name":"Model and runtime requirements for the playground","level":1,"index":2,"id":"_model_and_runtime_requirements_for_the_playground"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Key model selection factors","level":2,"index":0,"id":"_key_model_selection_factors"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Example model configuration","level":2,"index":1,"id":"_example_model_configuration"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/playground-overview/"},"sections":[{"parentId":null,"name":"Core capabilities","level":1,"index":0,"id":"_core_capabilities"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-traces-in-external-tracing-platforms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-hardware-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-centralized-auth-oidc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-feature-definitions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-kserve-deployment-modes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-llama-stack-search-types/"},"sections":[{"parentId":null,"name":"Supported search modes","level":1,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":2,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":2,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":2,"index":2,"id":"_hybrid_search"},{"parentId":null,"name":"Retrieval database support","level":1,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Model serving platform","level":1,"index":0,"id":"_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":1,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-organizing-features-by-using-entities/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-persistent-storage/"},"sections":[{"parentId":null,"name":"Storage classes in {productname-short}","level":1,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":null,"name":"Access modes","level":1,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":2,"index":0,"id":"_using_shared_storage_rwx"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-ai-assets-endpoints-page/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-python-index/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authentication-token-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-built-in-alerts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-hugging-face-models-with-an-environment-variable-token/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-administration-interface-for-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/activating-the-llama-stack-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-connection-to-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-certificates-to-a-cluster-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-certificates-to-a-custom-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-external-artifacts-to-pipeline-run-workspaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-feature-definitions-and-initializing-your-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-workbench-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-custom-image-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-workbench-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-workbench-overview/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/audience-for-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-kfp-sdk-with-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/auth-on-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/before-you-begin/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/benchmarking-embedding-models-with-beir-datasets-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/clone-an-example-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/collecting-metrics-from-user-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/compare-the-performance-of-osft-and-sft/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/compiling-kubernetes-native-manifests-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/compiling-the-pipeline-yaml-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-certificate-for-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-certificate-for-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-cluster-for-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-playground-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-inference-service-for-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-inference-service-for-spyre/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-offline-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-online-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-and-managing-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-authentication-for-llmd/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-default-workspace-pvc-settings-in-dspa/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-mcp-servers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-metric-based-autoscaling/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-model-catalog-sources-in-openshift/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-oidc-auth-gateway-api/"},"sections":[{"parentId":null,"name":"Security considerations","level":1,"index":0,"id":"_security_considerations"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-pipelines-with-your-own-argo-workflows-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-ragas-remote-provider-for-production/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-built-in-detector-and-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-feature-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator logs","level":1,"index":0,"id":"_viewing_the_productname_short_operator_logs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-workload-management-with-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/controlling-caching-in-pipelines/"},"sections":[{"parentId":null,"name":"Disabling caching for individual tasks","level":1,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":null,"name":"Disabling caching for a pipeline at submit time","level":1,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":null,"name":"Disabling caching for a pipeline at compile time","level":1,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":null,"name":"Disabling caching for all pipelines (pipeline server)","level":1,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-feature-store-instance-in-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorch-training-script-configmap-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorchjob-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-multi-node-pytorch-training-job-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-for-distributed-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-feature-views/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-oci-compatible-connection-types-api/"},"sections":[{"parentId":null,"name":"Using an OCI connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_an_oci_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using an OCI connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_an_oci_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-project-scoped-resources-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-project-scoped-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-s3-compatible-connection-type-api/"},"sections":[{"parentId":null,"name":"Using an Amazon S3 connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_an_amazon_s3_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using an Amazon S3 connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_an_amazon_s3_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-uri-compatible-connection-type-api/"},"sections":[{"parentId":null,"name":"Using a URI connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_a_uri_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using a URI connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_a_uri_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-model-selection-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline-by-using-the-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-drift-metric-by-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-playground-from-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-llama-model-with-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-llamastackdistribution-instance/"},"sections":[{"parentId":null,"name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":1,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":null,"name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":1,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":null,"name":"Example C: LlamaStackDistribution with <strong>Inline FAISS</strong>","level":1,"index":2,"id":"_example_c_llamastackdistribution_with_inline_faiss"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-remote-milvus-vector-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-model-stored-in-oci-image-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-distributed-inference/"},"sections":[{"parentId":null,"name":"Configuring authentication for {llmd} using {org-name} Connectivity Link","level":1,"index":0,"id":"configuring-authentication-for-llmd_{context}"},{"parentId":null,"name":"Enabling {llmd}","level":1,"index":1,"id":"enabling-distributed-inference_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-vllm-gpu-metrics-dashboard-grafana/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/detecting-hateful-and-profane-language/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/detecting-pii-by-using-guardrails-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/determining-gpu-requirements-for-llm-powered-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-node-caching-in-elyra/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-an-existing-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/emptying-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-automatic-authentication-and-publishing-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-distributed-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-kueue-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-feature-store-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-observability-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-kserve-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/end-to-end-model-customization-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/estimate-memory-usage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/evaluating-rag-system-quality-with-ragas/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/explore-the-data-processing-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/explore-the-kubeflow-pipeline-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/explore-the-sdg-hub-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/explore-the-training-hub-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/feature-store-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/granting-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-metrics-to-external-observability-tools/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-auto-config/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-configuring-the-hugging-face-detector-serving-runtime/"},"sections":[{"parentId":null,"name":"Guardrails Detector Hugging Face serving runtime configuration values","level":1,"index":0,"id":"_guardrails_detector_hugging_face_serving_runtime_configuration_values"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-enforcing-configured-safety-pipelines-for-llm-inference-using-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-filtering-flagged-content-by-sending-requests-to-the-regex-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-gateway-config-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-configmap-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-detectors/"},"sections":[{"parentId":null,"name":"Built-in Detector","level":1,"index":0,"id":"_built_in_detector"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-parameters/"},"sections":[{"parentId":null,"name":"Gateway Pa","level":0,"index":0,"id":"_gateway_pa"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guided-example-build-a-kfp-pipeline-for-sdg/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guidelines-for-metrics-based-autoscaling/"},"sections":[{"parentId":null,"name":"Choosing metrics for latency and throughput-optimized scaling","level":1,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":null,"name":"Choosing the right sliding window","level":1,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":null,"name":"Optimizing HPA scale-down configuration","level":1,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":null,"name":"Considering model size for optimal scaling","level":1,"index":3,"id":"_considering_model_size_for_optimal_scaling"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-basic-workbench-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ibm-spyre-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/import-example-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/inference-performance-metrics/"},"sections":[{"parentId":null,"name":"Latency","level":1,"index":0,"id":"_latency"},{"parentId":null,"name":"Throughput","level":1,"index":1,"id":"_throughput"},{"parentId":null,"name":"Cost per million tokens","level":1,"index":2,"id":"_cost_per_million_tokens"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ingesting-content-into-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/install-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/kueue-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/llama-stack-apis/"},"sections":[{"parentId":null,"name":"Supported Llama Stack APIs in {productname-short}","level":1,"index":0,"id":"_supported_llama_stack_apis_in_productname_short"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Agents API","level":2,"index":0,"id":"_agents_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Datasets_IO API","level":2,"index":1,"id":"_datasets_io_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Evaluation API","level":2,"index":2,"id":"_evaluation_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Inference API","level":2,"index":3,"id":"_inference_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Safety API","level":2,"index":4,"id":"_safety_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Tool Runtime API","level":2,"index":5,"id":"_tool_runtime_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Vector_IO API","level":2,"index":6,"id":"_vector_io_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/llama-stack-providers-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job-properties/"},"sections":[{"parentId":null,"name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":1,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-features-available-for-real-time-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates-without-the-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/migrating-pipelines-from-database-to-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/migrating-to-the-rhbok-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/mirror-the-python-index/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/mitigating-prompt-injection-by-using-a-hugging-face-prompt-injection-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/model-serving-runtimes-for-accelerators/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"},{"parentId":null,"name":"IBM Spyre AI accelerators on x86 and IBM Z","level":1,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86_and_ibm_z"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/model-serving-runtimes/"},"sections":[{"parentId":null,"name":"ServingRuntime","level":1,"index":0,"id":"_servingruntime"},{"parentId":null,"name":"InferenceService","level":1,"index":1,"id":"_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-playground/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/openai-compatibility-for-rag-apis-in-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/openai-compatible-apis-in-llama-stack/"},"sections":[{"parentId":null,"name":"Supported OpenAI-compatible APIs in {productname-short}","level":1,"index":0,"id":"_supported_openai_compatible_apis_in_productname_short"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Chat Completions API","level":2,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Completions API","level":2,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Embeddings API","level":2,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Files API","level":2,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Stores API","level":2,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Store Files API","level":2,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Models API","level":2,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Responses API","level":2,"index":7,"id":"_responses_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":[{"parentId":null,"name":"Distributed workloads infrastructure","level":1,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":null,"name":"Types of distributed workloads","level":1,"index":1,"id":"_types_of_distributed_workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-evaluating-ai-systems/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-faiss-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Kueue management states","level":1,"index":0,"id":"_kueue_management_states"},{"parentId":null,"name":"Queue enforcement for projects","level":1,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":null,"name":"Restrictions for managing workloads with Kueue","level":1,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-milvus-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-ml-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-registries/"},"sections":[{"parentId":null,"name":"Model catalog","level":1,"index":0,"id":"_model_catalog"},{"parentId":null,"name":"Model registry","level":1,"index":1,"id":"_model_registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-monitoring-your-ai-systems/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-rag/"},"sections":[{"parentId":null,"name":"Audience for RAG","level":1,"index":0,"id":"_audience_for_rag"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-the-model-customization-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/performance-considerations-for-doc-apps/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/performing-model-evaluations-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/playground-overview/"},"sections":[{"parentId":null,"name":"Core capabilities","level":1,"index":0,"id":"_core_capabilities"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/playground-prerequisites/"},"sections":[{"parentId":null,"name":"Cluster administrator prerequisites","level":1,"index":0,"id":"_cluster_administrator_prerequisites"},{"parentId":null,"name":"User prerequisites","level":1,"index":1,"id":"_user_prerequisites"},{"parentId":null,"name":"Model and runtime requirements for the playground","level":1,"index":2,"id":"_model_and_runtime_requirements_for_the_playground"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Key model selection factors","level":2,"index":0,"id":"_key_model_selection_factors"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Example model configuration","level":2,"index":1,"id":"_example_model_configuration"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preparing-documents-with-docling-for-llama-stack-retrieval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/querying-ingested-content-in-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-accelerator-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-cpu-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-default-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-distributed-inference/"},"sections":[{"parentId":null,"name":"Single-node GPU deployment","level":1,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":null,"name":"Multi-node deployment","level":1,"index":1,"id":"_multi_node_deployment"},{"parentId":null,"name":"Intelligent inference scheduler with KV cache routing","level":1,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-fsdp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-ddp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-nccl/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kubernetes-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-oidc-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-pvc-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-grafana-metrics/"},"sections":[{"parentId":null,"name":"Accelerator metrics","level":1,"index":0,"id":"ref-accelerator-metrics_{context}"},{"parentId":null,"name":"CPU metrics","level":1,"index":1,"id":"ref-cpu-metrics_{context}"},{"parentId":null,"name":"vLLM metrics","level":1,"index":2,"id":"ref-vllm-metrics_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":1,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":1,"index":2,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":1,"index":3,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM AMD GPU ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":1,"index":5,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre s390x ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_spyre_s390x_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Seldon MLServer","level":1,"index":8,"id":"_seldon_mlserver"},{"parentId":null,"name":"Additional resources","level":1,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-trainingclient-api-job-related-methods/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-vllm-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-the-ca-bundle-from-a-single-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-the-ca-bundle-from-all-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/retrieving-data-science-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-workload-with-a-kueue-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-custom-evaluations-with-LMEval-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ai-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-jupyter-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-feature-store-UI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval-s3-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-ragas-inline-provider/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/glossary-of-common-terms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-your-working-environment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-your-playground-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-available-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/specifying-files-to-ignore/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/specifying-the-data-source-for-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-basic-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-starting-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-data-with-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/support-philosophy/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-baseline-model-responses/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-with-model-control-protocol-servers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-your-model-with-rag/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-your-vllm-model-endpoints/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-gateway-api/"},"sections":[{"parentId":null,"name":"The <code>GatewayConfig</code> status shows as not ready","level":1,"index":0,"id":"_the_gatewayconfig_status_shows_as_not_ready"},{"parentId":null,"name":"Authentication proxy fails to start","level":1,"index":1,"id":"_authentication_proxy_fails_to_start"},{"parentId":null,"name":"The Gateway is inaccessible","level":1,"index":2,"id":"_the_gateway_is_inaccessible"},{"parentId":null,"name":"The OIDC authentication fails","level":1,"index":3,"id":"_the_oidc_authentication_fails"},{"parentId":null,"name":"The dashboard is not accessible after authentication","level":1,"index":4,"id":"_the_dashboard_is_not_accessible_after_authentication"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-workbenches-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s workbench does not start","level":1,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-workbenches-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-Kueue/"},"sections":[{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for Kueue","level":1,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":1,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a \"local_queue provided does not exist\" error message","level":1,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"The pod provisioned by Kueue is terminated before the image is pulled","level":1,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":2,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":3,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for Kueue","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster does not start","level":1,"index":3,"id":"_my_ray_cluster_does_not_start"},{"parentId":null,"name":"I see a \"Default Local Queue not found\" error message","level":1,"index":4,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a \"local_queue provided does not exist\" error message","level":1,"index":5,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":6,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":7,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSPA components","level":1,"index":0,"id":"_common_errors_across_dspa_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-playground-issues/"},"sections":[{"parentId":null,"name":"The chatbot thinks indefinitely","level":1,"index":0,"id":"_the_chatbot_thinks_indefinitely"},{"parentId":null,"name":"The model does not use RAG data","level":1,"index":1,"id":"_the_model_does_not_use_rag_data"},{"parentId":null,"name":"MCP servers are missing from the UI","level":1,"index":2,"id":"_mcp_servers_are_missing_from_the_ui"},{"parentId":null,"name":"The model fails to call MCP tools","level":1,"index":3,"id":"_the_model_fails_to_call_mcp_tools"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-pipeline-run-workspaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-rag-evaluation-providers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-rag-settings/"},"sections":[{"parentId":null,"name":"Understanding RAG settings","level":1,"index":0,"id":"understanding-rag-settings_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-lmeval-job-configuration-using-the-web-console/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-workbench-settings-by-restarting-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-playground-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-model-files-to-pvc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-custom-unitxt-card/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-drift-metric-in-a-credit-card-scenario/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-kserve-inference-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-connections-api/"},"sections":[{"parentId":null,"name":"Namespace isolation in connections API","level":1,"index":0,"id":"_namespace_isolation_in_connections_api"},{"parentId":null,"name":"Role-based access control (RBAC) requirements in connections API","level":1,"index":1,"id":"_role_based_access_control_rbac_requirements_in_connections_api"},{"parentId":null,"name":"Validation scope","level":1,"index":2,"id":"_validation_scope"},{"parentId":null,"name":"Using connection annotations based on workload type","level":1,"index":3,"id":"_using_connection_annotations_based_on_workload_type"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-llm-as-a-judge-metrics-with-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-pvcs-as-storage/"},"sections":[{"parentId":null,"name":"Managed PVCs","level":1,"index":0,"id":"_managed_pvcs"},{"parentId":null,"name":"Existing PVCs","level":1,"index":1,"id":"_existing_pvcs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-ragas-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-the-cluster-CA-bundle-for-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-the-cluster-server-and-token-to-authenticate/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-feature-store-objects-in-the-web-based-ui/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-installed-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-models-in-the-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-traces-in-external-tracing-platforms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-hardware-profiles/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"preamble\">\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As an {openshift-platform} cluster administrator, you can deploy a RetrievalAugmented Generation (RAG) stack in {productname-short}. This stack provides the infrastructure, including LLM inference, vector storage, and retrieval services that data scientists and AI engineers use to build conversational workflows in their projects.</p>\n</div>\n<div class=\"paragraph\">\n<p>To deploy the RAG stack in a project, complete the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Activate the Llama Stack Operator in {productname-short}.</p>\n</li>\n<li>\n<p>Enable GPU support on the {openshift-platform} cluster. This task includes installing the required NVIDIA Operators.</p>\n</li>\n<li>\n<p>Deploy an inference model, for example, the llama-3.2-3b-instruct model. This task includes creating a storage connection and configuring GPU allocation.</p>\n</li>\n<li>\n<p>Create a <code>LlamaStackDistribution</code> instance to enable RAG functionality. This action deploys LlamaStack alongside a Milvus vector store and connects both components to the inference model.</p>\n</li>\n<li>\n<p>Ingest domain data into Milvus by running Docling in an AI pipeline or Jupyter notebook. This process keeps the embeddings synchronized with the source data.</p>\n</li>\n<li>\n<p>Expose and secure the model endpoints.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"overview-of-rag_{context}\">Overview of RAG</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>Retrieval-augmented generation (RAG) in {productname-short} enhances large language models (LLMs) by integrating domain-specific data sources directly into the model&#8217;s context. Domain-specific data sources can be structured data, such as relational database tables, or unstructured data, such as PDF documents.</p>\n</div>\n<div class=\"paragraph\">\n<p>RAG indexes content and builds an embedding store that data scientists and AI engineers can query. When data scientists or AI engineers pose a question to a RAG chatbot, the RAG pipeline retrieves the most relevant pieces of data, passes them to the LLM as context, and generates a response that reflects both the prompt and the retrieved content.</p>\n</div>\n<div class=\"paragraph\">\n<p>By implementing RAG, data scientists and AI engineers can obtain tailored, accurate, and verifiable answers to complex queries based on their own datasets within a project.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_audience_for_rag\">Audience for RAG</h3>\n<div class=\"paragraph\">\n<p>The target audience for RAG is practitioners who build data-grounded conversational AI applications using {productname-short} infrastructure.</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\">For Data Scientists</dt>\n<dd>\n<p>Data scientists can use RAG to prototype and validate models that answer natural-language queries against data sources without managing low-level embedding pipelines or vector stores. They can focus on creating prompts and evaluating model outputs instead of building retrieval infrastructure.</p>\n</dd>\n<dt class=\"hdlist1\">For MLOps Engineers</dt>\n<dd>\n<p>MLOps engineers typically deploy and operate RAG pipelines in production. Within {productname-short}, they manage LLM endpoints, monitor performance, and ensure that both retrieval and generation scale reliably. RAG decouples vector store maintenance from the serving layer, enabling MLOps engineers to apply CI/CD workflows to data ingestion and model deployment alike.</p>\n</dd>\n<dt class=\"hdlist1\">For Data Engineers</dt>\n<dd>\n<p>Data engineers build workflows to load data into storage that {productname-short} indexes. They keep embeddings in sync with source systems, such as S3 buckets or relational tables to ensure that chatbot responses are accurate.</p>\n</dd>\n<dt class=\"hdlist1\">For AI Engineers</dt>\n<dd>\n<p>AI engineers architect RAG chatbots by defining prompt templates, retrieval methods, and fallback logic. They configure agents and add domain-specific tools, such as {openshift-platform} job triggers, enabling rapid iteration.</p>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"overview-of-vector-databases_{context}\">Overview of vector databases</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>Vector databases are a crucial component of retrieval-augmented generation (RAG) in {productname-short}. They store and index vector embeddings that represent the semantic meaning of text or other data. When you integrate vector databases with Llama Stack in {productname-short}, you can build RAG applications that combine large language models (LLMs) with relevant, domain-specific knowledge.</p>\n</div>\n<div class=\"paragraph\">\n<p>Vector databases provide the following capabilities:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Store vector embeddings generated by embedding models.</p>\n</li>\n<li>\n<p>Support efficient similarity search to retrieve semantically related content.</p>\n</li>\n<li>\n<p>Enable RAG workflows by supplying the LLM with contextually relevant data from a specific domain.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>When you deploy RAG workloads in {productname-short}, you can deploy vector databases through the Llama Stack Operator.\nCurrently, {productname-short} supports the following vector databases:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Inline Milvus Lite</strong>\nAn Inline Milvus vector database runs embedded within the Llama Stack Distribution (LSD) pod and is suitable for lightweight experimentation and small-scale development. Inline Milvus stores data in a local SQLite database and is limited in scale and persistence.</p>\n</li>\n<li>\n<p><strong>Inline FAISS</strong>\nInline FAISS provides an alternative lightweight vector store for RAG workloads. FAISS (Facebook AI Similarity Search) is an open-source library for efficient similarity search and clustering of dense vectors. When configured inline with a SQLite backend, FAISS runs entirely within the Llama Stack container and stores embeddings locally without requiring a separate database service. Inline FAISS is ideal for testing and experimental RAG deployments.</p>\n</li>\n<li>\n<p><strong>Remote Milvus</strong>\nA remote Milvus vector database runs as a standalone service in your project namespace or as an external managed deployment.\nRemote Milvus is best for production-grade RAG use cases because it provides persistence, scalability, and isolation from the Llama Stack Distribution (LSD) pod.\nIn {openshift-platform} environments, you must deploy Milvus with an etcd service directly in your project.\nFor more information on using etcd services, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/etcd/index\">Providing redundancy with etcd</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Consider the following points when you decide which vector database to use for your RAG workloads:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Use <strong>inline Milvus Lite</strong> if you want to experiment quickly with RAG in a self-contained setup and do not require persistence across pod restarts.</p>\n</li>\n<li>\n<p>Use <strong>inline FAISS</strong> if you need a lightweight, in-process vector store with local persistence through SQLite and no network dependency.</p>\n</li>\n<li>\n<p>Use <strong>remote Milvus</strong> if you need reliable storage, high availability, and the ability to scale RAG workloads in your {productname-short} environment.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect2\">\n<h3 id=\"overview-of-milvus-vector-databases_{context}\">Overview of Milvus vector databases</h3>\n<div class=\"paragraph _abstract\">\n<p>Milvus is an open source vector database designed for high-performance similarity search across embedding data. In {productname-short}, Milvus is supported as a remote vector database provider for the Llama Stack Operator. Milvus enables retrieval-augmented generation (RAG) workloads that require persistence, scalability, and efficient search across large document collections.</p>\n</div>\n<div class=\"paragraph\">\n<p>Milvus vector databases provide you with the following capabilities in {productname-short}:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Similarity search using Approximate Nearest Neighbor (ANN) algorithms.</p>\n</li>\n<li>\n<p>Persistent storage support for vectors.</p>\n</li>\n<li>\n<p>Indexing and query optimizations for embedding-based search.</p>\n</li>\n<li>\n<p>Integration with external metadata and APIs.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>In {productname-short}, you can use Milvus vector databases in the following operational modes:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Inline Milvus Lite</strong>, which runs embedded in the Llama Stack Distribution pod for testing or small-scale experiments.</p>\n</li>\n<li>\n<p><strong>Remote Milvus</strong>, which runs as a standalone service in your OpenShift project or as an external managed Milvus service. Remote Milvus is recommended for production workloads.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>When you deploy a remote Milvus vector database, you must run the following components in your {openshift-platform} project:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Secret (<code>milvus-secret</code>)</strong>:  Stores sensitive data such as the Milvus root password.</p>\n</li>\n<li>\n<p><strong>PersistentVolumeClaim (<code>milvus-pvc</code>)</strong>: Provides persistent storage for Milvus data.</p>\n</li>\n<li>\n<p><strong>Deployment (<code>etcd-deployment</code>)</strong>: Runs an etcd instance that Milvus uses for metadata storage and service coordination.</p>\n</li>\n<li>\n<p><strong>Service (<code>etcd-service</code>)</strong>: Exposes the etcd port for Milvus to connect to.</p>\n</li>\n<li>\n<p><strong>Deployment (<code>milvus-standalone</code>)</strong>: Runs Milvus in standalone mode and connects it to the etcd service and PVC.</p>\n</li>\n<li>\n<p><strong>Service (<code>milvus-service</code>)</strong>: Exposes Milvus gRPC (19530) and HTTP (9091 health check) ports for client access.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Milvus requires an etcd service to manage metadata such as collections, indexes, and partitions, and to provide service discovery and coordination among Milvus components. Even when running in standalone mode, Milvus depends on etcd to operate correctly and maintain metadata consistency. For more information on using etcd services, see <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/etcd/index\" target=\"_blank\" rel=\"noopener\">Providing redundancy with etcd</a>.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Do not use the OpenShift control plane etcd for Milvus. You must deploy a separate etcd instance inside your project or connect to an external etcd service.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>Use Remote Milvus when you require a persistent, scalable, and production-ready vector database that integrates seamlessly with {productname-short}. Consider choosing a remote Milvus vector database if your deployment must cater for the following requirements:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Persistent vector storage across restarts or upgrades.</p>\n</li>\n<li>\n<p>Scalable indexing and high-performance vector search.</p>\n</li>\n<li>\n<p>A production-grade RAG architecture integrated with {productname-short}.</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"overview-of-faiss-vector-databases_{context}\">Overview of FAISS vector databases</h3>\n<div class=\"paragraph\">\n<p>The FAISS (Facebook AI Similarity Search) library is an open-source framework for high-performance vector search and clustering. It is optimized for dense numerical embeddings and supports both CPU and GPU execution. You can enable inline FAISS in {productname-short} with an embedded SQLite backend in your Llama Stack Distribution. This configures Llama Stack to use FAISS as an in-process vector store, storing embeddings locally within the container without requiring a separate vector database service.</p>\n</div>\n<div class=\"paragraph\">\n<p>Inline FAISS enables efficient similarity search and retrieval within retrieval augmented generation (RAG) workflows. It operates entirely within the <code>LlamaStackDistribution</code> instance, making it a lightweight option for experimental and testing environments.</p>\n</div>\n<div class=\"paragraph\">\n<p>Inline FAISS offers the following benefits:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Simplified setup with no external database or network dependencies.</p>\n</li>\n<li>\n<p>Persistent local storage of FAISS vector data.</p>\n</li>\n<li>\n<p>Reduced latency for embedding ingestion and query operations.</p>\n</li>\n<li>\n<p>Compatibility with OpenAI-compatible Vector Store API endpoints.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Inline FAISS stores vectors either in memory or in a local SQLite database file, allowing the deployment to retain vector data across sessions with minimal overhead.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Inline FAISS is best for experimental or testing environments. It does not provide distributed storage or high availability. For production-grade workloads that require scalability or redundancy, consider using an external vector database, such as Milvus.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"Deploying-a-llama-model-with-kserve_{context}\">Deploying a Llama model with KServe</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To use Llama Stack and retrieval-augmented generation (RAG) workloads in {productname-short}, you must deploy a Llama model with a vLLM model server and configure KServe in KServe RawDeployment mode.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed {openshift-platform} {ocp-minimum-version} or newer.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>You have cluster administrator privileges for your {openshift-platform} cluster.</p>\n</li>\n<li>\n<p>You have activated the Llama Stack Operator.\nFor more information, see <a href=\"{odhdocshome}/working-with-llama-stack/#installing-the-llama-stack-operator_rag\">Installing the Llama Stack Operator</a>.</p>\n</li>\n<li>\n<p>You have installed KServe.</p>\n</li>\n<li>\n<p>You have enabled the model serving platform. For more information about enabling the model serving platform, see <a href=\"{odhdocshome}/configuring-your-model-serving-platform/#enabling-the-model-serving-platform_odh-admin\" target=\"_blank\" rel=\"noopener\">Enabling the model serving platform</a>.</p>\n</li>\n<li>\n<p>You can access the model serving platform in the dashboard configuration.\nFor more information about setting dashboard configuration options, see <a href=\"{odhdocshome}/managing-resources/#customizing-the-dashboard\">Customizing the dashboard</a>.</p>\n</li>\n<li>\n<p>You have enabled GPU support in {productname-short}, including installing the Node Feature Discovery Operator and NVIDIA GPU Operator. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on {org-name} OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for {rosa-productname}</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have created a project.</p>\n</li>\n<li>\n<p>The vLLM serving runtime is installed and available in your environment.</p>\n</li>\n<li>\n<p>You have created a storage connection for your model that contains a <code>URI - v1</code> connection type. This storage connection must define the location of your Llama 3.2 model artifacts. For example, <code>oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct</code>.\nFor more information about creating storage connections, see <a href=\"{odhdocshome}/working-on-projects/#adding-a-connection-to-your-project_projects\">Adding a connection to your project</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"title\">Procedure</div>\n<div class=\"paragraph\">\n<p>These steps are only supported in {productname-short} versions 2.19 and later.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the {productname-short} dashboard, navigate to the project details page and click the <strong>Deployments</strong> tab.</p>\n</li>\n<li>\n<p>In the <strong>Model serving platform</strong> tile, click <strong>Select model</strong>.</p>\n</li>\n<li>\n<p>Click the <strong>Deploy model</strong> button.</p>\n<div class=\"paragraph\">\n<p>The <strong>Deploy model</strong> dialog opens.</p>\n</div>\n</li>\n<li>\n<p>Configure the deployment properties for your model:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>In the <strong>Model deployment name</strong> field, enter a unique name for your deployment.</p>\n</li>\n<li>\n<p>In the <strong>Serving runtime</strong> field, select <code>vLLM NVIDIA GPU serving runtime for KServe</code> from the drop-down list.</p>\n</li>\n<li>\n<p>In the <strong>Deployment mode</strong> field, select <strong>KServe RawDeployment</strong> from the drop-down list.</p>\n</li>\n<li>\n<p>Set <strong>Number of model server replicas to deploy</strong> to <code>1</code>.</p>\n</li>\n<li>\n<p>In the <strong>Model server size</strong> field, select <code>Custom</code> from the drop-down list.</p>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Set <strong>CPUs requested</strong> to <code>1 core</code>.</p>\n</li>\n<li>\n<p>Set <strong>Memory requested</strong> to <code>10 GiB</code>.</p>\n</li>\n<li>\n<p>Set <strong>CPU limit</strong> to <code>2 core</code>.</p>\n</li>\n<li>\n<p>Set <strong>Memory limit</strong> to <code>14 GiB</code>.</p>\n</li>\n<li>\n<p>Set <strong>Accelerator</strong> to <code>NVIDIA GPUs</code>.</p>\n</li>\n<li>\n<p>Set <strong>Accelerator count</strong> to <code>1</code>.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</li>\n<li>\n<p>From the <strong>Connection type</strong>, select a relevant data connection from the drop-down list.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>In the <strong>Additional serving runtime arguments</strong> field, specify the following recommended arguments:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-shell\" data-lang=\"shell\">--dtype=half\n--max-model-len=20000\n--gpu-memory-utilization=0.95\n--enable-chunked-prefill\n--enable-auto-tool-choice\n--tool-call-parser=llama3_json\n--chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja</code></pre>\n</div>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Click <strong>Deploy</strong>.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Model deployment can take several minutes, especially for the first model that is deployed on the cluster. Initial deployment may take more than 10 minutes while the relevant images download.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>Verify that the <code>kserve-controller-manager</code> and <code>odh-model-controller</code> pods are running:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Open a new terminal window.</p>\n</li>\n<li>\n<p>Log in to your {openshift-platform} cluster from the CLI:</p>\n</li>\n<li>\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the {openshift-cli}.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Enter the following command to verify that the <code>kserve-controller-manager</code> and <code>odh-model-controller</code> pods are running:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc get pods -n opendatahub | grep -E 'kserve-controller-manager|odh-model-controller'</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Confirm that you see output similar to the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>kserve-controller-manager-7c865c9c9f-xyz12   1/1     Running   0          4m21s\nodh-model-controller-7b7d5fd9cc-wxy34        1/1     Running   0          3m55s</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If you do not see either of the <code>kserve-controller-manager</code> and <code>odh-model-controller</code> pods, there could be a problem with your deployment. In addition, if the pods appear in the list, but their <code>Status</code> is not set to <code>Running</code>, check the pod logs for errors:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc logs &lt;pod-name&gt; -n opendatahub</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Check the status of the inference service:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc get inferenceservice -n llamastack\n$ oc get pods -n &lt;project name&gt; | grep llama</code></pre>\n</div>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The deployment automatically creates the following resources:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>A <code>ServingRuntime</code> resource.</p>\n</li>\n<li>\n<p>An <code>InferenceService</code> resource, a <code>Deployment</code>, a pod, and a service pointing to the pod.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Verify that the server is running. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc logs llama-32-3b-instruct-predictor-77f6574f76-8nl4r  -n &lt;project name&gt;</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Check for output similar to the following example log:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-log\" data-lang=\"log\">INFO     2025-05-15 11:23:52,750 __main__:498 server: Listening on ['::', '0.0.0.0']:8321\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO     2025-05-15 11:23:52,765 __main__:151 server: Starting up\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>The deployed model displays in the <strong>Deployments</strong> tab on the project details page for the project it was deployed under.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>If you see a <code>ConvertTritonGPUToLLVM</code> error in the pod logs when querying the <code>/v1/chat/completions</code> API, and the vLLM server restarts or returns a <code>500 Internal Server</code> error, apply the following workaround:</p>\n<div class=\"paragraph\">\n<p>Before deploying the model, remove the <code>--enable-chunked-prefill</code> argument from the <strong>Additional serving runtime arguments</strong> field in the deployment dialog.</p>\n</div>\n<div class=\"paragraph\">\n<p>The error is displayed similar to the following:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-log\" data-lang=\"log\">/opt/vllm/lib64/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py:36:0: error: Failures have been detected while processing an MLIR pass pipeline\n/opt/vllm/lib64/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py:36:0: note: Pipeline failed while executing [`ConvertTritonGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`\nINFO:     10.129.2.8:0 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"testing-your-vllm-model-endpoints_{context}\">Testing your vLLM model endpoints</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To verify that your deployed Llama 3.2 model is accessible externally, ensure that your vLLM model server is exposed as a network endpoint. You can then test access to the model from outside both the {openshift-platform} cluster and the {productname-short} interface.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you selected <strong>Make deployed models available through an external route</strong> during deployment, your vLLM model endpoint is already accessible outside the cluster. You do not need to manually expose the model server. Manually exposing vLLM model endpoints, for example, by using <code>oc expose</code>, creates an unsecured route unless you configure authentication. Avoid exposing endpoints without security controls to prevent unauthorized access.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your {openshift-platform} cluster.</p>\n</li>\n<li>\n<p>You have logged in to {productname-long}.</p>\n</li>\n<li>\n<p>You have activated the Llama Stack Operator in {productname-short}.</p>\n</li>\n<li>\n<p>You have deployed an inference model, for example, the llama-3.2-3b-instruct model.</p>\n</li>\n<li>\n<p>You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for {rosa-productname}</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Log in to your {openshift-platform} cluster from the CLI:</p>\n</li>\n<li>\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>.</p>\n</li>\n<li>\n<p>After you have logged in, click <strong>Display token</strong>.</p>\n</li>\n<li>\n<p>Copy the <strong>Log in with this token</strong> command and paste it in the {openshift-cli}.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>If you enabled <strong>Require token authentication</strong> during model deployment, retrieve your token:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-sh\" data-lang=\"sh\">$ export MODEL_TOKEN=$(oc get secret default-name-llama-32-3b-instruct-sa -n &lt;project name&gt; --template='{{ .data.token }}' | base64 -d)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Obtain your model endpoint URL:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you enabled <strong>Make deployed models available through an external route</strong> during model deployment, click <strong>Endpoint details</strong> on the <strong>Deployments</strong> page in the {productname-short} dashboard to obtain your model endpoint URL.</p>\n</li>\n<li>\n<p>In addition, if you did not enable <strong>Require token authentication</strong> during model deployment, you can also enter the following command to retrieve the endpoint URL:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-sh\" data-lang=\"sh\">$ export MODEL_ENDPOINT=\"https://$(oc get route llama-32-3b-instruct -n &lt;project name&gt; --template='{{ .spec.host }}')\"</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Test the endpoint with a sample chat completion request:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>If you did not enable <strong>Require token authentication</strong> during model deployment, enter a chat completion request. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-sh\" data-lang=\"sh\">$ curl -X POST $MODEL_ENDPOINT/v1/chat/completions \\\n -H \"Content-Type: application/json\" \\\n -d '{\n \"model\": \"llama-32-3b-instruct\",\n \"messages\": [\n   {\n     \"role\": \"user\",\n     \"content\": \"Hello\"\n   }\n ]\n}'</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If you enabled <strong>Require token authentication</strong> during model deployment, include a token in your request. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-sh\" data-lang=\"sh\">curl -s -k $MODEL_ENDPOINT/v1/chat/completions \\\n--header \"Authorization: Bearer $MODEL_TOKEN\" \\\n--header 'Content-Type: application/json' \\\n-d '{\n  \"model\": \"llama-32-3b-instruct\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"can you tell me a funny joke?\"\n    }\n  ]\n}' | jq .</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>-k</code> flag disables SSL verification and should only be used in test environments or with self-signed certificates.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>Confirm that you received a JSON response containing a chat completion. For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-json\" data-lang=\"json\">{\n  \"id\": \"chatcmpl-05d24b91b08a4b78b0e084d4cc91dd7e\",\n  \"object\": \"chat.completion\",\n  \"created\": 1747279170,\n  \"model\": \"llama-32-3b-instruct\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"reasoning_content\": null,\n      \"content\": \"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n      \"tool_calls\": []\n    },\n    \"logprobs\": null,\n    \"finish_reason\": \"stop\",\n    \"stop_reason\": null\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 37,\n    \"total_tokens\": 62,\n    \"completion_tokens\": 25,\n    \"prompt_tokens_details\": null\n  },\n  \"prompt_logprobs\": null\n}</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>If you do not receive a response similar to the example, verify that the endpoint URL and token are correct, and ensure your model deployment is running.</p>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"deploying-a-remote-milvus-vector-database_{context}\">Deploying a remote Milvus vector database</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>To use Milvus as a remote vector database provider for Llama Stack in {productname-short}, you must deploy Milvus and its required etcd service in your OpenShift project. This procedure shows how to deploy Milvus in standalone mode without the Milvus Operator.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The following example configuration is intended for testing or evaluation environments. For production-grade deployments, see <a href=\"https://milvus.io/docs\" target=\"_blank\" rel=\"noopener\">https://milvus.io/docs</a> in the Milvus documentation.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed {openshift-platform} {ocp-minimum-version} or newer.</p>\n</li>\n<li>\n<p>You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on {org-name} OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>You have cluster administrator privileges for your {openshift-platform} cluster.</p>\n</li>\n<li>\n<p>You are logged in to {productname-long}.</p>\n</li>\n<li>\n<p>You have a StorageClass available that can provision persistent volumes.</p>\n</li>\n<li>\n<p>You created a root password to secure your Milvus service.</p>\n</li>\n<li>\n<p>You have deployed an inference model with vLLM, for example, the llama-3.2-3b-instruct model, and you have selected <strong>Make deployed models available through an external route</strong> and <strong>Require token authentication</strong> during model deployment.</p>\n</li>\n<li>\n<p>You have the correct inference model identifier, for example, llama-3-2-3b.</p>\n</li>\n<li>\n<p>You have the model endpoint URL, ending with <code>/v1</code>, such as <code><a href=\"https://llama-32-3b-instruct-predictor:8443/v1\" class=\"bare\">https://llama-32-3b-instruct-predictor:8443/v1</a></code>.</p>\n</li>\n<li>\n<p>You have the API token required to access the model endpoint.</p>\n</li>\n<li>\n<p>You have installed the OpenShift command line interface (<code>oc</code>) as described in <a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a>.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the  {openshift-platform}  console, click the <strong>Quick Create</strong> (<span class=\"image\"><img src=\"/static/docs/images/quick-create-icon.png\" alt=\"quick create icon\"></span>) icon and then click the <strong>Import YAML</strong> option.</p>\n</li>\n<li>\n<p>Verify that your project is the selected project.</p>\n</li>\n<li>\n<p>In the <strong>Import YAML</strong> editor, paste the following manifest and click <strong>Create</strong>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: v1\nkind: Secret\nmetadata:\n  name: milvus-secret\ntype: Opaque\nstringData:\n  root-password: \"MyStr0ngP@ssw0rd\"\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: milvus-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n  volumeMode: Filesystem\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: etcd-deployment\n  labels:\n    app: etcd\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: etcd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: etcd\n    spec:\n      containers:\n        - name: etcd\n          image: quay.io/coreos/etcd:v3.5.5\n          command:\n            - etcd\n            - --advertise-client-urls=http://127.0.0.1:2379\n            - --listen-client-urls=http://0.0.0.0:2379\n            - --data-dir=/etcd\n          ports:\n            - containerPort: 2379\n          volumeMounts:\n            - name: etcd-data\n              mountPath: /etcd\n          env:\n            - name: ETCD_AUTO_COMPACTION_MODE\n              value: revision\n            - name: ETCD_AUTO_COMPACTION_RETENTION\n              value: \"1000\"\n            - name: ETCD_QUOTA_BACKEND_BYTES\n              value: \"4294967296\"\n            - name: ETCD_SNAPSHOT_COUNT\n              value: \"50000\"\n      volumes:\n        - name: etcd-data\n          emptyDir: {}\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: etcd-service\nspec:\n  ports:\n    - port: 2379\n      targetPort: 2379\n  selector:\n    app: etcd\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: milvus-standalone\n  name: milvus-standalone\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: milvus-standalone\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: milvus-standalone\n    spec:\n      containers:\n        - name: milvus-standalone\n          image: milvusdb/milvus:v2.6.0\n          args: [\"milvus\", \"run\", \"standalone\"]\n          env:\n            - name: DEPLOY_MODE\n              value: standalone\n            - name: ETCD_ENDPOINTS\n              value: etcd-service:2379\n            - name: COMMON_STORAGETYPE\n              value: local\n            - name: MILVUS_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: milvus-secret\n                  key: root-password\n          livenessProbe:\n            exec:\n              command: [\"curl\", \"-f\", \"http://localhost:9091/healthz\"]\n            initialDelaySeconds: 90\n            periodSeconds: 30\n            timeoutSeconds: 20\n            failureThreshold: 5\n          ports:\n            - containerPort: 19530\n              protocol: TCP\n            - containerPort: 9091\n              protocol: TCP\n          volumeMounts:\n            - name: milvus-data\n              mountPath: /var/lib/milvus\n      restartPolicy: Always\n      volumes:\n        - name: milvus-data\n          persistentVolumeClaim:\n            claimName: milvus-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: milvus-service\nspec:\n  selector:\n    app: milvus-standalone\n  ports:\n    - name: grpc\n      port: 19530\n      targetPort: 19530\n    - name: http\n      port: 9091\n      targetPort: 9091</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Use the gRPC port (<code>19530</code>) for the <code>MILVUS_ENDPOINT</code> setting in Llama Stack.</p>\n</li>\n<li>\n<p>The HTTP port (<code>9091</code>) is reserved for health checks.</p>\n</li>\n<li>\n<p>If you deploy Milvus in a different namespace, use the fully qualified service name in your Llama Stack configuration. For example:\n<code><a href=\"http://milvus-service.&lt;namespace&gt;.svc.cluster.local:19530\" class=\"bare\">http://milvus-service.&lt;namespace&gt;.svc.cluster.local:19530</a></code></p>\n</li>\n</ul>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In the {openshift-platform} web console, click <strong>Workloads</strong>  <strong>Deployments</strong>.</p>\n</li>\n<li>\n<p>Verify that both <code>etcd-deployment</code> and <code>milvus-standalone</code> show a status of <strong>1 of 1 pods available</strong>.</p>\n</li>\n<li>\n<p>Click <strong>Pods</strong> in the navigation panel and confirm that pods for both deployments are <strong>Running</strong>.</p>\n</li>\n<li>\n<p>Click the <code>milvus-standalone</code> pod name, then select the <strong>Logs</strong> tab.</p>\n</li>\n<li>\n<p>Verify that Milvus reports a healthy startup with output similar to:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-log\" data-lang=\"log\">Milvus Standalone is ready to serve ...\nListening on 0.0.0.0:19530 (gRPC)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Networking</strong>  <strong>Services</strong> and confirm that the <code>milvus-service</code> and <code>etcd-service</code> resources exist and are exposed on ports <code>19530</code> and <code>2379</code>, respectively.</p>\n</li>\n<li>\n<p>(Optional) Click <strong>Pods</strong>  <strong>milvus-standalone</strong>  <strong>Terminal</strong> and run the following health check:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">curl http://localhost:9091/healthz</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>A response of <code>{\"status\": \"healthy\"}</code> confirms that Milvus is running correctly.</p>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"deploying-a-llamastackdistribution-instance_{context}\">Deploying a LlamaStackDistribution instance</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can deploy Llama Stack with retrieval-augmented generation (RAG) by pairing it with a vLLM-served Llama 3.2 model. This module provides the following deployment examples of the <code>LlamaStackDistribution</code> custom resource (CR):</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>Example A:</strong> Inline Milvus (embedded, single-node)</p>\n</li>\n<li>\n<p><strong>Example B:</strong> Remote Milvus (external service)</p>\n</li>\n<li>\n<p><strong>Example C:</strong> Inline FAISS (SQLite backend)</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed {openshift-platform} {ocp-minimum-version} or newer.</p>\n</li>\n<li>\n<p>You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on {org-name} OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>You have cluster administrator privileges for your {openshift-platform} cluster.</p>\n</li>\n<li>\n<p>You are logged in to {productname-long}.</p>\n</li>\n<li>\n<p>You have activated the Llama Stack Operator in {productname-short}.</p>\n</li>\n<li>\n<p>You have deployed an inference model with vLLM (for example, <strong>llama-3.2-3b-instruct</strong>) and selected <strong>Make deployed models available through an external route</strong> and <strong>Require token authentication</strong> during model deployment. In addition, in <strong>Add custom runtime arguments</strong>, you have added <strong>--enable-auto-tool-choice</strong>.</p>\n</li>\n<li>\n<p>You have the correct inference model identifier, for example, <code>llama-3-2-3b</code>.</p>\n</li>\n<li>\n<p>You have the model endpoint URL ending with <code>/v1</code>, for example, <code><a href=\"https://llama-32-3b-instruct-predictor:8443/v1\" class=\"bare\">https://llama-32-3b-instruct-predictor:8443/v1</a></code>.</p>\n</li>\n<li>\n<p>You have the API token required to access the model endpoint.</p>\n</li>\n<li>\n<p>You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for {rosa-productname}</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Open a new terminal window and log in to your {openshift-platform} cluster from the CLI:</p>\n<div class=\"paragraph\">\n<p>In the upper-right corner of the OpenShift web console, click your user name and select <strong>Copy login command</strong>. After you have logged in, click <strong>Display token</strong>. Copy the <strong>Log in with this token</strong> command and paste it in the {openshift-cli}.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a secret that contains the inference model environment variables:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">export INFERENCE_MODEL=\"llama-3-2-3b\"\nexport VLLM_URL=\"https://llama-32-3b-instruct-predictor:8443/v1\"\nexport VLLM_TLS_VERIFY=\"false\"   # Use \"true\" in production\nexport VLLM_API_TOKEN=\"&lt;token identifier&gt;\"\n\noc create secret generic llama-stack-inference-model-secret \\\n  --from-literal=INFERENCE_MODEL=\"$INFERENCE_MODEL\" \\\n  --from-literal=VLLM_URL=\"$VLLM_URL\" \\\n  --from-literal=VLLM_TLS_VERIFY=\"$VLLM_TLS_VERIFY\" \\\n  --from-literal=VLLM_API_TOKEN=\"$VLLM_API_TOKEN\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Choose <strong>one</strong> of the following deployment examples:</p>\n</li>\n</ol>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>To enable Llama Stack in a disconnected environment, you need add the following parameters to your <code>LlamaStackDistribution</code> custom resource.</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">- name: SENTENCE_TRANSFORMERS_HOME\n  value: /opt/app-root/src/.cache/huggingface/hub\n- name: HF_HUB_OFFLINE\n  value: \"1\"\n- name: TRANSFORMERS_OFFLINE\n  value: \"1\"\n- name: HF_DATASETS_OFFLINE\n  value: \"1\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The built-in Llama Stack tool <code>websearch</code> is not available in the {org-name} Llama Stack Distribution in disconnected environments. In addition, the built-in Llama Stack tool <code>wolfram_alpha</code> tool is not available in the {org-name} Llama Stack Distribution in all clusters.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_example_a_llamastackdistribution_with_inline_milvus\">Example A: LlamaStackDistribution with <strong>Inline Milvus</strong></h3>\n<div class=\"paragraph\">\n<p>Use this example for development or small datasets where an embedded, single-node Milvus is sufficient. No <code>MILVUS_*</code> connection variables are required.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift web console, select <strong>Administrator</strong>  <strong>Quick Create</strong> (<span class=\"image\"><img src=\"/static/docs/images/quick-create-icon.png\" alt=\"quick create icon\"></span>)  <strong>Import YAML</strong>, and create a CR similar to the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: llamastack.io/v1alpha1\nkind: LlamaStackDistribution\nmetadata:\n  name: lsd-llama-milvus-inline\nspec:\n  replicas: 1\n  server:\n    containerSpec:\n      resources:\n        requests:\n          cpu: \"250m\"\n          memory: \"500Mi\"\n        limits:\n          cpu: 4\n          memory: \"12Gi\"\n      env:\n        - name: INFERENCE_MODEL\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: INFERENCE_MODEL\n        - name: VLLM_MAX_TOKENS\n          value: \"4096\"\n        - name: VLLM_URL\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_URL\n        - name: VLLM_TLS_VERIFY\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_TLS_VERIFY\n        - name: VLLM_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_API_TOKEN\n      name: llama-stack\n      port: 8321\n    distribution:\n      name: rh-dev</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>rh-dev</code> value is an internal image reference. When you create the <code>LlamaStackDistribution</code> custom resource, the {productname-short} Operator automatically resolves <code>rh-dev</code> to the container image in the appropriate registry. This internal image reference allows the underlying image to update without requiring changes to your custom resource.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_example_b_llamastackdistribution_with_remote_milvus\">Example B: LlamaStackDistribution with <strong>Remote Milvus</strong></h3>\n<div class=\"paragraph\">\n<p>Use this example for production-grade or large datasets with an external Milvus service. This configuration reads both <code>MILVUS_ENDPOINT</code> <strong>and</strong> <code>MILVUS_TOKEN</code> from a dedicated secret.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Create the Milvus connection secret:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\"># Required: gRPC endpoint on port 19530\nexport MILVUS_ENDPOINT=\"tcp://milvus-service:19530\"\nexport MILVUS_TOKEN=\"&lt;milvus-root-or-user-token&gt;\"\nexport MILVUS_CONSISTENCY_LEVEL=\"Bounded\"   # Optional; choose per your deployment\n\noc create secret generic milvus-secret \\\n  --from-literal=MILVUS_ENDPOINT=\"$MILVUS_ENDPOINT\" \\\n  --from-literal=MILVUS_TOKEN=\"$MILVUS_TOKEN\" \\\n  --from-literal=MILVUS_CONSISTENCY_LEVEL=\"$MILVUS_CONSISTENCY_LEVEL\"</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Use the <strong>gRPC port <code>19530</code></strong> for <code>MILVUS_ENDPOINT</code>. Ports such as <code>9091</code> are typically used for health checks and are not valid for client traffic.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>In the OpenShift web console, select <strong>Administrator</strong>  <strong>Quick Create</strong> (<span class=\"image\"><img src=\"/static/docs/images/quick-create-icon.png\" alt=\"quick create icon\"></span>)  <strong>Import YAML</strong>, and create a CR similar to the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: llamastack.io/v1alpha1\nkind: LlamaStackDistribution\nmetadata:\n  name: lsd-llama-milvus-remote\nspec:\n  replicas: 1\n  server:\n    containerSpec:\n      resources:\n        requests:\n          cpu: \"250m\"\n          memory: \"500Mi\"\n        limits:\n          cpu: 4\n          memory: \"12Gi\"\n      env:\n        - name: INFERENCE_MODEL\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: INFERENCE_MODEL\n        - name: VLLM_MAX_TOKENS\n          value: \"4096\"\n        - name: VLLM_URL\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_URL\n        - name: VLLM_TLS_VERIFY\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_TLS_VERIFY\n        - name: VLLM_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_API_TOKEN\n        # --- Remote Milvus configuration from secret ---\n        - name: MILVUS_ENDPOINT\n          valueFrom:\n            secretKeyRef:\n              name: milvus-secret\n              key: MILVUS_ENDPOINT\n        - name: MILVUS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: milvus-secret\n              key: MILVUS_TOKEN\n        - name: MILVUS_CONSISTENCY_LEVEL\n          valueFrom:\n            secretKeyRef:\n              name: milvus-secret\n              key: MILVUS_CONSISTENCY_LEVEL\n      name: llama-stack\n      port: 8321\n    distribution:\n      name: rh-dev</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_example_c_llamastackdistribution_with_inline_faiss\">Example C: LlamaStackDistribution with <strong>Inline FAISS</strong></h3>\n<div class=\"paragraph\">\n<p>Use this example to enable the inline FAISS vector store. This configuration stores vector data locally within the Llama Stack container using an embedded SQLite database.</p>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift web console, select <strong>Administrator</strong>  <strong>Quick Create</strong> (<span class=\"image\"><img src=\"/static/docs/images/quick-create-icon.png\" alt=\"quick create icon\"></span>)  <strong>Import YAML</strong>, and create a CR similar to the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: llamastack.io/v1alpha1\nkind: LlamaStackDistribution\nmetadata:\n  name: lsd-llama-faiss-inline\nspec:\n  replicas: 1\n  server:\n    containerSpec:\n      resources:\n        requests:\n          cpu: \"250m\"\n          memory: \"500Mi\"\n        limits:\n          cpu: \"8\"\n          memory: \"12Gi\"\n      env:\n        # vLLM inference model configuration\n        - name: INFERENCE_MODEL\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: INFERENCE_MODEL\n        - name: VLLM_URL\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_URL\n        - name: VLLM_TLS_VERIFY\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_TLS_VERIFY\n        - name: VLLM_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: llama-stack-inference-model-secret\n              key: VLLM_API_TOKEN\n\n        # Enable inline FAISS with SQLite backend\n        - name: ENABLE_FAISS\n          value: faiss\n        - name: FAISS_KVSTORE_DB_PATH\n          value: /opt/app-root/src/.llama/distributions/rh/sqlite_vec.db\n\n        # Recommended workarounds for SQLite accessibility and version check\n        - name: LLAMA_STACK_CONFIG_DIR\n          value: /opt/app-root/src/.llama/distributions/rh\n      name: llama-stack\n      port: 8321\n    distribution:\n      name: rh-dev</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <code>FAISS_KVSTORE_DB_PATH</code> environment variable defines the local path where the FAISS SQLite backend stores its index data. Ensure that this directory exists and is writable inside the container. Inline FAISS is only suitable for experimental or testing use cases.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Click <strong>Create</strong>.</p>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>In the left-hand navigation, click <strong>Workloads</strong>  <strong>Pods</strong> and verify that the Llama Stack pod is running in the correct namespace.</p>\n</li>\n<li>\n<p>To verify that the Llama Stack server is running, click the pod name and select the <strong>Logs</strong> tab. Look for output similar to the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-log\" data-lang=\"log\">INFO     2025-05-15 11:23:52,750 __main__:498 server: Listening on ['::', '0.0.0.0']:8321\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO     2025-05-15 11:23:52,765 __main__:151 server: Starting up\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Confirm that a Service resource for the Llama Stack backend is present in your namespace and points to the running pod: <strong>Networking</strong>  <strong>Services</strong>.</p>\n</li>\n</ul>\n</div>\n<div class=\"admonitionblock tip\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Tip</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you switch from Inline Milvus to Remote Milvus, delete the existing pod to ensure the new environment variables and backing store are picked up cleanly.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"ingesting-content-into-a-llama-model_{context}\">Ingesting content into a Llama model</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can quickly customize and prototype your retrievable content by ingesting raw text into your model from inside a Jupyter notebook. This approach avoids building a separate ingestion pipeline. By using the Llama Stack SDK, you can embed and store text in your vector store in real time, enabling immediate RAG workflows.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed {openshift-platform} {ocp-minimum-version} or newer.</p>\n</li>\n<li>\n<p>You have deployed a Llama 3.2 model with a vLLM model server.</p>\n</li>\n<li>\n<p>You have created a <code>LlamaStackDistribution</code> instance (Llama Stack).</p>\n</li>\n<li>\n<p>You have created a workbench within a project.</p>\n</li>\n<li>\n<p>You have opened a Jupyter notebook and it is running in your workbench environment.</p>\n</li>\n<li>\n<p>You have installed the <code>llama_stack_client</code> version 0.3.1 or later in your workbench environment.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a new notebook cell, install the client:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">%pip install llama_stack_client</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Import <code>LlamaStackClient</code> and create a client instance:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">from llama_stack_client import LlamaStackClient\nclient = LlamaStackClient(base_url=\"&lt;your deployment endpoint&gt;\")</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>List the available models:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\"># Fetch all registered models\nmodels = client.models.list()</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Verify that the list includes your Llama model and an embedding model. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">[Model(identifier='llama-32-3b-instruct', metadata={}, api_model_type='llm', provider_id='vllm-inference', provider_resource_id='llama-32-3b-instruct', type='model', model_type='llm'),\n Model(identifier='ibm-granite/granite-embedding-125m-english', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', provider_resource_id='ibm-granite/granite-embedding-125m-english', type='model', model_type='embedding')]</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Select one LLM and one embedding model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">model_id = next(m.identifier for m in models if m.model_type == \"llm\")\nembedding_model = next(m for m in models if m.model_type == \"embedding\")\nembedding_model_id = embedding_model.identifier\nembedding_dimension = int(embedding_model.metadata[\"embedding_dimension\"])</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>(Optional) Create a vector store (choose one). Skip this step if you already have one.</p>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 1. Option 1: Inline Milvus Lite (embedded)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_inline_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"milvus\",   # inline Milvus Lite\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered inline Milvus Lite DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nUse inline Milvus Lite for development and small datasets. Persistence and scale are limited compared to remote Milvus.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 2. Option 2: Remote Milvus (recommended for production)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_remote_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"milvus-remote\",  # remote Milvus provider\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered remote Milvus DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nEnsure your <code>LlamaStackDistribution</code> sets <code>MILVUS_ENDPOINT</code> (gRPC <code>:19530</code>) and <code>MILVUS_TOKEN</code>.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 3. Option 3: Inline FAISS (SQLite backend)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_faiss_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"faiss\",   # inline FAISS provider\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered inline FAISS DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nInline FAISS (available in {productname-short} 3.0 and later) is a lightweight, in-process vector store with SQLite-based persistence. It is best for local experimentation, disconnected environments, or single-node RAG deployments.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>If you already have a vector store, set its identifier:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\"># For an existing vector store:\n# vector_store_id = \"&lt;your existing vector store ID&gt;\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Define raw text to ingest:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">raw_text = \"\"\"Llama Stack can embed raw text into a vector store for retrieval.\nThis example ingests a small passage for demonstration.\"\"\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Ingest raw text by using the Vector Store Files API:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">items = [\n    {\n        \"id\": \"raw_text_001\",\n        \"text\": raw_text,\n        \"mime_type\": \"text/plain\",\n        \"metadata\": {\"source\": \"example_passage\"},\n    }\n]\nresult = client.vector_stores.files.create(\n    vector_store_id=vector_store_id,\n    items=items,\n    chunk_size_in_tokens=100,\n)\nprint(\"Text ingestion result:\", result)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Ingest an HTML source:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">html_item = [\n    {\n        \"id\": \"doc_html_001\",\n        \"text\": \"https://www.paulgraham.com/greatwork.html\",\n        \"mime_type\": \"text/html\",\n        \"metadata\": {\"note\": \"Example URL\"},\n    }\n]\nresult = client.vector_stores.files.create(\n    vector_store_id=vector_store_id,\n    items=html_item,\n    chunk_size_in_tokens=50,\n)\nprint(\"HTML ingestion result:\", result)</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>Review the output to confirm successful ingestion. A typical response includes file or chunk counts and any warnings or errors.</p>\n</li>\n<li>\n<p>The model list returned by <code>client.models.list()</code> includes your Llama 3.2 model and an embedding model.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"querying-ingested-content-in-a-llama-model_{context}\">Querying ingested content in a Llama model</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can use the Llama Stack SDK in your Jupyter notebook to query ingested content by running retrieval-augmented generation (RAG) queries on text or HTML stored in your vector store. You can perform one-off lookups or start multi-turn conversational flows without setting up a separate retrieval service.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed {openshift-platform} {ocp-minimum-version} or newer.</p>\n</li>\n<li>\n<p>You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on {org-name} OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>If you are using GPU acceleration, you have at least one NVIDIA GPU available.</p>\n</li>\n<li>\n<p>You have activated the Llama Stack Operator in {productname-short}.</p>\n</li>\n<li>\n<p>You have deployed an inference model, for example, the <strong>llama-3.2-3b-instruct</strong> model.</p>\n</li>\n<li>\n<p>You have created a <code>LlamaStackDistribution</code> instance to enable RAG functionality.</p>\n</li>\n<li>\n<p>You have created a workbench within a project and opened a running Jupyter notebook.</p>\n</li>\n<li>\n<p>You have installed <code>llama_stack_client</code> version 0.3.1 or later in your workbench environment.</p>\n</li>\n<li>\n<p>You have already ingested content into a vector store.</p>\n</li>\n</ul>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>This procedure requires that you have already ingested some text, HTML, or document data into a vector store, and that this content is available for retrieval. If no content is ingested, queries return empty results.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a new notebook cell, install the client:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">%pip install -q llama_stack_client</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>In a new notebook cell, import <code>Agent</code>, <code>AgentEventLogger</code>, and <code>LlamaStackClient</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">from llama_stack_client import Agent, AgentEventLogger, LlamaStackClient</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a client instance by setting your deployment endpoint:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">client = LlamaStackClient(base_url=\"&lt;your deployment endpoint&gt;\")</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>List available models:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">models = client.models.list()</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Select an LLM (and, if needed below, capture an embedding model for store registration):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">model_id = next(m.identifier for m in models if m.model_type == \"llm\")\n\nembedding = next((m for m in models if m.model_type == \"embedding\"), None)\nif embedding:\n    embedding_model_id = embedding.identifier\n    embedding_dimension = int(embedding.metadata.get(\"embedding_dimension\", 768))</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>If you do not already have a vector store ID, register a vector store (choose one):</p>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 4. Option 1: Inline Milvus Lite (embedded)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_inline_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"milvus\",   # inline Milvus Lite\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered inline Milvus Lite DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nUse inline Milvus Lite for development and small datasets. Persistence and scale are limited compared to remote Milvus.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 5. Option 2: Remote Milvus (recommended for production)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_remote_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"milvus-remote\",  # remote Milvus provider\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered remote Milvus DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nEnsure your <code>LlamaStackDistribution</code> sets <code>MILVUS_ENDPOINT</code> (gRPC <code>:19530</code>) and <code>MILVUS_TOKEN</code>.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 6. Option 3: Inline FAISS (SQLite backend)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_faiss_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"faiss\",   # inline FAISS provider\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered inline FAISS DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nInline FAISS (available in {productname-short} 3.0 and later) is a lightweight, in-process vector store with SQLite-based persistence. It is best for local experimentation, disconnected environments, or single-node RAG deployments.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>If you already have a vector store, set its identifier:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\"># For an existing store:\n# vector_store_id = \"&lt;your existing vector store ID&gt;\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Query the ingested content by using the OpenAI-compatible Responses API with file search:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">query = \"What benefits do the ingested passages provide for retrieval?\"\n\nresponse = client.responses.create(\n    model=model_id,\n    input=query,\n    tools=[\n        {\n            \"type\": \"file_search\",\n            \"vector_store_ids\": [vector_store_id],\n        }\n    ],\n)\nprint(\"Responses API result:\", getattr(response, \"output_text\", response))</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Query the ingested content by using the high-level Agent API:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">agent = Agent(\n    client,\n    model=model_id,\n    instructions=\"You are a helpful assistant.\",\n    tools=[\n        {\n            \"name\": \"builtin::rag/knowledge_search\",\n            \"args\": {\"vector_store_ids\": [vector_store_id]},\n        }\n    ],\n)\n\nprompt = \"How do you do great work?\"\nprint(\"Prompt&gt;\", prompt)\n\nsession_id = agent.create_session(\"rag_session\")\nstream = agent.create_turn(\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    session_id=session_id,\n    stream=True,\n)\n\nfor log in AgentEventLogger().log(stream):\n    log.print()</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The notebook prints query results for both the Responses API and the Agent API.</p>\n</li>\n<li>\n<p>No errors appear in the output, confirming the model can retrieve and respond to ingested content from your vector store.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"preparing-documents-with-docling-for-llama-stack-retrieval_{context}\">Preparing documents with Docling for Llama Stack retrieval</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>You can transform your source documents with a Docling-enabled pipeline and ingest the output into a Llama Stack vector store by using the Llama Stack SDK. This modular approach separates document preparation from ingestion, yet still delivers an end-to-end, retrieval-augmented generation (RAG) workflow.</p>\n</div>\n<div class=\"paragraph\">\n<p>The pipeline registers a vector store and downloads the source PDFs, then splits them for parallel processing and converts each batch to Markdown with Docling. It generates sentence-transformer embeddings from the Markdown and stores them in the vector store, making the documents searchable through Llama Stack.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed {openshift-platform} {ocp-minimum-version} or newer.</p>\n</li>\n<li>\n<p>You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see <a href=\"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html\" target=\"_blank\" rel=\"noopener\">NVIDIA GPU Operator on {org-name} OpenShift Container Platform</a> in the NVIDIA documentation.</p>\n</li>\n<li>\n<p>You have logged in to the {openshift-platform} web console.</p>\n</li>\n<li>\n<p>You have a project and access to pipelines in the {productname-short} dashboard.</p>\n</li>\n<li>\n<p>You have created and configured a pipeline server within the project that contains your workbench.</p>\n</li>\n<li>\n<p>You have activated the Llama Stack Operator in {productname-short}.</p>\n</li>\n<li>\n<p>You have deployed an inference model, for example, the <strong>llama-3.2-3b-instruct</strong> model.</p>\n</li>\n<li>\n<p>You have configured a Llama Stack deployment by creating a <code>LlamaStackDistribution</code> instance to enable RAG functionality.</p>\n</li>\n<li>\n<p>You have created a workbench within a project.</p>\n</li>\n<li>\n<p>You have opened a Jupyter notebook and it is running in your workbench environment.</p>\n</li>\n<li>\n<p>You have installed local object storage buckets and created connections, as described in <a href=\"{odhdocshome}/working-on-projects/#adding-a-connection-to-your-project_projects\">Adding a connection to your project</a>.</p>\n</li>\n<li>\n<p>You have installed the <code>llama_stack_client</code> version 0.3.1 or later in your workbench environment.</p>\n</li>\n<li>\n<p>You have compiled to YAML a pipeline that includes a Docling transform, either one of the RAG demo samples or your own custom pipeline.</p>\n</li>\n<li>\n<p>Your project quota allows between 500 millicores (0.5 CPU) and 4 CPU cores for the pipeline run.</p>\n</li>\n<li>\n<p>Your project quota allows from 2 GiB up to 6 GiB of RAM for the pipeline run.</p>\n</li>\n<li>\n<p>If you are using GPU acceleration, you have at least one NVIDIA GPU available.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a new notebook cell, install the client:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">%pip install -q llama_stack_client</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>In a new notebook cell, import <code>Agent</code>, <code>AgentEventLogger</code>, and <code>LlamaStackClient</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">from llama_stack_client import Agent, AgentEventLogger, LlamaStackClient</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>In a new notebook cell, assign your deployment endpoint to the <code>base_url</code> parameter to create a <code>LlamaStackClient</code> instance:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">client = LlamaStackClient(base_url=\"&lt;your deployment endpoint&gt;\")</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>List the available models:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">models = client.models.list()</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Select the first LLM and the first embedding model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">model_id = next(m.identifier for m in models if m.model_type == \"llm\")\nembedding_model = next(m for m in models if m.model_type == \"embedding\")\nembedding_model_id = embedding_model.identifier\nembedding_dimension = int(embedding_model.metadata.get(\"embedding_dimension\", 768))</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Register a vector store (choose one option). Skip this step if your pipeline registers the store automatically.</p>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 7. Option 1: Inline Milvus Lite (embedded)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_inline_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"milvus\",   # inline Milvus Lite\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered inline Milvus Lite DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nInline Milvus Lite is best for development. Data durability and scale are limited compared to remote Milvus.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 8. Option 2: Remote Milvus (recommended for production)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_remote_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"milvus-remote\",  # remote Milvus provider\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered remote Milvus DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nEnsure your <code>LlamaStackDistribution</code> includes <code>MILVUS_ENDPOINT</code> and <code>MILVUS_TOKEN</code> (gRPC <code>:19530</code>).\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"exampleblock\">\n<div class=\"title\">Example 9. Option 3: Inline FAISS (SQLite backend)</div>\n<div class=\"content\">\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">vector_store_name = \"my_faiss_db\"\nvector_store = client.vector_stores.create(\n    name=vector_store_name,\n    extra_body={\n        \"embedding_model\": embedding_model_id,\n        \"embedding_dimension\": embedding_dimension,\n        \"provider_id\": \"faiss\",   # inline FAISS provider\n    },\n)\nvector_store_id = vector_store.id\nprint(f\"Registered inline FAISS DB: {vector_store_id}\")</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\nInline FAISS (available in {productname-short} 3.0 and later) is a lightweight, in-process vector store with SQLite-based persistence. It is best for local experimentation, disconnected environments, or single-node RAG deployments.\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you are using the sample Docling pipeline from the RAG demo repository, the pipeline registers the vector store automatically and you can skip the previous step. If you are using your own pipeline, you must register the vector store yourself.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>In the {openshift-platform} web console, import your YAML file containing your Docling pipeline into your project, as described in <a href=\"{odhdocshome}/working-with-ai-pipelines/#importing-a-pipeline-version\">Importing a pipeline version</a>.</p>\n</li>\n<li>\n<p>Create a pipeline run to execute your Docling pipeline, as described in <a href=\"{odhdocshome}/working-with-ai-pipelines/#executing-a-pipeline-run_ai-pipelines\">Executing a pipeline run</a>. The pipeline run inserts your PDF documents into the vector store. If you run the Docling pipeline from the <a href=\"https://github.com/opendatahub-io/rag/tree/main/demos/kfp/docling/pdf-conversion\">RAG demo samples repository</a>, you can optionally customize the following parameters before starting the pipeline run:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>base_url</code>: The base URL to fetch PDF files from.</p>\n</li>\n<li>\n<p><code>pdf_filenames</code>: A comma-separated list of PDF filenames to download and convert.</p>\n</li>\n<li>\n<p><code>num_workers</code>: The number of parallel workers.</p>\n</li>\n<li>\n<p><code>vector_store_id</code>: The vector store identifier.</p>\n</li>\n<li>\n<p><code>service_url</code>: The Milvus service URL (only for remote Milvus).</p>\n</li>\n<li>\n<p><code>embed_model_id</code>: The embedding model to use.</p>\n</li>\n<li>\n<p><code>max_tokens</code>: The maximum tokens for each chunk.</p>\n</li>\n<li>\n<p><code>use_gpu</code>: Enable or disable GPU acceleration.</p>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>In your Jupyter notebook, query the LLM with a question that relates to the ingested content. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">from llama_stack_client import Agent, AgentEventLogger\nimport uuid\n\nrag_agent = Agent(\n    client,\n    model=model_id,\n    instructions=\"You are a helpful assistant\",\n    tools=[\n        {\n            \"name\": \"builtin::rag/knowledge_search\",\n            \"args\": {\"vector_store_ids\": [vector_store_id]},\n        }\n    ],\n)\n\nprompt = \"What can you tell me about the birth of word processing?\"\nprint(\"prompt&gt;\", prompt)\n\nsession_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n\nresponse = rag_agent.create_turn(\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    session_id=session_id,\n    stream=True,\n)\n\nfor log in AgentEventLogger().log(response):\n    log.print()</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Query chunks from the vector store:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">query_result = client.vector_io.query(\n    vector_store_id=vector_store_id,\n    query=\"what do you know about?\",\n)\nprint(query_result)</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Verification</div>\n<ul>\n<li>\n<p>The pipeline run completes successfully in your project.</p>\n</li>\n<li>\n<p>Document embeddings are stored in the vector store and are available for retrieval.</p>\n</li>\n<li>\n<p>No errors or warnings appear in the pipeline logs or your notebook output.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"about-llama-stack-search-types_{context}\">About Llama stack search types</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>Llama Stack supports keyword, vector, and hybrid search modes for retrieving context in retrieval-augmented generation (RAG) workloads. Each mode offers different tradeoffs in precision, recall, semantic depth, and computational cost.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_supported_search_modes\">Supported search modes</h3>\n<div class=\"sect3\">\n<h4 id=\"_keyword_search\">Keyword search</h4>\n<div class=\"paragraph\">\n<p>Keyword search applies lexical matching techniques, such as TF-IDF or BM25, to locate documents that contain exact or near-exact query terms. This approach is effective when precise term-matching is critical and remains widely used in information-retrieval systems. For more information, see <a href=\"https://www.researchgate.net/publication/220613776_The_Probabilistic_Relevance_Framework_BM25_and_Beyond\">The Probabilistic Relevance Framework: BM25 and Beyond</a>.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_vector_search\">Vector search</h4>\n<div class=\"paragraph\">\n<p>Vector search encodes documents and queries as dense numerical vectors, known as embeddings, and measures similarity with metrics such as cosine similarity or inner product. This approach captures contextual meaning and supports semantic matching beyond exact word overlap. For more information, see <a href=\"https://ieeexplore.ieee.org/document/8733051\">Billion-scale similarity search with GPUs</a>.</p>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_hybrid_search\">Hybrid search</h4>\n<div class=\"paragraph\">\n<p>Hybrid search blends keyword and vector techniques, typically by combining individual scores with a weighted sum or methods, such as Reciprocal Rank Fusion (RRF). This approach returns results that balance exact matches with semantic relevance. For more information, see <a href=\"https://arxiv.org/html/2410.20381v1\">Sparse, Dense, and Hybrid Retrieval for Answer Ranking</a>.</p>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_retrieval_database_support\">Retrieval database support</h3>\n<div class=\"paragraph\">\n<p>Milvus is the supported retrieval database for Llama Stack. It currently provides vector search. However, keyword and hybrid search capabilities are not currently supported.</p>\n</div>\n</div>\n</div>\n</div>","id":"8b9149d7-d204-5641-81af-8e7bcb66d45c","document":{"title":"Deploying a RAG stack in a project"}},"markdownRemark":null},"pageContext":{"id":"8b9149d7-d204-5641-81af-8e7bcb66d45c"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}