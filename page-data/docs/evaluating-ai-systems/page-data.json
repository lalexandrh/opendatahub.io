{"componentChunkName":"component---src-templates-docs-page-tsx","path":"/docs/evaluating-ai-systems/","result":{"data":{"allFile":{"edges":[{"node":{"childAsciidoc":{"fields":{"slug":"/docs/README/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/api-workbench/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"api-workbench-overview_api-workbench"},{"parentId":null,"name":"Creating a custom image by using the <code>ImageStream</code> CRD","level":1,"index":1,"id":"api-custom-image-creating_api-workbench"},{"parentId":null,"name":"Creating a workbench by using the <code>Notebook</code> CRD","level":1,"index":2,"id":"api-workbench-creating_api-workbench"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/configuring-your-model-serving-platform/"},"sections":[{"parentId":null,"name":"About model-serving platforms","level":1,"index":0,"id":"configuring-your-model-serving-platform_odh-admin"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"About model serving","level":2,"index":0,"id":"about-model-serving_odh-admin"},{"parentId":"about-model-serving_odh-admin","name":"Model serving platform","level":3,"index":0,"id":"_model_serving_platform"},{"parentId":"about-model-serving_odh-admin","name":"NVIDIA NIM model serving platform","level":3,"index":1,"id":"_nvidia_nim_model_serving_platform"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"Model-serving runtimes","level":2,"index":1,"id":"model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes_odh-admin","name":"ServingRuntime","level":3,"index":0,"id":"_servingruntime"},{"parentId":"model-serving-runtimes_odh-admin","name":"InferenceService","level":3,"index":1,"id":"_inferenceservice"},{"parentId":"configuring-your-model-serving-platform_odh-admin","name":"Model-serving runtimes for accelerators","level":2,"index":2,"id":"model-serving-runtimes-for-accelerators_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"NVIDIA GPUs","level":3,"index":0,"id":"_nvidia_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Intel Gaudi accelerators","level":3,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"AMD GPUs","level":3,"index":2,"id":"_amd_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"IBM Spyre AI accelerators on x86 and IBM Z","level":3,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86_and_ibm_z"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Supported model-serving runtimes","level":3,"index":4,"id":"supported-model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Tested and verified model-serving runtimes","level":3,"index":5,"id":"tested-verified-runtimes_odh-admin"},{"parentId":null,"name":"Configuring model servers","level":1,"index":1,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the model serving platform","level":2,"index":0,"id":"enabling-the-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers","name":"Enabling speculative decoding and multi-modal inferencing","level":2,"index":1,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_odh-admin"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":2,"index":2,"id":"adding-a-custom-model-serving-runtime_odh-admin"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified runtime","level":2,"index":3,"id":"adding-a-tested-and-verified-runtime_odh-admin"},{"parentId":null,"name":"Configuring model servers on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform","name":"Enabling the NVIDIA NIM model serving platform","level":2,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_odh-admin"},{"parentId":null,"name":"Customizing model deployments","level":1,"index":3,"id":"_customizing_model_deployments"},{"parentId":"_customizing_model_deployments","name":"Customizing the parameters of a deployed model-serving runtime","level":2,"index":0,"id":"customizing-parameters-serving-runtime_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizable model serving runtime parameters","level":2,"index":1,"id":"customizable-model-serving-runtime-parameters_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizing the vLLM model-serving runtime","level":2,"index":2,"id":"Customizing-the-vllm-runtime_odh-admin"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/customize-models-to-build-gen-ai-applications/"},"sections":[{"parentId":null,"name":"Overview of the model customization workflow","level":1,"index":0,"id":"overview-of-the-model-customization-workflow_custom-models"},{"parentId":null,"name":"Set up your working environment","level":1,"index":1,"id":"set-up-your-working-environment_custom-models"},{"parentId":"set-up-your-working-environment_custom-models","name":"About the Red&#160;Hat Python Index","level":2,"index":0,"id":"about-the-python-index_custom-models"},{"parentId":"set-up-your-working-environment_custom-models","name":"Mirror the Python Index for your disconnected environment","level":2,"index":1,"id":"mirror-the-python-index_custom-models"},{"parentId":"set-up-your-working-environment_custom-models","name":"Install packages","level":2,"index":2,"id":"install-packages_custom-models"},{"parentId":"set-up-your-working-environment_custom-models","name":"Import example notebooks","level":2,"index":3,"id":"import-example-notebooks_custom-models"},{"parentId":"import-example-notebooks_custom-models","name":"Clone an example Git repository","level":3,"index":0,"id":"clone-an-example-git-repository_custom-models"},{"parentId":null,"name":"Prepare your data for AI consumption","level":1,"index":2,"id":"prepare-your-data-for-ai-consumption_custom-models"},{"parentId":"prepare-your-data-for-ai-consumption_custom-models","name":"Process data by using Docling","level":2,"index":0,"id":"_process_data_by_using_docling"},{"parentId":"prepare-your-data-for-ai-consumption_custom-models","name":"Explore the data processing examples","level":2,"index":1,"id":"explore-the-data-processing-examples_custom-models"},{"parentId":"prepare-your-data-for-ai-consumption_custom-models","name":"Automate data processing steps by building AI pipelines","level":2,"index":2,"id":"_automate_data_processing_steps_by_building_ai_pipelines"},{"parentId":"prepare-your-data-for-ai-consumption_custom-models","name":"Explore the kubeflow pipeline examples","level":2,"index":3,"id":"explore-the-kubeflow-pipeline-examples_custom-models"},{"parentId":null,"name":"Generate synthetic data","level":1,"index":3,"id":"generate-synthetic-data_custom-models"},{"parentId":"generate-synthetic-data_custom-models","name":"Explore the SDG Hub examples","level":2,"index":0,"id":"explore-the-sdg-hub-examples_custom-models"},{"parentId":"generate-synthetic-data_custom-models","name":"Guided example - Build a KFP pipeline for SDG","level":2,"index":1,"id":"guided-example-build-a-kfp-pipeline-for-sdg_custom-models"},{"parentId":null,"name":"Train the model by using your prepared data","level":1,"index":4,"id":"train-the-model-by-using-your-prepared-data_custom-models"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Explore the Training Hub examples","level":2,"index":0,"id":"explore-the-training-hub-examples_custom-models"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Estimate memory usage","level":2,"index":1,"id":"estimate-memory-usage_custom-models"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Compare the performance of OSFT and SFT training algorithms","level":2,"index":2,"id":"compare-the-performance-of-osft-and-sft_custom-models"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Distribute training jobs by using the KubeFlow Trainer Operator","level":2,"index":3,"id":"_distribute_training_jobs_by_using_the_kubeflow_trainer_operator"},{"parentId":"train-the-model-by-using-your-prepared-data_custom-models","name":"Distributed fine-tuning with Training Hub and Kubeflow Trainer","level":2,"index":4,"id":"_distributed_fine_tuning_with_training_hub_and_kubeflow_trainer"},{"parentId":null,"name":"End-to-end model customization workflow","level":1,"index":5,"id":"end-to-end-model-customization-workflow_custom-models"},{"parentId":null,"name":"Support philosophy: A secure platform","level":1,"index":6,"id":"support-philosophy_custom-models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/deploying-models/"},"sections":[{"parentId":null,"name":"Storing models","level":1,"index":0,"id":"deploying-models_odh-user"},{"parentId":"deploying-models_odh-user","name":"Using OCI containers for model storage","level":2,"index":0,"id":"using-oci-containers-for-model-storage_odh-user"},{"parentId":"deploying-models_odh-user","name":"Storing a model in an OCI image","level":2,"index":1,"id":"storing-a-model-in-oci-image_odh-user"},{"parentId":"deploying-models_odh-user","name":"Uploading model files to a Persistent Volume Claim (PVC)","level":2,"index":2,"id":"uploading-model-files-to-pvc_odh-user"},{"parentId":null,"name":"Deploying models","level":1,"index":1,"id":"_deploying_models"},{"parentId":"_deploying_models","name":"Deploying models on the model serving platform","level":2,"index":0,"id":"deploying-models-on-the-model-serving-platform_odh-user"},{"parentId":"_deploying_models","name":"Deploying a model stored in an OCI image by using the CLI","level":2,"index":1,"id":"deploying-model-stored-in-oci-image_odh-user"},{"parentId":"_deploying_models","name":"Deploying models by using Distributed Inference with llm-d","level":2,"index":2,"id":"deploying-models-using-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Configuring authentication for Distributed Inference with llm-d using Red&#160;Hat Connectivity Link","level":3,"index":0,"id":"configuring-authentication-for-llmd_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Enabling Distributed Inference with llm-d","level":3,"index":1,"id":"enabling-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Example usage for Distributed Inference with llm-d","level":3,"index":2,"id":"ref-example-distributed-inference_odh-user"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Single-node GPU deployment","level":4,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Multi-node deployment","level":4,"index":1,"id":"_multi_node_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Intelligent inference scheduler with KV cache routing","level":4,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Configuring authentication for Distributed Inference with llm-d using Red&#160;Hat Connectivity Link","level":3,"index":3,"id":"configuring-authentication-for-llmd_odh-user"},{"parentId":"_deploying_models","name":"Monitoring models","level":2,"index":3,"id":"_monitoring_models"},{"parentId":"_monitoring_models","name":"Viewing performance metrics for a deployed model","level":3,"index":0,"id":"viewing-performance-metrics-for-deployed-model_odh-user"},{"parentId":"_monitoring_models","name":"Viewing model-serving runtime metrics for the model serving platform","level":3,"index":1,"id":"viewing-metrics-for-the-model-serving-platform_odh-user"},{"parentId":null,"name":"Deploying models on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_deploying_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Deploying models on the NVIDIA NIM model serving platform","level":2,"index":0,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing NVIDIA NIM metrics for a NIM model","level":2,"index":1,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing performance metrics for a NIM model","level":2,"index":2,"id":"viewing-performance-metrics-for-a-nim-model_odh-user"},{"parentId":null,"name":"Making inference requests to deployed models","level":1,"index":3,"id":"_making_inference_requests_to_deployed_models"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the authentication token for a deployed model","level":2,"index":0,"id":"accessing-authentication-token-for-deployed-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the inference endpoint for a deployed model","level":2,"index":1,"id":"accessing-inference-endpoint-for-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Making inference requests to models deployed on the model serving platform","level":2,"index":2,"id":"making-inference-requests-to-models-deployed-on-model-serving-platform_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Inference endpoints","level":2,"index":3,"id":"inference-endpoints_odh-user"},{"parentId":"inference-endpoints_odh-user","name":"Caikit TGIS ServingRuntime for KServe","level":3,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"OpenVINO Model Server","level":3,"index":1,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_odh-user","name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":3,"index":2,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":3,"index":3,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM AMD GPU ServingRuntime for KServe","level":3,"index":4,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":3,"index":5,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre s390x ServingRuntime for KServe","level":3,"index":6,"id":"_vllm_spyre_s390x_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"NVIDIA Triton Inference Server","level":3,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_odh-user","name":"Seldon MLServer","level":3,"index":8,"id":"_seldon_mlserver"},{"parentId":"inference-endpoints_odh-user","name":"Additional resources","level":3,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/enabling-ai-safety/"},"sections":[{"parentId":null,"name":"Enabling AI safety with Guardrails","level":1,"index":0,"id":"enabling-ai-safety-with-guardrails_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Understanding detectors","level":2,"index":0,"id":"guardrails-detectors_safety"},{"parentId":"guardrails-detectors_safety","name":"Built-in Detector","level":3,"index":0,"id":"_built_in_detector"},{"parentId":"guardrails-detectors_safety","name":"The Hugging Face Detector serving runtime","level":3,"index":1,"id":"guardrails-configuring-the-hugging-face-detector-serving-runtime_safety"},{"parentId":"guardrails-configuring-the-hugging-face-detector-serving-runtime_safety","name":"Guardrails Detector Hugging Face serving runtime configuration values","level":4,"index":0,"id":"_guardrails_detector_hugging_face_serving_runtime_configuration_values"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Orchestrator Configuration Parameters","level":2,"index":1,"id":"guardrails-orchestrator-config-parameters_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Guardrails Gateway Config Parameters","level":2,"index":2,"id":"guardrails-gateway-config-parameters_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Deploying the Guardrails Orchestrator","level":2,"index":3,"id":"deploying-the-guardrails-orchestrator-service_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Auto-configuring Guardrails","level":2,"index":4,"id":"guardrails-auto-config_safety"},{"parentId":"enabling-ai-safety-with-guardrails_safety","name":"Configuring the OpenTelemetry exporter","level":2,"index":5,"id":"configuring-the-opentelemetry-exporter_safety"},{"parentId":null,"name":"Using Guardrails for AI safety","level":1,"index":1,"id":"using-guardrails-for-ai-safety_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Detecting PII and sensitive data","level":2,"index":0,"id":"_detecting_pii_and_sensitive_data"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":2,"index":1,"id":"detecting-pii-by-using-guardrails-with-llama-stack_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Filtering flagged content by sending requests to the regex detector","level":2,"index":2,"id":"filtering-flagged-content-by-sending-requests-to-the-regex-detector_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Securing prompts","level":2,"index":3,"id":"_securing_prompts"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Mitigating Prompt Injection by using a Hugging Face Prompt Injection detector","level":2,"index":4,"id":"mitigating-prompt-injection-by-using-a-hugging-face-prompt-injection-detector_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Moderating and safeguarding content","level":2,"index":5,"id":"_moderating_and_safeguarding_content"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Detecting hateful and profane language","level":2,"index":6,"id":"detecting-hateful-and-profane-language_safety"},{"parentId":"using-guardrails-for-ai-safety_safety","name":"Enforcing configured safety pipelines for LLM inference by using Guardrails Gateway","level":2,"index":7,"id":"enforcing-configured-safety-pipelines-for-llm-inference-using-guardrails-gateway_safety"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/evaluating-ai-systems/"},"sections":[{"parentId":null,"name":"Overview of evaluating AI systems","level":1,"index":0,"id":"overview-evaluating-ai-systems_evaluate"},{"parentId":null,"name":"Evaluating large language models","level":1,"index":1,"id":"evaluating-large-language-models_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"Setting up LM-Eval","level":2,"index":0,"id":"setting-up-lmeval_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"Enabling external resource access for LMEval jobs","level":2,"index":1,"id":"enabling-external-resource-access-for-lmeval-jobs_evaluate"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_evaluate","name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":3,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_evaluate"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_evaluate","name":"Updating LMEval job configuration using the web console","level":3,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"LM-Eval evaluation job","level":2,"index":2,"id":"lmeval-evaluation-job_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"LM-Eval evaluation job properties","level":2,"index":3,"id":"lmeval-evaluation-job-properties_evaluate"},{"parentId":"lmeval-evaluation-job-properties_evaluate","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":3,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":"evaluating-large-language-models_evaluate","name":"Performing model evaluations in the dashboard","level":2,"index":4,"id":"performing-model-evaluations-in-the-dashboard_evaluate"},{"parentId":"evaluating-large-language-models_evaluate","name":"LM-Eval scenarios","level":2,"index":5,"id":"lmeval-scenarios_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Accessing Hugging Face models with an environment variable token","level":3,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Using a custom Unitxt card","level":3,"index":1,"id":"using-a-custom-unitxt-card_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Using PVCs as storage","level":3,"index":2,"id":"using-pvcs-as-storage_evaluate"},{"parentId":"using-pvcs-as-storage_evaluate","name":"Managed PVCs","level":4,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_evaluate","name":"Existing PVCs","level":4,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_evaluate","name":"Using a KServe Inference Service","level":3,"index":3,"id":"using-a-kserve-inference-service_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Setting up LM-Eval S3 Support","level":3,"index":4,"id":"setting-up-lmeval-s3-support_evaluate"},{"parentId":"lmeval-scenarios_evaluate","name":"Using LLM-as-a-Judge metrics with LM-Eval","level":3,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_evaluate"},{"parentId":null,"name":"Using llama stack with TrustyAI","level":1,"index":2,"id":"using-llama-stack-with-trustyai_evaluate"},{"parentId":"using-llama-stack-with-trustyai_evaluate","name":"Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI","level":2,"index":0,"id":"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_evaluate"},{"parentId":"using-llama-stack-with-trustyai_evaluate","name":"Running custom evaluations with LM-Eval and Llama Stack","level":2,"index":1,"id":"running-custom-evaluations-with-LMEval-and-llama-stack_evaluate"},{"parentId":"using-llama-stack-with-trustyai_evaluate","name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":2,"index":2,"id":"detecting-pii-by-using-guardrails-with-llama-stack_evaluate"},{"parentId":null,"name":"Evaluating RAG systems with Ragas","level":1,"index":3,"id":"evaluating-rag-systems-with-ragas_evaluate"},{"parentId":"evaluating-rag-systems-with-ragas_evaluate","name":"About Ragas evaluation","level":2,"index":0,"id":"_about_ragas_evaluation"},{"parentId":"_about_ragas_evaluation","name":"Key Ragas metrics","level":3,"index":0,"id":"_key_ragas_metrics"},{"parentId":"_about_ragas_evaluation","name":"Use cases for Ragas in AI engineering workflows","level":3,"index":1,"id":"_use_cases_for_ragas_in_ai_engineering_workflows"},{"parentId":"_about_ragas_evaluation","name":"Ragas provider deployment modes","level":3,"index":2,"id":"_ragas_provider_deployment_modes"},{"parentId":"evaluating-rag-systems-with-ragas_evaluate","name":"Setting up the Ragas inline provider for development","level":2,"index":1,"id":"setting-up-ragas-inline-provider_evaluate"},{"parentId":"evaluating-rag-systems-with-ragas_evaluate","name":"Configuring the Ragas remote provider for production","level":2,"index":2,"id":"configuring-ragas-remote-provider-for-production_evaluate"},{"parentId":"evaluating-rag-systems-with-ragas_evaluate","name":"Evaluating RAG system quality with Ragas metrics","level":2,"index":3,"id":"evaluating-rag-system-quality-with-ragas_evaluate"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/experimenting-with-models-in-the-gen-ai-playground/"},"sections":[{"parentId":null,"name":"Experimenting with models in the gen AI playground","level":1,"index":0,"id":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Playground overview","level":2,"index":0,"id":"playground-overview_rhoai-user"},{"parentId":"playground-overview_rhoai-user","name":"Core capabilities","level":3,"index":0,"id":"_core_capabilities"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Playground prerequisites","level":2,"index":1,"id":"playground-prerequisites_rhoai-user"},{"parentId":"playground-prerequisites_rhoai-user","name":"Cluster administrator prerequisites","level":3,"index":0,"id":"_cluster_administrator_prerequisites"},{"parentId":"playground-prerequisites_rhoai-user","name":"User prerequisites","level":3,"index":1,"id":"_user_prerequisites"},{"parentId":"playground-prerequisites_rhoai-user","name":"Model and runtime requirements for the playground","level":3,"index":2,"id":"_model_and_runtime_requirements_for_the_playground"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Key model selection factors","level":4,"index":0,"id":"_key_model_selection_factors"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Example model configuration","level":4,"index":1,"id":"_example_model_configuration"},{"parentId":"playground-prerequisites_rhoai-user","name":"Configuring Model Control Protocol (MCP) servers","level":3,"index":3,"id":"configuring-model-control-protocol-servers_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"About the AI assets endpoint page","level":2,"index":2,"id":"About-the-ai-assets-endpoint-page_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Configuring a playground for your project","level":2,"index":3,"id":"configuring-a-playground-for-your-project_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Testing baseline model responses","level":2,"index":4,"id":"testing-baseline-model-responses_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Testing your model with retrieval augmented generation (RAG)","level":2,"index":5,"id":"testing-your-model-with-rag_rhoai-user"},{"parentId":"testing-your-model-with-rag_rhoai-user","name":"Understanding RAG settings","level":3,"index":0,"id":"understanding-rag-settings_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Testing with model control protocol (MCP) servers","level":2,"index":6,"id":"testing-with-model-control-protocol-servers_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Exporting your playground configuration","level":2,"index":7,"id":"exporting-your-playground-configuration_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Updating your playground configuration","level":2,"index":8,"id":"updating-your-playground-configuration_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Deleting a playground from your project","level":2,"index":9,"id":"Deleting-a-playground-from-your-project_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Next steps","level":2,"index":10,"id":"next-steps_rhoai-user"},{"parentId":"experimenting-with-models-in-the-gen-ai-playground_rhoai-user","name":"Troubleshooting playground issues","level":2,"index":11,"id":"troubleshooting-playground-issues_rhoai-user"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The chatbot thinks indefinitely","level":3,"index":0,"id":"_the_chatbot_thinks_indefinitely"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The model does not use RAG data","level":3,"index":1,"id":"_the_model_does_not_use_rag_data"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"MCP servers are missing from the UI","level":3,"index":2,"id":"_mcp_servers_are_missing_from_the_ui"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The model fails to call MCP tools","level":3,"index":3,"id":"_the_model_fails_to_call_mcp_tools"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/getting-started-with-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview","level":1,"index":0,"id":"overview-for-getting-started_get-started"},{"parentId":"overview-for-getting-started_get-started","name":"Data science workflow","level":2,"index":0,"id":"_data_science_workflow"},{"parentId":"overview-for-getting-started_get-started","name":"About this guide","level":2,"index":1,"id":"_about_this_guide"},{"parentId":"overview-for-getting-started_get-started","name":"Glossary of common terms","level":2,"index":2,"id":"glossary-of-common-terms_get-started"},{"parentId":null,"name":"Logging in to Open Data Hub","level":1,"index":1,"id":"logging-in_get-started"},{"parentId":"logging-in_get-started","name":"Viewing installed Open Data Hub components","level":2,"index":0,"id":"viewing-installed-components_get-started"},{"parentId":null,"name":"Creating a project","level":1,"index":2,"id":"creating-a-project_get-started"},{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":3,"id":"creating-a-workbench-select-ide_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_get-started"},{"parentId":"creating-a-workbench-select-ide_get-started","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_get-started"},{"parentId":null,"name":"Next steps","level":1,"index":4,"id":"next-steps_get-started"},{"parentId":"next-steps_get-started","name":"Additional resources","level":2,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/installing-open-data-hub/"},"sections":[{"parentId":null,"name":"Installing Open Data Hub version 2","level":1,"index":0,"id":"installing-odh-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Configuring custom namespaces","level":2,"index":0,"id":"configuring-custom-namespaces"},{"parentId":"installing-odh-v2_installv2","name":"Installing the Open Data Hub Operator version 2","level":2,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":"installing-odh-v2_installv2","name":"Installing Open Data Hub components","level":2,"index":2,"id":"installing-odh-components_installv2"},{"parentId":null,"name":"Configuring pipelines with your own Argo Workflows instance","level":1,"index":1,"id":"configuring-pipelines-with-your-own-argo-workflows-instance_install"},{"parentId":null,"name":"Installing the distributed workloads components","level":1,"index":2,"id":"installing-the-distributed-workloads-components_install"},{"parentId":null,"name":"Accessing the dashboard","level":1,"index":3,"id":"accessing-the-dashboard_install"},{"parentId":null,"name":"Working with certificates","level":1,"index":4,"id":"working-with-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Understanding how Open Data Hub handles certificates","level":2,"index":0,"id":"understanding-certificates_certs"},{"parentId":"working-with-certificates_certs","name":"Adding certificates","level":2,"index":1,"id":"_adding_certificates"},{"parentId":"working-with-certificates_certs","name":"Adding certificates to a cluster-wide CA bundle","level":2,"index":2,"id":"adding-certificates-to-a-cluster-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Adding certificates to a custom CA bundle","level":2,"index":3,"id":"adding-certificates-to-a-custom-ca-bundle_certs"},{"parentId":"working-with-certificates_certs","name":"Using self-signed certificates with Open Data Hub components","level":2,"index":4,"id":"_using_self_signed_certificates_with_open_data_hub_components"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Accessing S3-compatible object storage with self-signed certificates","level":3,"index":0,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Configuring a certificate for pipelines","level":3,"index":1,"id":"configuring-a-certificate-for-pipelines_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Configuring a certificate for workbenches","level":3,"index":2,"id":"configuring-a-certificate-for-workbenches_certs"},{"parentId":"_using_self_signed_certificates_with_open_data_hub_components","name":"Using the cluster-wide CA bundle for the model serving platform","level":3,"index":3,"id":"using-the-cluster-CA-bundle-for-model-serving_certs"},{"parentId":"working-with-certificates_certs","name":"Managing certificates without the Open Data Hub Operator","level":2,"index":5,"id":"managing-certificates-without-the-operator_certs"},{"parentId":"working-with-certificates_certs","name":"Removing the CA bundle","level":2,"index":6,"id":"_removing_the_ca_bundle"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from all namespaces","level":3,"index":0,"id":"removing-the-ca-bundle-from-all-namespaces_certs"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from a single namespace","level":3,"index":1,"id":"removing-the-ca-bundle-from-a-single-namespace_certs"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":5,"id":"viewing-logs-and-audit-records_install"},{"parentId":"viewing-logs-and-audit-records_install","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_install"},{"parentId":"configuring-the-operator-logger_install","name":"Viewing the Open Data Hub Operator logs","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_logs"},{"parentId":"viewing-logs-and-audit-records_install","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_install"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-and-monitoring-models/"},"sections":[{"parentId":null,"name":"Managing model-serving runtimes","level":1,"index":0,"id":"managing-and-monitoring-models_cluster-admin"},{"parentId":"managing-and-monitoring-models_cluster-admin","name":"Adding a custom model-serving runtime","level":2,"index":0,"id":"adding-a-custom-model-serving-runtime_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models","level":1,"index":1,"id":"_managing_and_monitoring_models"},{"parentId":"_managing_and_monitoring_models","name":"Setting a timeout for KServe","level":2,"index":0,"id":"setting-timeout-for-kserve_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Deploying models by using multiple GPU nodes","level":2,"index":1,"id":"deploying-models-using-multiple-gpu-nodes_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Configuring an inference service for Kueue","level":2,"index":2,"id":"configuring-an-inference-service-for-kueue_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Configuring an inference service for Spyre","level":2,"index":3,"id":"configuring-inference-service-for-spyre_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Optimizing performance and tuning","level":2,"index":4,"id":"_optimizing_performance_and_tuning"},{"parentId":"_optimizing_performance_and_tuning","name":"Determining GPU requirements for LLM-powered applications","level":3,"index":0,"id":"determining-gpu-requirements-for-llm-powered-applications_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Performance considerations for text-summarization and retrieval-augmented generation (RAG) applications","level":3,"index":1,"id":"performance-considerations-for-document-based-apps_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Inference performance metrics","level":3,"index":2,"id":"inference-performance-metrics_cluster-admin"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Latency","level":4,"index":0,"id":"_latency"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Throughput","level":4,"index":1,"id":"_throughput"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Cost per million tokens","level":4,"index":2,"id":"_cost_per_million_tokens"},{"parentId":"_optimizing_performance_and_tuning","name":"Configuring metrics-based autoscaling","level":3,"index":3,"id":"configuring-metrics-based-autoscaling_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Guidelines for metrics-based autoscaling","level":3,"index":4,"id":"guidelines-for-metrics-based-autoscaling_cluster-admin"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing metrics for latency and throughput-optimized scaling","level":4,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing the right sliding window","level":4,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Optimizing HPA scale-down configuration","level":4,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Considering model size for optimal scaling","level":4,"index":3,"id":"_considering_model_size_for_optimal_scaling"},{"parentId":"_managing_and_monitoring_models","name":"Monitoring models","level":2,"index":5,"id":"_monitoring_models"},{"parentId":"_monitoring_models","name":"Configuring monitoring for the model serving platform","level":3,"index":0,"id":"configuring-monitoring-for-the-model-serving-platform_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Using Grafana to monitor model performance","level":2,"index":6,"id":"_using_grafana_to_monitor_model_performance"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a Grafana metrics dashboard","level":3,"index":0,"id":"deploying-a-grafana-metrics-dashboard_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a vLLM/GPU metrics dashboard on a Grafana instance","level":3,"index":1,"id":"deploying-vllm-gpu-metrics-dashboard-grafana_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Grafana metrics","level":3,"index":2,"id":"ref-grafana-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"Accelerator metrics","level":4,"index":0,"id":"ref-accelerator-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"CPU metrics","level":4,"index":1,"id":"ref-cpu-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"vLLM metrics","level":4,"index":2,"id":"ref-vllm-metrics_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models on the NVIDIA NIM model serving platform","level":1,"index":2,"id":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Customizing model selection options for the NVIDIA NIM model serving platform","level":2,"index":0,"id":"Customizing-model-selection-options_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":2,"index":1,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin","name":"Enabling graph generation for an existing NIM deployment","level":3,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-odh/"},"sections":[{"parentId":null,"name":"Managing users and groups","level":1,"index":0,"id":"managing-users-and-groups"},{"parentId":"managing-users-and-groups","name":"Overview of user types and permissions","level":2,"index":0,"id":"overview-of-user-types-and-permissions_managing-odh"},{"parentId":"managing-users-and-groups","name":"Viewing Open Data Hub users","level":2,"index":1,"id":"viewing-data-science-users_managing-odh"},{"parentId":"managing-users-and-groups","name":"Adding users to Open Data Hub user groups","level":2,"index":2,"id":"adding-users-to-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Selecting Open Data Hub administrator and user groups","level":2,"index":3,"id":"selecting-admin-and-user-groups_managing-odh"},{"parentId":"managing-users-and-groups","name":"Deleting users","level":2,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":3,"index":0,"id":"about-deleting-users-and-resources_managing-odh"},{"parentId":"_deleting_users","name":"Stopping basic workbenches owned by other users","level":3,"index":1,"id":"stopping-basic-workbenches-owned-by-other-users_managing-odh"},{"parentId":"_deleting_users","name":"Revoking user access to basic workbenches","level":3,"index":2,"id":"revoking-user-access-to-basic-workbenches_managing-odh"},{"parentId":"_deleting_users","name":"Backing up storage data","level":3,"index":3,"id":"backing-up-storage-data_managing-odh"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":3,"index":4,"id":"cleaning-up-after-deleting-users_managing-odh"},{"parentId":null,"name":"Creating custom workbench images","level":1,"index":1,"id":"creating-custom-workbench-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from a default Open Data Hub image","level":2,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Creating a custom image from your own image","level":2,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":3,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":3,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-custom-workbench-images","name":"Enabling custom images in Open Data Hub","level":2,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":"creating-custom-workbench-images","name":"Importing a custom workbench image","level":2,"index":3,"id":"importing-a-custom-workbench-image_custom-images"},{"parentId":null,"name":"Managing applications that show in the dashboard","level":1,"index":2,"id":"managing-applications-that-show-in-the-dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Adding an application to the dashboard","level":2,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Preventing users from adding applications to the dashboard","level":2,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Disabling applications connected to Open Data Hub","level":2,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Showing or hiding information about available applications","level":2,"index":3,"id":"showing-hiding-information-about-available-applications_dashboard"},{"parentId":"managing-applications-that-show-in-the-dashboard","name":"Hiding the default basic workbench application","level":2,"index":4,"id":"hiding-the-default-basic-workbench-application_dashboard"},{"parentId":null,"name":"Creating project-scoped resources","level":1,"index":3,"id":"creating-project-scoped-resources_managing-odh"},{"parentId":null,"name":"Allocating additional resources to Open Data Hub users","level":1,"index":4,"id":"allocating-additional-resources-to-users_managing-odh"},{"parentId":null,"name":"Customizing component deployment resources","level":1,"index":5,"id":"customizing-component-deployment-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Overview of component resource customization","level":2,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Customizing component resources","level":2,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Disabling component resource customization","level":2,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":"customizing-component-deployment-resources_managing-resources","name":"Re-enabling component resource customization","level":2,"index":3,"id":"reenabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":6,"id":"enabling-accelerators"},{"parentId":"enabling-accelerators","name":"Enabling NVIDIA GPUs","level":2,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":"enabling-accelerators","name":"Intel Gaudi AI Accelerator integration","level":2,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":"intel-gaudi-ai-accelerator-integration_managing-odh","name":"Enabling Intel Gaudi AI accelerators","level":3,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_managing-odh"},{"parentId":"enabling-accelerators","name":"AMD GPU Integration","level":2,"index":2,"id":"amd-gpu-integration_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Verifying AMD GPU availability on your cluster","level":3,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Enabling AMD GPUs","level":3,"index":1,"id":"enabling-amd-gpus_managing-odh"},{"parentId":null,"name":"Managing workloads with Kueue","level":1,"index":7,"id":"managing-workloads-with-kueue"},{"parentId":"managing-workloads-with-kueue","name":"Overview of managing workloads with Kueue","level":2,"index":0,"id":"overview-of-managing-workloads-with-kueue_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue management states","level":3,"index":0,"id":"_kueue_management_states"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Queue enforcement for projects","level":3,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Restrictions for managing workloads with Kueue","level":3,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue workflow","level":3,"index":3,"id":"kueue-workflow_kueue"},{"parentId":"managing-workloads-with-kueue","name":"Configuring workload management with Kueue","level":2,"index":1,"id":"configuring-workload-management-with-kueue_kueue"},{"parentId":"configuring-workload-management-with-kueue_kueue","name":"Enabling Kueue in the dashboard","level":3,"index":0,"id":"enabling-kueue-in-the-dashboard_kueue"},{"parentId":"managing-workloads-with-kueue","name":"Troubleshooting common problems with Kueue","level":2,"index":2,"id":"troubleshooting-common-problems-with-Kueue_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"failed to call webhook\" error message for Kueue","level":3,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":3,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"local_queue provided does not exist\" error message","level":3,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"The pod provisioned by Kueue is terminated before the image is pulled","level":3,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"Additional resources","level":3,"index":4,"id":"_additional_resources"},{"parentId":"managing-workloads-with-kueue","name":"Migrating to the Red Hat build of Kueue Operator","level":2,"index":3,"id":"migrating-to-the-rhbok-operator_kueue"},{"parentId":null,"name":"Managing distributed workloads","level":1,"index":8,"id":"managing-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring quota management for distributed workloads","level":2,"index":0,"id":"configuring-quota-management-for-distributed-workloads_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Example Kueue resource configurations for distributed workloads","level":2,"index":1,"id":"ref-example-kueue-resource-configurations_managing-odh"},{"parentId":"ref-example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs without shared cohort","level":3,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":4,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":4,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":4,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":4,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"ref-example-kueue-resource-configurations_managing-odh","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":3,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":4,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":4,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":4,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":4,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Configuring a cluster for RDMA","level":2,"index":2,"id":"configuring-a-cluster-for-rdma_managing-odh"},{"parentId":"managing-distributed-workloads_managing-odh","name":"Troubleshooting common problems with distributed workloads for administrators","level":2,"index":3,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a suspended state","level":3,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster is in a failed state","level":3,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user&#8217;s Ray cluster does not start","level":3,"index":2,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"A user cannot create a Ray cluster or submit jobs","level":3,"index":3,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_managing-odh","name":"Additional resources","level":3,"index":4,"id":"_additional_resources_2"},{"parentId":null,"name":"Configuring a central authentication service for an external OIDC identity provider","level":1,"index":9,"id":"configuring-external-oidc-provider_managing-odh"},{"parentId":"configuring-external-oidc-provider_managing-odh","name":"About centralized authentication Gateway API","level":2,"index":0,"id":"about-centralized-auth-oidc_managing-odh"},{"parentId":"configuring-external-oidc-provider_managing-odh","name":"Configuring OpenID Connect (OIDC) authentication for Gateway API","level":2,"index":1,"id":"configuring-oidc-auth-gateway-api_managing-odh"},{"parentId":"configuring-oidc-auth-gateway-api_managing-odh","name":"Security considerations","level":3,"index":0,"id":"_security_considerations"},{"parentId":"configuring-external-oidc-provider_managing-odh","name":"Troubleshooting common problems with Gateway API configuration","level":2,"index":2,"id":"troubleshooting-common-problems-gateway-api_managing-odh"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"The <code>GatewayConfig</code> status shows as not ready","level":3,"index":0,"id":"_the_gatewayconfig_status_shows_as_not_ready"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"Authentication proxy fails to start","level":3,"index":1,"id":"_authentication_proxy_fails_to_start"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"The Gateway is inaccessible","level":3,"index":2,"id":"_the_gateway_is_inaccessible"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"The OIDC authentication fails","level":3,"index":3,"id":"_the_oidc_authentication_fails"},{"parentId":"troubleshooting-common-problems-gateway-api_managing-odh","name":"The dashboard is not accessible after authentication","level":3,"index":4,"id":"_the_dashboard_is_not_accessible_after_authentication"},{"parentId":null,"name":"Backing up data","level":1,"index":10,"id":"backing-up-data_data-mgmt"},{"parentId":"backing-up-data_data-mgmt","name":"Backing up storage data","level":2,"index":0,"id":"backing-up-storage-data_data-mgmt"},{"parentId":"backing-up-data_data-mgmt","name":"Backing up your cluster","level":2,"index":1,"id":"backing-up-your-cluster_data-mgmt"},{"parentId":null,"name":"Managing observability","level":1,"index":11,"id":"managing-observability_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Enabling the observability stack","level":2,"index":0,"id":"enabling-the-observability-stack_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Collecting metrics from user workloads","level":2,"index":1,"id":"collecting-metrics-from-user-workloads_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Exporting metrics to external observability tools","level":2,"index":2,"id":"exporting-metrics-to-external-observability-tools_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Viewing traces in external tracing platforms","level":2,"index":3,"id":"viewing-traces-in-external-tracing-platforms_managing-odh"},{"parentId":"managing-observability_managing-odh","name":"Accessing built-in alerts","level":2,"index":4,"id":"accessing-built-in-alerts_managing-odh"},{"parentId":null,"name":"Viewing logs and audit records","level":1,"index":12,"id":"viewing-logs-and-audit-records_managing-odh"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Configuring the Open Data Hub Operator logger","level":2,"index":0,"id":"configuring-the-operator-logger_managing-odh"},{"parentId":"configuring-the-operator-logger_managing-odh","name":"Viewing the Open Data Hub Operator logs","level":3,"index":0,"id":"_viewing_the_open_data_hub_operator_logs"},{"parentId":"viewing-logs-and-audit-records_managing-odh","name":"Viewing audit records","level":2,"index":1,"id":"viewing-audit-records_managing-odh"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/managing-resources/"},"sections":[{"parentId":null,"name":"Selecting Open Data Hub administrator and user groups","level":1,"index":0,"id":"selecting-admin-and-user-groups_managing-resources"},{"parentId":null,"name":"Customizing the dashboard","level":1,"index":1,"id":"customizing-the-dashboard"},{"parentId":"customizing-the-dashboard","name":"Editing the dashboard configuration","level":2,"index":0,"id":"editing-the-dashboard-configuration_dashboard"},{"parentId":"customizing-the-dashboard","name":"Dashboard configuration options","level":2,"index":1,"id":"ref-dashboard-configuration-options_dashboard"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":2,"id":"importing-a-custom-workbench-image_managing-resources"},{"parentId":null,"name":"Managing cluster PVC size","level":1,"index":3,"id":"managing-cluster-pvc-size"},{"parentId":"managing-cluster-pvc-size","name":"Configuring the default PVC size for your cluster","level":2,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":"managing-cluster-pvc-size","name":"Restoring the default PVC size for your cluster","level":2,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_managing-resources"},{"parentId":null,"name":"Managing connection types","level":1,"index":4,"id":"managing-connection-types"},{"parentId":"managing-connection-types","name":"Viewing connection types","level":2,"index":0,"id":"viewing-connection-types_managing-resources"},{"parentId":"managing-connection-types","name":"Creating a connection type","level":2,"index":1,"id":"creating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Duplicating a connection type","level":2,"index":2,"id":"duplicating-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Editing a connection type","level":2,"index":3,"id":"editing-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Enabling a connection type","level":2,"index":4,"id":"enabling-a-connection-type_managing-resources"},{"parentId":"managing-connection-types","name":"Deleting a connection type","level":2,"index":5,"id":"deleting-a-connection-type_managing-resources"},{"parentId":null,"name":"Managing storage classes","level":1,"index":5,"id":"managing-storage-classes"},{"parentId":"managing-storage-classes","name":"About persistent storage","level":2,"index":0,"id":"about-persistent-storage_managing-resources"},{"parentId":"about-persistent-storage_managing-resources","name":"Storage classes in Open Data Hub","level":3,"index":0,"id":"_storage_classes_in_open_data_hub"},{"parentId":"about-persistent-storage_managing-resources","name":"Access modes","level":3,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":4,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":"managing-storage-classes","name":"Configuring storage class settings","level":2,"index":1,"id":"configuring-storage-class-settings_managing-resources"},{"parentId":"managing-storage-classes","name":"Configuring the default storage class for your cluster","level":2,"index":2,"id":"configuring-the-default-storage-class-for-your-cluster_managing-resources"},{"parentId":"managing-storage-classes","name":"Overview of object storage endpoints","level":2,"index":3,"id":"overview-of-object-storage-endpoints_managing-resources"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"MinIO (On-Cluster)","level":3,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Amazon S3","level":3,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Other S3-Compatible Object Stores","level":3,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_managing-resources","name":"Verification and Troubleshooting","level":3,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Managing basic workbenches","level":1,"index":6,"id":"managing-basic-workbenches"},{"parentId":"managing-basic-workbenches","name":"Accessing the administration interface for basic workbenches","level":2,"index":0,"id":"accessing-the-administration-interface-for-basic-workbenches_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Starting basic workbenches owned by other users","level":2,"index":1,"id":"starting-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Accessing basic workbenches owned by other users","level":2,"index":2,"id":"accessing-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Stopping basic workbenches owned by other users","level":2,"index":3,"id":"stopping-basic-workbenches-owned-by-other-users_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Stopping idle workbenches","level":2,"index":4,"id":"stopping-idle-workbenches_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Adding workbench pod tolerations","level":2,"index":5,"id":"adding-workbench-pod-tolerations_managing-resources"},{"parentId":"managing-basic-workbenches","name":"Troubleshooting common problems in workbenches for administrators","level":2,"index":6,"id":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":3,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"A user&#8217;s workbench does not start","level":3,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_managing-resources","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":3,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-data-science-models/"},"sections":[{"parentId":null,"name":"Overview of model monitoring","level":1,"index":0,"id":"overview-of-model-monitoring_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for your model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling TrustyAI Integration with KServe RawDeployment","level":2,"index":4,"id":"enabling-trustyai-kserve-integration_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Uploading training data to TrustyAI","level":2,"index":1,"id":"uploading-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":2,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":3,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Using bias metrics","level":2,"index":3,"id":"using-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":4,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-by-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using drift metrics","level":2,"index":3,"id":"using-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using a drift metric in a credit card scenario","level":2,"index":4,"id":"using-a-drift-metric-in-a-credit-card-scenario_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":5,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Using explainers","level":2,"index":2,"id":"using-explainers_explainers"},{"parentId":null,"name":"Evaluating large language models","level":1,"index":6,"id":"evaluating-large-language-models_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"Setting up LM-Eval","level":2,"index":0,"id":"setting-up-lmeval_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"Enabling external resource access for LMEval jobs","level":2,"index":1,"id":"enabling-external-resource-access-for-lmeval-jobs_monitor"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_monitor","name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":3,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_monitor"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_monitor","name":"Updating LMEval job configuration using the web console","level":3,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job","level":2,"index":2,"id":"lmeval-evaluation-job_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval evaluation job properties","level":2,"index":3,"id":"lmeval-evaluation-job-properties_monitor"},{"parentId":"lmeval-evaluation-job-properties_monitor","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":3,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":"evaluating-large-language-models_monitor","name":"Performing model evaluations in the dashboard","level":2,"index":4,"id":"performing-model-evaluations-in-the-dashboard_monitor"},{"parentId":"evaluating-large-language-models_monitor","name":"LM-Eval scenarios","level":2,"index":5,"id":"lmeval-scenarios_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Accessing Hugging Face models with an environment variable token","level":3,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using a custom Unitxt card","level":3,"index":1,"id":"using-a-custom-unitxt-card_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using PVCs as storage","level":3,"index":2,"id":"using-pvcs-as-storage_monitor"},{"parentId":"using-pvcs-as-storage_monitor","name":"Managed PVCs","level":4,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_monitor","name":"Existing PVCs","level":4,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_monitor","name":"Using a KServe Inference Service","level":3,"index":3,"id":"using-a-kserve-inference-service_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Setting up LM-Eval S3 Support","level":3,"index":4,"id":"setting-up-lmeval-s3-support_monitor"},{"parentId":"lmeval-scenarios_monitor","name":"Using LLM-as-a-Judge metrics with LM-Eval","level":3,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_monitor"},{"parentId":null,"name":"Evaluating RAG systems with Ragas","level":1,"index":7,"id":"evaluating-rag-systems-with-ragas_monitor"},{"parentId":"evaluating-rag-systems-with-ragas_monitor","name":"About Ragas evaluation","level":2,"index":0,"id":"_about_ragas_evaluation"},{"parentId":"_about_ragas_evaluation","name":"Key Ragas metrics","level":3,"index":0,"id":"_key_ragas_metrics"},{"parentId":"_about_ragas_evaluation","name":"Use cases for Ragas in AI engineering workflows","level":3,"index":1,"id":"_use_cases_for_ragas_in_ai_engineering_workflows"},{"parentId":"_about_ragas_evaluation","name":"Ragas provider deployment modes","level":3,"index":2,"id":"_ragas_provider_deployment_modes"},{"parentId":"evaluating-rag-systems-with-ragas_monitor","name":"Setting up the Ragas inline provider for development","level":2,"index":1,"id":"setting-up-ragas-inline-provider_monitor"},{"parentId":"evaluating-rag-systems-with-ragas_monitor","name":"Configuring the Ragas remote provider for production","level":2,"index":2,"id":"configuring-ragas-remote-provider-for-production_monitor"},{"parentId":"evaluating-rag-systems-with-ragas_monitor","name":"Evaluating RAG system quality with Ragas metrics","level":2,"index":3,"id":"evaluating-rag-system-quality-with-ragas_monitor"},{"parentId":null,"name":"Using llama stack with TrustyAI","level":1,"index":8,"id":"using-llama-stack-with-trustyai_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI","level":2,"index":0,"id":"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Running custom evaluations with LM-Eval and Llama Stack","level":2,"index":1,"id":"running-custom-evaluations-with-LMEval-and-llama-stack_monitor"},{"parentId":"using-llama-stack-with-trustyai_monitor","name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":2,"index":2,"id":"detecting-pii-by-using-guardrails-with-llama-stack_monitor"},{"parentId":null,"name":"Bias monitoring tutorial - Gender bias example","level":1,"index":9,"id":"bias-monitoring-tutorial_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Introduction","level":2,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":3,"index":0,"id":"_about_the_example_models"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Setting up your environment","level":2,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":3,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":3,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":3,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":3,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":3,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":3,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Deploying models","level":2,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Sending training data to the models","level":2,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Labeling data fields","level":2,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Checking model fairness","level":2,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling a fairness metric request","level":2,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling an identity metric request","level":2,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Simulating real world data","level":2,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Reviewing the results","level":2,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":3,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":3,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/monitoring-your-ai-systems/"},"sections":[{"parentId":null,"name":"Overview of monitoring your AI systems","level":1,"index":0,"id":"overview-of-monitoring-your-ai-systems_monitor"},{"parentId":null,"name":"Configuring TrustyAI","level":1,"index":1,"id":"configuring-trustyai_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring monitoring for your model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling the TrustyAI component","level":2,"index":1,"id":"enabling-trustyai-component_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Configuring TrustyAI with a database","level":2,"index":2,"id":"configuring-trustyai-with-a-database_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Installing the TrustyAI service for a project","level":2,"index":3,"id":"installing-trustyai-service_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the dashboard","level":3,"index":0,"id":"installing-trustyai-service-using-dashboard_monitor"},{"parentId":"installing-trustyai-service_monitor","name":"Installing the TrustyAI service by using the CLI","level":3,"index":1,"id":"installing-trustyai-service-using-cli_monitor"},{"parentId":"configuring-trustyai_monitor","name":"Enabling TrustyAI Integration with KServe RawDeployment","level":2,"index":4,"id":"enabling-trustyai-kserve-integration_monitor"},{"parentId":null,"name":"Setting up TrustyAI for your project","level":1,"index":2,"id":"setting-up-trustyai-for-your-project_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Authenticating the TrustyAI service","level":2,"index":0,"id":"authenticating-trustyai-service_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Uploading training data to TrustyAI","level":2,"index":1,"id":"uploading-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Sending training data to TrustyAI","level":2,"index":2,"id":"sending-training-data-to-trustyai_monitor"},{"parentId":"setting-up-trustyai-for-your-project_monitor","name":"Labeling data fields","level":2,"index":3,"id":"labeling-data-fields_monitor"},{"parentId":null,"name":"Monitoring model bias","level":1,"index":3,"id":"monitoring-model-bias_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Creating a bias metric","level":2,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":3,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":3,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":3,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Deleting a bias metric","level":2,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":3,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":3,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Viewing bias metrics for a model","level":2,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":"monitoring-model-bias_bias-monitoring","name":"Using bias metrics","level":2,"index":3,"id":"using-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Bias monitoring tutorial - Gender bias example","level":1,"index":4,"id":"bias-monitoring-tutorial_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Introduction","level":2,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":3,"index":0,"id":"_about_the_example_models"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Setting up your environment","level":2,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":3,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":3,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":3,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":3,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":3,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":3,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Deploying models","level":2,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Sending training data to the models","level":2,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Labeling data fields","level":2,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Checking model fairness","level":2,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling a fairness metric request","level":2,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Scheduling an identity metric request","level":2,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Simulating real world data","level":2,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":"bias-monitoring-tutorial_bias-tutorial","name":"Reviewing the results","level":2,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":3,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":3,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"},{"parentId":null,"name":"Monitoring data drift","level":1,"index":5,"id":"monitoring-data-drift_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Creating a drift metric","level":2,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":3,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Deleting a drift metric by using the CLI","level":2,"index":1,"id":"deleting-a-drift-metric-by-using-cli_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Viewing drift metrics for a model","level":2,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using drift metrics","level":2,"index":3,"id":"using-drift-metrics_drift-monitoring"},{"parentId":"monitoring-data-drift_drift-monitoring","name":"Using a drift metric in a credit card scenario","level":2,"index":4,"id":"using-a-drift-metric-in-a-credit-card-scenario_drift-monitoring"},{"parentId":null,"name":"Using explainability","level":1,"index":6,"id":"using-explainability_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a LIME explanation","level":2,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":3,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Requesting a SHAP explanation","level":2,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":3,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":"using-explainability_explainers","name":"Using explainers","level":2,"index":2,"id":"using-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/upgrading-open-data-hub/"},"sections":[{"parentId":null,"name":"Overview of upgrading Open Data Hub","level":1,"index":0,"id":"overview-of-upgrading-odh_upgrade"},{"parentId":null,"name":"Upgrading Open Data Hub version 2.0 to version 2.2 or later","level":1,"index":1,"id":"upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Requirements for upgrading Open Data Hub version 2","level":2,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":"upgrading-odh-v2_upgradev2","name":"Upgrading the Open Data Hub Operator","level":2,"index":1,"id":"upgrading-the-odh-operator_upgradev2"},{"parentId":null,"name":"Upgrading Open Data Hub version 1 to version 2","level":1,"index":2,"id":"upgrading-odh-v1-to-v2_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Requirements for upgrading Open Data Hub version 1","level":2,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":"upgrading-odh-v1-to-v2_upgradev1","name":"Upgrading the Open Data Hub Operator","level":2,"index":1,"id":"upgrading-the-odh-operator_upgradev1"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":3,"id":"installing-odh-components_upgrade"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-in-your-data-science-ide/"},"sections":[{"parentId":null,"name":"Accessing your workbench IDE","level":1,"index":0,"id":"accessing-your-workbench-ide_ide"},{"parentId":null,"name":"Working in JupyterLab","level":1,"index":1,"id":"_working_in_jupyterlab"},{"parentId":"_working_in_jupyterlab","name":"Creating and importing Jupyter notebooks","level":2,"index":0,"id":"creating-and-importing-jupyter-notebooks_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Creating a Jupyter notebook","level":3,"index":0,"id":"creating-a-jupyter-notebook_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":3,"index":2,"id":"deleting-files-in-trash-directory_ide"},{"parentId":"deleting-files-in-trash-directory_ide","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":4,"index":0,"id":"emptying-trash-directory_ide"},{"parentId":"creating-and-importing-jupyter-notebooks_ide","name":"Additional resources","level":3,"index":3,"id":"_additional_resources"},{"parentId":"_working_in_jupyterlab","name":"Collaborating on Jupyter notebooks by using Git","level":2,"index":1,"id":"collaborating-on-jupyter-notebooks-by-using-git_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_ide","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_ide"},{"parentId":"_working_in_jupyterlab","name":"Managing Python packages","level":2,"index":2,"id":"managing-python-packages_ide"},{"parentId":"managing-python-packages_ide","name":"Viewing Python packages installed on your workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_ide"},{"parentId":"managing-python-packages_ide","name":"Installing Python packages on your workbench","level":3,"index":1,"id":"installing-python-packages-on-your-workbench_ide"},{"parentId":"_working_in_jupyterlab","name":"Troubleshooting common problems in workbenches for users","level":2,"index":3,"id":"troubleshooting-common-problems-in-workbenches-for-users_ide"},{"parentId":null,"name":"Working in code-server","level":1,"index":2,"id":"_working_in_code_server"},{"parentId":"_working_in_code_server","name":"Creating code-server workbenches","level":2,"index":0,"id":"creating-code-server-workbenches_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Creating a workbench","level":3,"index":0,"id":"creating-a-project-workbench_ide"},{"parentId":"creating-code-server-workbenches_ide","name":"Uploading an existing notebook file to code-server from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_ide"},{"parentId":"_working_in_code_server","name":"Collaborating on workbenches in code-server by using Git","level":2,"index":1,"id":"collaborating-on-workbenches-in-code-server-by-using-git_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file from a Git repository by using code-server","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Updating your project in code-server with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_ide"},{"parentId":"collaborating-on-workbenches-in-code-server-by-using-git_ide","name":"Pushing project changes in code-server to a Git repository","level":3,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_ide"},{"parentId":"_working_in_code_server","name":"Managing Python packages in code-server","level":2,"index":2,"id":"managing-python-packages-in-code-server_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Viewing Python packages installed on your code-server workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_ide"},{"parentId":"managing-python-packages-in-code-server_ide","name":"Installing Python packages on your code-server workbench","level":3,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_ide"},{"parentId":"_working_in_code_server","name":"Installing extensions with code-server","level":2,"index":3,"id":"installing-extensions-with-code-server_ide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-on-projects/"},"sections":[{"parentId":null,"name":"Using projects","level":1,"index":0,"id":"using-projects_projects"},{"parentId":"using-projects_projects","name":"Creating a project","level":2,"index":0,"id":"creating-a-project_projects"},{"parentId":"using-projects_projects","name":"Updating a project","level":2,"index":1,"id":"updating-a-project_projects"},{"parentId":"using-projects_projects","name":"Deleting a project","level":2,"index":2,"id":"deleting-a-project_projects"},{"parentId":null,"name":"Using project workbenches","level":1,"index":1,"id":"using-project-workbenches_projects"},{"parentId":"using-project-workbenches_projects","name":"Creating a workbench and selecting an IDE","level":2,"index":0,"id":"creating-a-workbench-select-ide_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"About workbench images","level":3,"index":0,"id":"about-workbench-images_projects"},{"parentId":"creating-a-workbench-select-ide_projects","name":"Creating a workbench","level":3,"index":1,"id":"creating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Starting a workbench","level":2,"index":1,"id":"starting-a-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Updating a project workbench","level":2,"index":2,"id":"updating-a-project-workbench_projects"},{"parentId":"using-project-workbenches_projects","name":"Deleting a workbench from a project","level":2,"index":3,"id":"deleting-a-workbench-from-a-project_projects"},{"parentId":null,"name":"Using connections","level":1,"index":2,"id":"using-connections_projects"},{"parentId":"using-connections_projects","name":"Adding a connection to your project","level":2,"index":0,"id":"adding-a-connection-to-your-project_projects"},{"parentId":"using-connections_projects","name":"Updating a connection","level":2,"index":1,"id":"updating-a-connection_projects"},{"parentId":"using-connections_projects","name":"Deleting a connection","level":2,"index":2,"id":"deleting-a-connection_projects"},{"parentId":"using-connections_projects","name":"Using the connections API","level":2,"index":3,"id":"using-connections-api_projects"},{"parentId":"using-connections-api_projects","name":"Namespace isolation in connections API","level":3,"index":0,"id":"_namespace_isolation_in_connections_api"},{"parentId":"using-connections-api_projects","name":"Role-based access control (RBAC) requirements in connections API","level":3,"index":1,"id":"_role_based_access_control_rbac_requirements_in_connections_api"},{"parentId":"using-connections-api_projects","name":"Validation scope","level":3,"index":2,"id":"_validation_scope"},{"parentId":"using-connections-api_projects","name":"Using connection annotations based on workload type","level":3,"index":3,"id":"_using_connection_annotations_based_on_workload_type"},{"parentId":"using-connections-api_projects","name":"Creating an Amazon S3-compatible connection type using the connections API","level":3,"index":4,"id":"creating-s3-compatible-connection-type-api_projects"},{"parentId":"creating-s3-compatible-connection-type-api_projects","name":"Using an Amazon S3 connection with <code>InferenceService</code> custom resource","level":4,"index":0,"id":"_using_an_amazon_s3_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-s3-compatible-connection-type-api_projects","name":"Using an Amazon S3 connection with <code>LLMInferenceService</code> custom resource","level":4,"index":1,"id":"_using_an_amazon_s3_connection_with_llminferenceservice_custom_resource"},{"parentId":"using-connections-api_projects","name":"Creating a URI-compatible connection type using the connections API","level":3,"index":5,"id":"creating-uri-compatible-connection-type-api_projects"},{"parentId":"creating-uri-compatible-connection-type-api_projects","name":"Using a URI connection with <code>InferenceService</code> custom resource","level":4,"index":0,"id":"_using_a_uri_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-uri-compatible-connection-type-api_projects","name":"Using a URI connection with <code>LLMInferenceService</code> custom resource","level":4,"index":1,"id":"_using_a_uri_connection_with_llminferenceservice_custom_resource"},{"parentId":"using-connections-api_projects","name":"Creating an OCI-compatible connection type using the connections API","level":3,"index":6,"id":"creating-oci-compatible-connection-type-api_projects"},{"parentId":"creating-oci-compatible-connection-type-api_projects","name":"Using an OCI connection with <code>InferenceService</code> custom resource","level":4,"index":0,"id":"_using_an_oci_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-oci-compatible-connection-type-api_projects","name":"Using an OCI connection with <code>LLMInferenceService</code> custom resource","level":4,"index":1,"id":"_using_an_oci_connection_with_llminferenceservice_custom_resource"},{"parentId":null,"name":"Configuring cluster storage","level":1,"index":3,"id":"configuring-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"About persistent storage","level":2,"index":0,"id":"about-persistent-storage_projects"},{"parentId":"about-persistent-storage_projects","name":"Storage classes in Open Data Hub","level":3,"index":0,"id":"_storage_classes_in_open_data_hub"},{"parentId":"about-persistent-storage_projects","name":"Access modes","level":3,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":4,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":"configuring-cluster-storage_projects","name":"Adding cluster storage to your project","level":2,"index":1,"id":"adding-cluster-storage-to-your-project_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Updating cluster storage","level":2,"index":2,"id":"updating-cluster-storage_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Changing the storage class for an existing cluster storage instance","level":2,"index":3,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_projects"},{"parentId":"configuring-cluster-storage_projects","name":"Deleting cluster storage from a project","level":2,"index":4,"id":"deleting-cluster-storage-from-a-project_projects"},{"parentId":null,"name":"Managing access to projects","level":1,"index":4,"id":"managing-access-to-projects_projects"},{"parentId":"managing-access-to-projects_projects","name":"Granting access to a project","level":2,"index":0,"id":"granting-access-to-a-project_projects"},{"parentId":"managing-access-to-projects_projects","name":"Updating access to a project","level":2,"index":1,"id":"updating-access-to-a-project_projects"},{"parentId":"managing-access-to-projects_projects","name":"Removing access to a project","level":2,"index":2,"id":"removing-access-to-a-project_projects"},{"parentId":null,"name":"Creating project-scoped resources for your project","level":1,"index":5,"id":"creating-project-scoped-resources-for-your-project_projects"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-accelerators/"},"sections":[{"parentId":null,"name":"Overview of accelerators","level":1,"index":0,"id":"overview-of-accelerators_accelerators"},{"parentId":null,"name":"Enabling accelerators","level":1,"index":1,"id":"enabling-accelerators_accelerators"},{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":2,"id":"enabling-nvidia-gpus_accelerators"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":3,"id":"intel-gaudi-ai-accelerator-integration_accelerators"},{"parentId":null,"name":"AMD GPU Integration","level":1,"index":4,"id":"amd-gpu-integration_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Verifying AMD GPU availability on your cluster","level":2,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_accelerators"},{"parentId":"amd-gpu-integration_accelerators","name":"Enabling AMD GPUs","level":2,"index":1,"id":"enabling-amd-gpus_accelerators"},{"parentId":null,"name":"IBM Spyre integration","level":1,"index":5,"id":"ibm-spyre-integration_accelerators"},{"parentId":null,"name":"Working with hardware profiles","level":1,"index":6,"id":"working-with-hardware-profiles_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Creating a hardware profile","level":2,"index":0,"id":"creating-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Updating a hardware profile","level":2,"index":1,"id":"updating-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Deleting a hardware profile","level":2,"index":2,"id":"deleting-a-hardware-profile_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Configuring a recommended accelerator for workbench images","level":2,"index":3,"id":"configuring-a-recommended-accelerator-for-workbench-images_accelerators"},{"parentId":"working-with-hardware-profiles_accelerators","name":"Configuring a recommended accelerator for serving runtimes","level":2,"index":4,"id":"configuring-a-recommended-accelerator-for-serving-runtimes_accelerators"},{"parentId":null,"name":"About GPU time slicing","level":1,"index":7,"id":"about-gpu-time-slicing_accelerators"},{"parentId":null,"name":"Enabling GPU time slicing","level":1,"index":8,"id":"enabling-gpu-time-slicing_accelerators"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-connected-applications/"},"sections":[{"parentId":null,"name":"Viewing applications that are connected to Open Data Hub","level":1,"index":0,"id":"viewing-connected-applications_connected-apps"},{"parentId":null,"name":"Enabling applications that are connected to Open Data Hub","level":1,"index":1,"id":"enabling-applications-connected_connected-apps"},{"parentId":null,"name":"Removing disabled applications from the dashboard","level":1,"index":2,"id":"removing-disabled-applications_connected-apps"},{"parentId":null,"name":"Using basic workbenches","level":1,"index":3,"id":"using-basic-workbenches_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Starting a basic workbench","level":2,"index":0,"id":"starting-a-basic-workbench_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Creating and importing Jupyter notebooks","level":2,"index":1,"id":"creating-and-importing-jupyter-notebooks_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Creating a Jupyter notebook","level":3,"index":0,"id":"creating-a-jupyter-notebook_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Uploading an existing notebook file to JupyterLab from local storage","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":3,"index":2,"id":"deleting-files-in-trash-directory_connected-apps"},{"parentId":"deleting-files-in-trash-directory_connected-apps","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":4,"index":0,"id":"emptying-trash-directory_connected-apps"},{"parentId":"creating-and-importing-jupyter-notebooks_connected-apps","name":"Additional resources","level":3,"index":3,"id":"_additional_resources"},{"parentId":"using-basic-workbenches_connected-apps","name":"Collaborating on Jupyter notebooks by using Git","level":2,"index":2,"id":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":3,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":3,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Updating your project with changes from a remote Git repository","level":3,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_connected-apps"},{"parentId":"collaborating-on-jupyter-notebooks-by-using-git_connected-apps","name":"Pushing project changes to a Git repository","level":3,"index":3,"id":"pushing-project-changes-to-a-git-repository_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Managing Python packages","level":2,"index":3,"id":"managing-python-packages_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Viewing Python packages installed on your workbench","level":3,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_connected-apps"},{"parentId":"managing-python-packages_connected-apps","name":"Installing Python packages on your workbench","level":3,"index":1,"id":"installing-python-packages-on-your-workbench_connected-apps"},{"parentId":"using-basic-workbenches_connected-apps","name":"Updating workbench settings by restarting your workbench","level":2,"index":4,"id":"updating-workbench-settings-by-restarting-your-workbench_connected-apps"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-ai-pipelines/"},"sections":[{"parentId":null,"name":"Managing AI pipelines","level":1,"index":0,"id":"managing-ai-pipelines_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Configuring a pipeline server","level":2,"index":0,"id":"configuring-a-pipeline-server_ai-pipelines"},{"parentId":"configuring-a-pipeline-server_ai-pipelines","name":"Configuring a pipeline server with an external Amazon RDS database","level":3,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Defining a pipeline","level":2,"index":1,"id":"defining-a-pipeline_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Compiling the pipeline YAML with the Kubeflow Pipelines SDK","level":3,"index":0,"id":"compiling-the-pipeline-yaml-with-kfp-sdk_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Compiling Kubernetes-native manifests with the Kubeflow Pipelines SDK","level":3,"index":1,"id":"compiling-kubernetes-native-manifests-with-kfp-sdk_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Authenticating the Kubeflow Pipelines SDK with a pipeline server","level":3,"index":2,"id":"authenticating-kfp-sdk-with-pipeline-server_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Defining a pipeline by using the Kubernetes API","level":3,"index":3,"id":"defining-a-pipeline-by-using-the-kubernetes-api_ai-pipelines"},{"parentId":"defining-a-pipeline_ai-pipelines","name":"Migrating pipelines from database to Kubernetes API storage","level":3,"index":4,"id":"migrating-pipelines-from-database-to-kubernetes-api_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Importing a pipeline","level":2,"index":2,"id":"importing-a-pipeline_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Deleting a pipeline","level":2,"index":3,"id":"deleting-a-pipeline_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Deleting a pipeline server","level":2,"index":4,"id":"deleting-a-pipeline-server_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Viewing the details of a pipeline server","level":2,"index":5,"id":"viewing-the-details-of-a-pipeline-server_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Viewing existing pipelines","level":2,"index":6,"id":"viewing-existing-pipelines_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Overview of pipeline versions","level":2,"index":7,"id":"overview-of-pipeline-versions_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Uploading a pipeline version","level":2,"index":8,"id":"uploading-a-pipeline-version_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Deleting a pipeline version","level":2,"index":9,"id":"deleting-a-pipeline-version_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Viewing the details of a pipeline version","level":2,"index":10,"id":"viewing-the-details-of-a-pipeline-version_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Downloading a pipeline version","level":2,"index":11,"id":"downloading-a-pipeline-version_ai-pipelines"},{"parentId":"managing-ai-pipelines_ai-pipelines","name":"Overview of pipelines caching","level":2,"index":12,"id":"overview-of-pipelines-caching_ai-pipelines"},{"parentId":"overview-of-pipelines-caching_ai-pipelines","name":"Caching criteria","level":3,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-pipelines-caching_ai-pipelines","name":"Viewing cached steps in the Open Data Hub user interface","level":3,"index":1,"id":"_viewing_cached_steps_in_the_open_data_hub_user_interface"},{"parentId":"overview-of-pipelines-caching_ai-pipelines","name":"Controlling caching in pipelines","level":3,"index":2,"id":"controlling-caching-in-pipelines_ai-pipelines"},{"parentId":"controlling-caching-in-pipelines_ai-pipelines","name":"Disabling caching for individual tasks","level":4,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"controlling-caching-in-pipelines_ai-pipelines","name":"Disabling caching for a pipeline at submit time","level":4,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":"controlling-caching-in-pipelines_ai-pipelines","name":"Disabling caching for a pipeline at compile time","level":4,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":"controlling-caching-in-pipelines_ai-pipelines","name":"Disabling caching for all pipelines (pipeline server)","level":4,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"},{"parentId":null,"name":"Managing pipeline experiments","level":1,"index":1,"id":"managing-pipeline-experiments_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Overview of pipeline experiments","level":2,"index":0,"id":"overview-of-pipeline-experiments_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Creating a pipeline experiment","level":2,"index":1,"id":"creating-a-pipeline-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Archiving a pipeline experiment","level":2,"index":2,"id":"archiving-a-pipeline-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Deleting an archived pipeline experiment","level":2,"index":3,"id":"deleting-an-archived-pipeline-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Restoring an archived pipeline experiment","level":2,"index":4,"id":"restoring-an-archived-pipeline-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Viewing pipeline task executions","level":2,"index":5,"id":"viewing-pipeline-task-executions_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Viewing pipeline artifacts","level":2,"index":6,"id":"viewing-pipeline-artifacts_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Comparing runs in an experiment","level":2,"index":7,"id":"comparing-runs-in-an-experiment_ai-pipelines"},{"parentId":"managing-pipeline-experiments_ai-pipelines","name":"Comparing runs in different experiments","level":2,"index":8,"id":"comparing-runs-in-different-experiments_ai-pipelines"},{"parentId":null,"name":"Managing pipeline runs","level":1,"index":2,"id":"managing-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Overview of pipeline runs","level":2,"index":0,"id":"overview-of-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Storing data with pipelines","level":2,"index":1,"id":"storing-data-with-pipelines_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Understanding pipeline run workspaces","level":2,"index":2,"id":"configuring-pipeline-run-workspaces_ai-pipelines"},{"parentId":"configuring-pipeline-run-workspaces_ai-pipelines","name":"Configuring default workspace PVC settings in DSPA","level":3,"index":0,"id":"configuring-default-workspace-pvc-settings-in-dspa_ai-pipelines"},{"parentId":"configuring-pipeline-run-workspaces_ai-pipelines","name":"Adding external artifacts to pipeline run workspaces","level":3,"index":1,"id":"adding-external-artifacts-to-pipeline-run-workspaces_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Viewing active pipeline runs","level":2,"index":3,"id":"viewing-active-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Executing a pipeline run","level":2,"index":4,"id":"executing-a-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Stopping an active pipeline run","level":2,"index":5,"id":"stopping-an-active-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Duplicating an active pipeline run","level":2,"index":6,"id":"duplicating-an-active-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Viewing scheduled pipeline runs","level":2,"index":7,"id":"viewing-scheduled-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Scheduling a pipeline run using a cron job","level":2,"index":8,"id":"scheduling-a-pipeline-run-using-a-cron-job_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Scheduling a pipeline run","level":2,"index":9,"id":"scheduling-a-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Duplicating a scheduled pipeline run","level":2,"index":10,"id":"duplicating-a-scheduled-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Deleting a scheduled pipeline run","level":2,"index":11,"id":"deleting-a-scheduled-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Viewing the details of a pipeline run","level":2,"index":12,"id":"viewing-the-details-of-a-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Viewing archived pipeline runs","level":2,"index":13,"id":"viewing-archived-pipeline-runs_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Archiving a pipeline run","level":2,"index":14,"id":"archiving-a-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Restoring an archived pipeline run","level":2,"index":15,"id":"restoring-an-archived-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Deleting an archived pipeline run","level":2,"index":16,"id":"deleting-an-archived-pipeline-run_ai-pipelines"},{"parentId":"managing-pipeline-runs_ai-pipelines","name":"Duplicating an archived pipeline run","level":2,"index":17,"id":"duplicating-an-archived-pipeline-run_ai-pipelines"},{"parentId":null,"name":"Working with pipeline logs","level":1,"index":3,"id":"working-with-pipeline-logs_ai-pipelines"},{"parentId":"working-with-pipeline-logs_ai-pipelines","name":"About pipeline logs","level":2,"index":0,"id":"about-pipeline-logs_ai-pipelines"},{"parentId":"working-with-pipeline-logs_ai-pipelines","name":"Viewing pipeline step logs","level":2,"index":1,"id":"viewing-pipeline-step-logs_ai-pipelines"},{"parentId":"working-with-pipeline-logs_ai-pipelines","name":"Downloading pipeline step logs","level":2,"index":2,"id":"downloading-pipeline-step-logs_ai-pipelines"},{"parentId":null,"name":"Working with pipelines in JupyterLab","level":1,"index":4,"id":"working-with-pipelines-in-jupyterlab_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Overview of pipelines in JupyterLab","level":2,"index":0,"id":"overview-of-pipelines-in-jupyterlab_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Accessing the pipeline editor","level":2,"index":1,"id":"accessing-the-pipeline-editor_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Disabling node caching in Elyra","level":2,"index":2,"id":"disabling-node-caching-in-elyra_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Creating a runtime configuration","level":2,"index":3,"id":"creating-a-runtime-configuration_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Updating a runtime configuration","level":2,"index":4,"id":"updating-a-runtime-configuration_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Deleting a runtime configuration","level":2,"index":5,"id":"deleting-a-runtime-configuration_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Duplicating a runtime configuration","level":2,"index":6,"id":"duplicating-a-runtime-configuration_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Running a pipeline in JupyterLab","level":2,"index":7,"id":"running-a-pipeline-in-jupyterlab_ai-pipelines"},{"parentId":"working-with-pipelines-in-jupyterlab_ai-pipelines","name":"Exporting a pipeline in JupyterLab","level":2,"index":8,"id":"exporting-a-pipeline-in-jupyterlab_ai-pipelines"},{"parentId":null,"name":"Troubleshooting DSPA component errors","level":1,"index":5,"id":"troubleshooting-dspa-component-errors_ai-pipelines"},{"parentId":"troubleshooting-dspa-component-errors_ai-pipelines","name":"Common errors across DSPA components","level":2,"index":0,"id":"_common_errors_across_dspa_components"},{"parentId":null,"name":"Additional resources","level":1,"index":6,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":1,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-distributed-workloads/"},"sections":[{"parentId":null,"name":"Overview of distributed workloads","level":1,"index":0,"id":"overview-of-distributed-workloads_distributed-workloads"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Distributed workloads infrastructure","level":2,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":"overview-of-distributed-workloads_distributed-workloads","name":"Types of distributed workloads","level":2,"index":1,"id":"_types_of_distributed_workloads"},{"parentId":null,"name":"Preparing the distributed training environment","level":1,"index":1,"id":"preparing-the-distributed-training-environment_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Creating a workbench for distributed training","level":2,"index":0,"id":"creating-a-workbench-for-distributed-training_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Using the cluster server and token to authenticate","level":2,"index":1,"id":"using-the-cluster-server-and-token-to-authenticate_distributed-workloads"},{"parentId":"preparing-the-distributed-training-environment_distributed-workloads","name":"Managing custom training images","level":2,"index":2,"id":"managing-custom-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"About base training images","level":3,"index":0,"id":"about-base-training-images_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Creating a custom training image","level":3,"index":1,"id":"creating-a-custom-training-image_distributed-workloads"},{"parentId":"managing-custom-training-images_distributed-workloads","name":"Pushing an image to the integrated OpenShift image registry","level":3,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_distributed-workloads"},{"parentId":null,"name":"Running Ray-based distributed workloads","level":1,"index":2,"id":"running-ray-based-distributed-workloads_distributed-workloads"},{"parentId":"running-ray-based-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from Jupyter notebooks","level":2,"index":0,"id":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Downloading the demo Jupyter notebooks from the CodeFlare SDK","level":3,"index":0,"id":"downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Running the demo Jupyter notebooks from the CodeFlare SDK","level":3,"index":1,"id":"running-the-demo-jupyter-notebooks-from-the-codeflare-sdk_distributed-workloads"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_distributed-workloads","name":"Managing Ray clusters from within a Jupyter notebook","level":3,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_distributed-workloads"},{"parentId":"running-ray-based-distributed-workloads_distributed-workloads","name":"Running distributed data science workloads from AI pipelines","level":2,"index":1,"id":"running-distributed-data-science-workloads-from-ai-pipelines_distributed-workloads"},{"parentId":null,"name":"Running Training Operator-based distributed training workloads","level":1,"index":3,"id":"running-kfto-based-distributed-training-workloads_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Using the Kubeflow Training Operator to run distributed training workloads","level":2,"index":0,"id":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":3,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorchJob resource","level":3,"index":1,"id":"creating-a-kfto-pytorchjob-resource_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":3,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorch training scripts","level":3,"index":3,"id":"example-kfto-pytorch-training-scripts_distributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: NCCL","level":4,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccldistributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: DDP","level":4,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_distributed-workloads"},{"parentId":"example-kfto-pytorch-training-scripts_distributed-workloads","name":"Example Training Operator PyTorch training script: FSDP","level":4,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Dockerfile for a Training Operator PyTorch training script","level":3,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_distributed-workloads"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorchJob resource for multi-node training","level":3,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Using the Training Operator SDK to run distributed training workloads","level":2,"index":1,"id":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"Configuring a training job by using the Training Operator SDK","level":3,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"Running a training job by using the Training Operator SDK","level":3,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_distributed-workloads"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_distributed-workloads","name":"TrainingClient API: Job-related methods","level":3,"index":2,"id":"ref-trainingclient-api-job-related-methods_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Fine-tuning a model by using Kubeflow Training","level":2,"index":2,"id":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Configuring the fine-tuning job","level":3,"index":0,"id":"configuring-the-fine-tuning-job_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Running the fine-tuning job","level":3,"index":1,"id":"running-the-fine-tuning-job_distributed-workloads"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_distributed-workloads","name":"Deleting the fine-tuning job","level":3,"index":2,"id":"deleting-the-fine-tuning-job_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Creating a multi-node PyTorch training job with RDMA","level":2,"index":3,"id":"creating-a-multi-node-pytorch-training-job-with-rdma_distributed-workloads"},{"parentId":"running-kfto-based-distributed-training-workloads_distributed-workloads","name":"Example Training Operator PyTorchJob resource configured to run with RDMA","level":2,"index":4,"id":"ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma_distributed-workloads"},{"parentId":null,"name":"Monitoring distributed workloads","level":1,"index":4,"id":"monitoring-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing project metrics for distributed workloads","level":2,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing the status of distributed workloads","level":2,"index":1,"id":"viewing-the-status-of-distributed-workloads_distributed-workloads"},{"parentId":"monitoring-distributed-workloads_distributed-workloads","name":"Viewing Kueue alerts for distributed workloads","level":2,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_distributed-workloads"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for users","level":1,"index":5,"id":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a suspended state","level":2,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster is in a failed state","level":2,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"failed to call webhook\" error message for Kueue","level":2,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My Ray cluster does not start","level":2,"index":3,"id":"_my_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"Default Local Queue not found\" error message","level":2,"index":4,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I see a \"local_queue provided does not exist\" error message","level":2,"index":5,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"I cannot create a Ray cluster or submit jobs","level":2,"index":6,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"My pod provisioned by Kueue is terminated before my image is pulled","level":2,"index":7,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads","name":"Additional resources","level":2,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-llama-stack/"},"sections":[{"parentId":null,"name":"Overview of Llama Stack","level":1,"index":0,"id":"overview-of-llama-stack_rag"},{"parentId":"overview-of-llama-stack_rag","name":"OpenAI compatibility for RAG APIs in Llama Stack","level":2,"index":0,"id":"openai-compatibility-for-rag-apis-in-llama-stack_rag"},{"parentId":"overview-of-llama-stack_rag","name":"OpenAI-compatible APIs in Llama Stack","level":2,"index":1,"id":"openai-compatible-apis-in-Llama-Stack_rag"},{"parentId":"openai-compatible-apis-in-Llama-Stack_rag","name":"Supported OpenAI-compatible APIs in Open Data Hub","level":3,"index":0,"id":"_supported_openai_compatible_apis_in_open_data_hub"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Chat Completions API","level":4,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Completions API","level":4,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Embeddings API","level":4,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Files API","level":4,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Vector Stores API","level":4,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Vector Store Files API","level":4,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Models API","level":4,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_open_data_hub","name":"Responses API","level":4,"index":7,"id":"_responses_api"},{"parentId":null,"name":"Activating the Llama Stack Operator","level":1,"index":1,"id":"activating-the-llama-stack-operator_rag"},{"parentId":null,"name":"Deploying a RAG stack in a project","level":1,"index":2,"id":"deploying-a-rag-stack-in-a-project_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Overview of RAG","level":2,"index":0,"id":"overview-of-rag_rag"},{"parentId":"overview-of-rag_rag","name":"Audience for RAG","level":3,"index":0,"id":"_audience_for_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Overview of vector databases","level":2,"index":1,"id":"overview-of-vector-databases_rag"},{"parentId":"overview-of-vector-databases_rag","name":"Overview of Milvus vector databases","level":3,"index":0,"id":"overview-of-milvus-vector-databases_rag"},{"parentId":"overview-of-vector-databases_rag","name":"Overview of FAISS vector databases","level":3,"index":1,"id":"overview-of-faiss-vector-databases_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Deploying a Llama model with KServe","level":2,"index":2,"id":"Deploying-a-llama-model-with-kserve_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Testing your vLLM model endpoints","level":2,"index":3,"id":"testing-your-vllm-model-endpoints_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Deploying a remote Milvus vector database","level":2,"index":4,"id":"deploying-a-remote-milvus-vector-database_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Deploying a LlamaStackDistribution instance","level":2,"index":5,"id":"deploying-a-llamastackdistribution-instance_rag"},{"parentId":"deploying-a-llamastackdistribution-instance_rag","name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":3,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_rag","name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":3,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_rag","name":"Example C: LlamaStackDistribution with <strong>Inline FAISS</strong>","level":3,"index":2,"id":"_example_c_llamastackdistribution_with_inline_faiss"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Ingesting content into a Llama model","level":2,"index":6,"id":"ingesting-content-into-a-llama-model_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Querying ingested content in a Llama model","level":2,"index":7,"id":"querying-ingested-content-in-a-llama-model_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"Preparing documents with Docling for Llama Stack retrieval","level":2,"index":8,"id":"preparing-documents-with-docling-for-llama-stack-retrieval_rag"},{"parentId":"deploying-a-rag-stack-in-a-project_rag","name":"About Llama stack search types","level":2,"index":9,"id":"about-llama-stack-search-types_rag"},{"parentId":"about-llama-stack-search-types_rag","name":"Supported search modes","level":3,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":4,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":4,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":4,"index":2,"id":"_hybrid_search"},{"parentId":"about-llama-stack-search-types_rag","name":"Retrieval database support","level":3,"index":1,"id":"_retrieval_database_support"},{"parentId":null,"name":"Configuring Llama Stack with OAuth Authentication","level":1,"index":3,"id":"auth-on-llama-stack_rag"},{"parentId":null,"name":"Evaluating RAG systems with Llama Stack","level":1,"index":4,"id":"evaluating-rag-systems-with-llama-stack_rag"},{"parentId":"evaluating-rag-systems-with-llama-stack_rag","name":"Understanding RAG evaluation providers","level":2,"index":0,"id":"understanding-rag-evaluation-providers_rag"},{"parentId":"evaluating-rag-systems-with-llama-stack_rag","name":"Using Ragas with Llama Stack","level":2,"index":1,"id":"using-ragas-with-llama-stack_rag"},{"parentId":"evaluating-rag-systems-with-llama-stack_rag","name":"Benchmarking embedding models with BEIR datasets and Llama Stack","level":2,"index":2,"id":"benchmarking-embedding-models-with-BEIR-datasets-and-Llama-Stack_rag"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-machine-learning-features/"},"sections":[{"parentId":null,"name":"Overview of machine learning features and Feature Store","level":1,"index":0,"id":"overview-of-ml-features-and-feature-store.adoc_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Audience for Feature Store","level":2,"index":0,"id":"audience-for-feature-store_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Overview of machine learning features","level":2,"index":1,"id":"overview-of-machine-learning-features_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Overview of Feature Store","level":2,"index":2,"id":"overview-of-feature-store_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Feature Store workflow","level":2,"index":3,"id":"feature-store-workflow_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Setting up the Feature Store user interface for initial use","level":2,"index":4,"id":"setting-up-feature-store-UI_featurestore"},{"parentId":"overview-of-ml-features-and-feature-store.adoc_featurestore","name":"Additional resources","level":2,"index":5,"id":"_additional_resources"},{"parentId":null,"name":"Configuring Feature Store","level":1,"index":1,"id":"_configuring_feature_store"},{"parentId":"_configuring_feature_store","name":"Setting up Feature Store","level":2,"index":0,"id":"setting-up-feature-store_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Before you begin","level":3,"index":0,"id":"before-you-begin_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Enabling the Feature Store component","level":3,"index":1,"id":"enabling-the-feature-store-component_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Creating a Feature Store instance in a project","level":3,"index":2,"id":"creating-a-feature-store-instance-in-a-project_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Configuring and managing Role Based Access Control","level":3,"index":3,"id":"configuring-and-managing-role-based-access-control_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Adding feature definitions and initializing your Feature Store instance","level":3,"index":4,"id":"adding-feature-definitions-and-initializing-your-feature-store-instance_featurestore"},{"parentId":"adding-feature-definitions-and-initializing-your-feature-store-instance_featurestore","name":"Specifying files to ignore","level":4,"index":0,"id":"specifying-files-to-ignore_featurestore"},{"parentId":"setting-up-feature-store_featurestore","name":"Viewing Feature Store objects in the web-based UI","level":3,"index":5,"id":"viewing-feature-store-objects-in-the-web-based-ui_featurestore"},{"parentId":"_configuring_feature_store","name":"Customizing your Feature Store configuration","level":2,"index":1,"id":"customizing-your-feature-store-configuration_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring an offline store","level":3,"index":0,"id":"configuring-an-offline-store_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring an online store","level":3,"index":1,"id":"configuring-an-online-store_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Configuring the feature registry","level":3,"index":2,"id":"configuring-the-feature-registry_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Example PVC configuration","level":3,"index":3,"id":"ref-example-pvc-configuration_featurestore"},{"parentId":"customizing-your-feature-store-configuration_featurestore","name":"Editing an existing Feature Store instance","level":3,"index":4,"id":"editing-an-existing-feature-store-instance_featurestore"},{"parentId":null,"name":"Defining machine learning features","level":1,"index":2,"id":"defining-ml-features_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Setting up your working environment","level":2,"index":0,"id":"setting-up-your-working-environment_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Enabling automatic authentication and publishing features","level":2,"index":1,"id":"enabling-automatic-authentication-and-publishing-features_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"About feature definitions","level":2,"index":2,"id":"about-feature-definitions_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Specifying the data source for features","level":2,"index":3,"id":"specifying-the-data-source-for-features_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"About organizing features by using entities","level":2,"index":4,"id":"about-organizing-features-by-using-entities_featurestore"},{"parentId":"defining-ml-features_featurestore","name":"Creating feature views","level":2,"index":5,"id":"creating-feature-views_featurestore"},{"parentId":null,"name":"Retrieving features for model training","level":1,"index":3,"id":"retrieving-features-for-model-training_featurestore"},{"parentId":"retrieving-features-for-model-training_featurestore","name":"Retrieving data science features","level":2,"index":0,"id":"retrieving-data-science-features_featurestore"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Overview of the model catalog and model registries","level":1,"index":0,"id":"overview-of-model-registries_model-registry"},{"parentId":"overview-of-model-registries_model-registry","name":"Model catalog","level":2,"index":0,"id":"_model_catalog"},{"parentId":"overview-of-model-registries_model-registry","name":"Model registry","level":2,"index":1,"id":"_model_registry"},{"parentId":null,"name":"Enabling the model registry component","level":0,"index":1,"id":"_enabling_the_model_registry_component"},{"parentId":"_enabling_the_model_registry_component","name":"Enabling the model registry component","level":1,"index":0,"id":"enabling-the-model-registry-component_model-registry"},{"parentId":null,"name":"Managing model registries","level":0,"index":2,"id":"_managing_model_registries"},{"parentId":"_managing_model_registries","name":"Creating a model registry","level":1,"index":0,"id":"creating-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Editing a model registry","level":1,"index":1,"id":"editing-a-model-registry_model-registry"},{"parentId":"_managing_model_registries","name":"Managing model registry permissions","level":1,"index":2,"id":"managing-model-registry-permissions_model-registry"},{"parentId":"_managing_model_registries","name":"Deleting a model registry","level":1,"index":3,"id":"deleting-a-model-registry_model-registry"},{"parentId":null,"name":"Working with model registries","level":0,"index":3,"id":"_working_with_model_registries"},{"parentId":"_working_with_model_registries","name":"Working with model registries","level":1,"index":0,"id":"working-with-model-registries_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model from the dashboard","level":2,"index":0,"id":"registering-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Registering a model version","level":2,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered models","level":2,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Viewing registered model versions","level":2,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model metadata in a model registry","level":2,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing model version metadata in a model registry","level":2,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deploying a model version from a model registry","level":2,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Editing the deployment properties of a deployed model version from a model registry","level":2,"index":7,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the model serving platform","level":3,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-model-serving-platform_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Deleting a deployed model version from a model registry","level":2,"index":8,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model","level":2,"index":9,"id":"archiving-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Archiving a model version","level":2,"index":10,"id":"archiving-a-model-version_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model","level":2,"index":11,"id":"restoring-a-model_model-registry"},{"parentId":"working-with-model-registries_model-registry","name":"Restoring a model version","level":2,"index":12,"id":"restoring-a-model-version_model-registry"},{"parentId":null,"name":"Working with the model catalog","level":0,"index":4,"id":"_working_with_the_model_catalog"},{"parentId":"_working_with_the_model_catalog","name":"Working with the model catalog","level":1,"index":0,"id":"working-with-the-model-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Discovering and evaluating models in the model catalog","level":2,"index":0,"id":"viewing-models-in-the-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Registering a model from the model catalog","level":2,"index":1,"id":"registering-a-model-from-the-model-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Deploying a model from the model catalog","level":2,"index":2,"id":"deploying-a-model-from-the-model-catalog_model-registry"},{"parentId":"working-with-the-model-catalog_model-registry","name":"Configuring model catalog sources in OpenShift","level":2,"index":3,"id":"configuring-model-catalog-sources-in-openshift_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/_artifacts/document-attributes-global/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/backing-up-data/"},"sections":[{"parentId":null,"name":"Backing up storage data","level":1,"index":0,"id":"backing-up-storage-data_data-mgmt"},{"parentId":null,"name":"Backing up your cluster","level":1,"index":1,"id":"backing-up-your-cluster_data-mgmt"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/bias-monitoring-tutorial/"},"sections":[{"parentId":null,"name":"Introduction","level":1,"index":0,"id":"t-bias-introduction_bias-tutorial"},{"parentId":"t-bias-introduction_bias-tutorial","name":"About the example models","level":2,"index":0,"id":"_about_the_example_models"},{"parentId":null,"name":"Setting up your environment","level":1,"index":1,"id":"t-bias-setting-up-your-environment_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Downloading the tutorial files","level":2,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Logging in to the OpenShift cluster from the command line","level":2,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Configuring monitoring for the model serving platform","level":2,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Enabling the TrustyAI component","level":2,"index":3,"id":"enabling-trustyai-component_bias-tutorial"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Setting up a project","level":2,"index":4,"id":"_setting_up_a_project"},{"parentId":"t-bias-setting-up-your-environment_bias-tutorial","name":"Authenticating the TrustyAI service","level":2,"index":5,"id":"_authenticating_the_trustyai_service"},{"parentId":null,"name":"Deploying models","level":1,"index":2,"id":"t-bias-deploying-models_bias-tutorial"},{"parentId":null,"name":"Sending training data to the models","level":1,"index":3,"id":"t-bias-sending-training-data-to-the-models_bias-tutorial"},{"parentId":null,"name":"Labeling data fields","level":1,"index":4,"id":"t-bias-labeling-data-fields_bias-tutorial"},{"parentId":null,"name":"Checking model fairness","level":1,"index":5,"id":"t-bias-checking-model-fairness_bias-tutorial"},{"parentId":null,"name":"Scheduling a fairness metric request","level":1,"index":6,"id":"t-bias-scheduling-a-fairness-metric-request_bias-tutorial"},{"parentId":null,"name":"Scheduling an identity metric request","level":1,"index":7,"id":"t-bias-scheduling-an-identity-metric-request_bias-tutorial"},{"parentId":null,"name":"Simulating real world data","level":1,"index":8,"id":"t-bias-simulating-real-world-data_bias-tutorial"},{"parentId":null,"name":"Reviewing the results","level":1,"index":9,"id":"t-bias-reviewing-the-results_bias-tutorial"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"Are the models biased?","level":2,"index":0,"id":"_are_the_models_biased"},{"parentId":"t-bias-reviewing-the-results_bias-tutorial","name":"How does the production data compare to the training data?","level":2,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-workbenches-in-code-server-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using code-server","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-code-server_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project in code-server with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-in-code-server-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes in code-server to a Git repository","level":1,"index":3,"id":"pushing-project-changes-in-code-server-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-cluster-storage/"},"sections":[{"parentId":null,"name":"About persistent storage","level":1,"index":0,"id":"about-persistent-storage_{context}"},{"parentId":"about-persistent-storage_{context}","name":"Storage classes in {productname-short}","level":2,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":"about-persistent-storage_{context}","name":"Access modes","level":2,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":3,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":null,"name":"Adding cluster storage to your project","level":1,"index":1,"id":"adding-cluster-storage-to-your-project_{context}"},{"parentId":null,"name":"Updating cluster storage","level":1,"index":2,"id":"updating-cluster-storage_{context}"},{"parentId":null,"name":"Changing the storage class for an existing cluster storage instance","level":1,"index":3,"id":"changing-the-storage-class-for-an-existing-cluster-storage-instance_{context}"},{"parentId":null,"name":"Deleting cluster storage from a project","level":1,"index":4,"id":"deleting-cluster-storage-from-a-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-external-oidc-provider/"},"sections":[{"parentId":null,"name":"About centralized authentication Gateway API","level":1,"index":0,"id":"about-centralized-auth-oidc_{context}"},{"parentId":null,"name":"Configuring OpenID Connect (OIDC) authentication for Gateway API","level":1,"index":1,"id":"configuring-oidc-auth-gateway-api_{context}"},{"parentId":"configuring-oidc-auth-gateway-api_{context}","name":"Security considerations","level":2,"index":0,"id":"_security_considerations"},{"parentId":null,"name":"Troubleshooting common problems with Gateway API configuration","level":1,"index":2,"id":"troubleshooting-common-problems-gateway-api_{context}"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"The <code>GatewayConfig</code> status shows as not ready","level":2,"index":0,"id":"_the_gatewayconfig_status_shows_as_not_ready"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"Authentication proxy fails to start","level":2,"index":1,"id":"_authentication_proxy_fails_to_start"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"The Gateway is inaccessible","level":2,"index":2,"id":"_the_gateway_is_inaccessible"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"The OIDC authentication fails","level":2,"index":3,"id":"_the_oidc_authentication_fails"},{"parentId":"troubleshooting-common-problems-gateway-api_{context}","name":"The dashboard is not accessible after authentication","level":2,"index":4,"id":"_the_dashboard_is_not_accessible_after_authentication"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/collaborating-on-jupyter-notebooks-by-using-git/"},"sections":[{"parentId":null,"name":"Uploading an existing notebook file from a Git repository by using JupyterLab","level":1,"index":0,"id":"uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from a Git repository by using the CLI","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli_{context}"},{"parentId":null,"name":"Updating your project with changes from a remote Git repository","level":1,"index":2,"id":"updating-your-project-with-changes-from-a-remote-git-repository_{context}"},{"parentId":null,"name":"Pushing project changes to a Git repository","level":1,"index":3,"id":"pushing-project-changes-to-a-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-feature-store-role-based-access-control/"},"sections":[{"parentId":null,"name":"Configuring role-based access control","level":1,"index":0,"id":"configuring-role-based-access-control_{context}"},{"parentId":null,"name":"Default authorization configuration","level":1,"index":1,"id":"ref-default-authorization-configuration_{context}"},{"parentId":null,"name":"Example OIDC Authorization configuration","level":1,"index":2,"id":"ref-example-oidc-authorization-configuration_{context}"},{"parentId":null,"name":"Example Kubernetes Authorization configuration","level":1,"index":3,"id":"ref-example-kubernetes-authorization-configuration_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-trustyai/"},"sections":[{"parentId":null,"name":"Configuring monitoring for your model serving platform","level":1,"index":0,"id":"configuring-monitoring-for-your-model-serving-platform_{context}"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":1,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Configuring TrustyAI with a database","level":1,"index":2,"id":"configuring-trustyai-with-a-database_{context}"},{"parentId":null,"name":"Installing the TrustyAI service for a project","level":1,"index":3,"id":"installing-trustyai-service_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the dashboard","level":2,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":"installing-trustyai-service_{context}","name":"Installing the TrustyAI service by using the CLI","level":2,"index":1,"id":"installing-trustyai-service-using-cli_{context}"},{"parentId":null,"name":"Enabling TrustyAI Integration with KServe RawDeployment","level":1,"index":4,"id":"enabling-trustyai-kserve-integration_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/configuring-your-model-serving-platform/"},"sections":[{"parentId":null,"name":"About model serving","level":1,"index":0,"id":"about-model-serving_odh-admin"},{"parentId":"about-model-serving_odh-admin","name":"Model serving platform","level":2,"index":0,"id":"_model_serving_platform"},{"parentId":"about-model-serving_odh-admin","name":"NVIDIA NIM model serving platform","level":2,"index":1,"id":"_nvidia_nim_model_serving_platform"},{"parentId":null,"name":"Model-serving runtimes","level":1,"index":1,"id":"model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes_odh-admin","name":"ServingRuntime","level":2,"index":0,"id":"_servingruntime"},{"parentId":"model-serving-runtimes_odh-admin","name":"InferenceService","level":2,"index":1,"id":"_inferenceservice"},{"parentId":null,"name":"Model-serving runtimes for accelerators","level":1,"index":2,"id":"model-serving-runtimes-for-accelerators_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"NVIDIA GPUs","level":2,"index":0,"id":"_nvidia_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Intel Gaudi accelerators","level":2,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"AMD GPUs","level":2,"index":2,"id":"_amd_gpus"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"IBM Spyre AI accelerators on x86 and IBM Z","level":2,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86_and_ibm_z"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Supported model-serving runtimes","level":2,"index":4,"id":"supported-model-serving-runtimes_odh-admin"},{"parentId":"model-serving-runtimes-for-accelerators_odh-admin","name":"Tested and verified model-serving runtimes","level":2,"index":5,"id":"tested-verified-runtimes_odh-admin"},{"parentId":null,"name":"Configuring model servers","level":0,"index":3,"id":"_configuring_model_servers"},{"parentId":"_configuring_model_servers","name":"Enabling the model serving platform","level":1,"index":0,"id":"enabling-the-model-serving-platform_odh-admin"},{"parentId":"_configuring_model_servers","name":"Enabling speculative decoding and multi-modal inferencing","level":1,"index":1,"id":"enabling-speculative-decoding-and-multi-modal-inferencing_odh-admin"},{"parentId":"_configuring_model_servers","name":"Adding a custom model-serving runtime","level":1,"index":2,"id":"adding-a-custom-model-serving-runtime_odh-admin"},{"parentId":"_configuring_model_servers","name":"Adding a tested and verified runtime","level":1,"index":3,"id":"adding-a-tested-and-verified-runtime_odh-admin"},{"parentId":null,"name":"Configuring model servers on the NVIDIA NIM model serving platform","level":0,"index":4,"id":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_configuring_model_servers_on_the_nvidia_nim_model_serving_platform","name":"Enabling the NVIDIA NIM model serving platform","level":1,"index":0,"id":"enabling-the-nvidia-nim-model-serving-platform_odh-admin"},{"parentId":null,"name":"Customizing model deployments","level":0,"index":5,"id":"_customizing_model_deployments"},{"parentId":"_customizing_model_deployments","name":"Customizing the parameters of a deployed model-serving runtime","level":1,"index":0,"id":"customizing-parameters-serving-runtime_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizable model serving runtime parameters","level":1,"index":1,"id":"customizable-model-serving-runtime-parameters_odh-admin"},{"parentId":"_customizing_model_deployments","name":"Customizing the vLLM model-serving runtime","level":1,"index":2,"id":"Customizing-the-vllm-runtime_odh-admin"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-and-importing-jupyter-notebooks/"},"sections":[{"parentId":null,"name":"Creating a Jupyter notebook","level":1,"index":0,"id":"creating-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to JupyterLab from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage_{context}"},{"parentId":null,"name":"Deleting files into the trash directory of your workbench in Jupyter notebooks","level":1,"index":2,"id":"deleting-files-in-trash-directory_{context}"},{"parentId":"deleting-files-in-trash-directory_{context}","name":"Emptying the trash directory of your workbench in Jupyter notebooks","level":2,"index":0,"id":"emptying-trash-directory_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":3,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-code-server-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench","level":1,"index":0,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Uploading an existing notebook file to code-server from local storage","level":1,"index":1,"id":"uploading-an-existing-notebook-file-to-code-server-from-local-storage_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/creating-custom-workbench-images/"},"sections":[{"parentId":null,"name":"Creating a custom image from a default {productname-short} image","level":1,"index":0,"id":"creating-a-custom-image-from-default-image_custom-images"},{"parentId":null,"name":"Creating a custom image from your own image","level":1,"index":1,"id":"creating-a-custom-image-from-your-own-image_custom-images"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Basic guidelines for creating your own workbench image","level":2,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":"creating-a-custom-image-from-your-own-image_custom-images","name":"Advanced guidelines for creating your own workbench image","level":2,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Enabling custom images in {productname-short}","level":1,"index":2,"id":"enabling-custom-images_custom-images"},{"parentId":null,"name":"Importing a custom workbench image","level":1,"index":3,"id":"importing-a-custom-workbench-image_custom-images"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-component-deployment-resources/"},"sections":[{"parentId":null,"name":"Overview of component resource customization","level":1,"index":0,"id":"overview-of-component-resource-customization_managing-resources"},{"parentId":null,"name":"Customizing component resources","level":1,"index":1,"id":"customizing-component-resources_managing-resources"},{"parentId":null,"name":"Disabling component resource customization","level":1,"index":2,"id":"disabling-component-resource-customization_managing-resources"},{"parentId":null,"name":"Re-enabling component resource customization","level":1,"index":3,"id":"reenabling-component-resource-customization_managing-resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-the-dashboard/"},"sections":[{"parentId":null,"name":"Editing the dashboard configuration","level":1,"index":0,"id":"editing-the-dashboard-configuration_dashboard"},{"parentId":null,"name":"Dashboard configuration options","level":1,"index":1,"id":"ref-dashboard-configuration-options_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/customizing-your-feature-store-configuration/"},"sections":[{"parentId":null,"name":"Configuring an offline store","level":1,"index":0,"id":"configuring-an-offline-store_{context}"},{"parentId":null,"name":"Configuring an online store","level":1,"index":1,"id":"configuring-an-online-store_{context}"},{"parentId":null,"name":"Configuring the feature registry","level":1,"index":2,"id":"configuring-the-feature-registry_{context}"},{"parentId":null,"name":"Example PVC configuration","level":1,"index":3,"id":"ref-example-pvc-configuration_{context}"},{"parentId":null,"name":"Editing an existing Feature Store instance","level":1,"index":4,"id":"editing-an-existing-feature-store-instance_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/defining-ml-features/"},"sections":[{"parentId":null,"name":"Setting up your working environment","level":1,"index":0,"id":"setting-up-your-working-environment_{context}"},{"parentId":null,"name":"Enabling automatic authentication and publishing features","level":1,"index":1,"id":"enabling-automatic-authentication-and-publishing-features_{context}"},{"parentId":null,"name":"About feature definitions","level":1,"index":2,"id":"about-feature-definitions_{context}"},{"parentId":null,"name":"Specifying the data source for features","level":1,"index":3,"id":"specifying-the-data-source-for-features_{context}"},{"parentId":null,"name":"About organizing features by using entities","level":1,"index":4,"id":"about-organizing-features-by-using-entities_{context}"},{"parentId":null,"name":"Creating feature views","level":1,"index":5,"id":"creating-feature-views_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deploying-a-rag-stack-in-a-project/"},"sections":[{"parentId":null,"name":"Overview of RAG","level":1,"index":0,"id":"overview-of-rag_{context}"},{"parentId":"overview-of-rag_{context}","name":"Audience for RAG","level":2,"index":0,"id":"_audience_for_rag"},{"parentId":null,"name":"Overview of vector databases","level":1,"index":1,"id":"overview-of-vector-databases_{context}"},{"parentId":"overview-of-vector-databases_{context}","name":"Overview of Milvus vector databases","level":2,"index":0,"id":"overview-of-milvus-vector-databases_{context}"},{"parentId":"overview-of-vector-databases_{context}","name":"Overview of FAISS vector databases","level":2,"index":1,"id":"overview-of-faiss-vector-databases_{context}"},{"parentId":null,"name":"Deploying a Llama model with KServe","level":1,"index":2,"id":"Deploying-a-llama-model-with-kserve_{context}"},{"parentId":null,"name":"Testing your vLLM model endpoints","level":1,"index":3,"id":"testing-your-vllm-model-endpoints_{context}"},{"parentId":null,"name":"Deploying a remote Milvus vector database","level":1,"index":4,"id":"deploying-a-remote-milvus-vector-database_{context}"},{"parentId":null,"name":"Deploying a LlamaStackDistribution instance","level":1,"index":5,"id":"deploying-a-llamastackdistribution-instance_{context}"},{"parentId":"deploying-a-llamastackdistribution-instance_{context}","name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":2,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_{context}","name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":2,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":"deploying-a-llamastackdistribution-instance_{context}","name":"Example C: LlamaStackDistribution with <strong>Inline FAISS</strong>","level":2,"index":2,"id":"_example_c_llamastackdistribution_with_inline_faiss"},{"parentId":null,"name":"Ingesting content into a Llama model","level":1,"index":6,"id":"ingesting-content-into-a-llama-model_{context}"},{"parentId":null,"name":"Querying ingested content in a Llama model","level":1,"index":7,"id":"querying-ingested-content-in-a-llama-model_{context}"},{"parentId":null,"name":"Preparing documents with Docling for Llama Stack retrieval","level":1,"index":8,"id":"preparing-documents-with-docling-for-llama-stack-retrieval_{context}"},{"parentId":null,"name":"About Llama stack search types","level":1,"index":9,"id":"about-llama-stack-search-types_{context}"},{"parentId":"about-llama-stack-search-types_{context}","name":"Supported search modes","level":2,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":3,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":3,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":3,"index":2,"id":"_hybrid_search"},{"parentId":"about-llama-stack-search-types_{context}","name":"Retrieval database support","level":2,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/deploying-models/"},"sections":[{"parentId":null,"name":"Using OCI containers for model storage","level":1,"index":0,"id":"using-oci-containers-for-model-storage_odh-user"},{"parentId":null,"name":"Storing a model in an OCI image","level":1,"index":1,"id":"storing-a-model-in-oci-image_odh-user"},{"parentId":null,"name":"Uploading model files to a Persistent Volume Claim (PVC)","level":1,"index":2,"id":"uploading-model-files-to-pvc_odh-user"},{"parentId":null,"name":"Deploying models","level":0,"index":3,"id":"_deploying_models"},{"parentId":"_deploying_models","name":"Deploying models on the model serving platform","level":1,"index":0,"id":"deploying-models-on-the-model-serving-platform_odh-user"},{"parentId":"_deploying_models","name":"Deploying a model stored in an OCI image by using the CLI","level":1,"index":1,"id":"deploying-model-stored-in-oci-image_odh-user"},{"parentId":"_deploying_models","name":"Deploying models by using {llmd}","level":1,"index":2,"id":"deploying-models-using-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Configuring authentication for {llmd} using {org-name} Connectivity Link","level":2,"index":0,"id":"configuring-authentication-for-llmd_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Enabling {llmd}","level":2,"index":1,"id":"enabling-distributed-inference_odh-user"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Example usage for {llmd}","level":2,"index":2,"id":"ref-example-distributed-inference_odh-user"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Single-node GPU deployment","level":3,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Multi-node deployment","level":3,"index":1,"id":"_multi_node_deployment"},{"parentId":"ref-example-distributed-inference_odh-user","name":"Intelligent inference scheduler with KV cache routing","level":3,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"},{"parentId":"deploying-models-using-distributed-inference_odh-user","name":"Configuring authentication for {llmd} using {org-name} Connectivity Link","level":2,"index":3,"id":"configuring-authentication-for-llmd_odh-user"},{"parentId":"_deploying_models","name":"Monitoring models","level":1,"index":3,"id":"_monitoring_models"},{"parentId":"_monitoring_models","name":"Viewing performance metrics for a deployed model","level":2,"index":0,"id":"viewing-performance-metrics-for-deployed-model_odh-user"},{"parentId":"_monitoring_models","name":"Viewing model-serving runtime metrics for the model serving platform","level":2,"index":1,"id":"viewing-metrics-for-the-model-serving-platform_odh-user"},{"parentId":null,"name":"Deploying models on the NVIDIA NIM model serving platform","level":0,"index":4,"id":"_deploying_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Deploying models on the NVIDIA NIM model serving platform","level":1,"index":0,"id":"deploying-models-on-the-NVIDIA-NIM-model-serving-platform_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing NVIDIA NIM metrics for a NIM model","level":1,"index":1,"id":"viewing-nvidia-nim-metrics-for-a-nim-model_odh-user"},{"parentId":"_deploying_models_on_the_nvidia_nim_model_serving_platform","name":"Viewing performance metrics for a NIM model","level":1,"index":2,"id":"viewing-performance-metrics-for-a-nim-model_odh-user"},{"parentId":null,"name":"Making inference requests to deployed models","level":0,"index":5,"id":"_making_inference_requests_to_deployed_models"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the authentication token for a deployed model","level":1,"index":0,"id":"accessing-authentication-token-for-deployed-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Accessing the inference endpoint for a deployed model","level":1,"index":1,"id":"accessing-inference-endpoint-for-model_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Making inference requests to models deployed on the model serving platform","level":1,"index":2,"id":"making-inference-requests-to-models-deployed-on-model-serving-platform_odh-user"},{"parentId":"_making_inference_requests_to_deployed_models","name":"Inference endpoints","level":1,"index":3,"id":"inference-endpoints_odh-user"},{"parentId":"inference-endpoints_odh-user","name":"Caikit TGIS ServingRuntime for KServe","level":2,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"OpenVINO Model Server","level":2,"index":1,"id":"_openvino_model_server"},{"parentId":"inference-endpoints_odh-user","name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":2,"index":2,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":2,"index":3,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM AMD GPU ServingRuntime for KServe","level":2,"index":4,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":2,"index":5,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"vLLM Spyre s390x ServingRuntime for KServe","level":2,"index":6,"id":"_vllm_spyre_s390x_servingruntime_for_kserve"},{"parentId":"inference-endpoints_odh-user","name":"NVIDIA Triton Inference Server","level":2,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":"inference-endpoints_odh-user","name":"Seldon MLServer","level":2,"index":8,"id":"_seldon_mlserver"},{"parentId":"inference-endpoints_odh-user","name":"Additional resources","level":2,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-accelerators/"},"sections":[{"parentId":null,"name":"Enabling NVIDIA GPUs","level":1,"index":0,"id":"enabling-nvidia-gpus_managing-odh"},{"parentId":null,"name":"Intel Gaudi AI Accelerator integration","level":1,"index":1,"id":"intel-gaudi-ai-accelerator-integration_managing-odh"},{"parentId":"intel-gaudi-ai-accelerator-integration_managing-odh","name":"Enabling Intel Gaudi AI accelerators","level":2,"index":0,"id":"enabling-intel-gaudi-ai-accelerators_managing-odh"},{"parentId":null,"name":"AMD GPU Integration","level":1,"index":2,"id":"amd-gpu-integration_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Verifying AMD GPU availability on your cluster","level":2,"index":0,"id":"verifying-amd-gpu-availability-on-your-cluster_managing-odh"},{"parentId":"amd-gpu-integration_managing-odh","name":"Enabling AMD GPUs","level":2,"index":1,"id":"enabling-amd-gpus_managing-odh"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-ai-safety-with-guardrails/"},"sections":[{"parentId":null,"name":"Understanding detectors","level":1,"index":0,"id":"guardrails-detectors_{context}"},{"parentId":"guardrails-detectors_{context}","name":"Built-in Detector","level":2,"index":0,"id":"_built_in_detector"},{"parentId":"guardrails-detectors_{context}","name":"The Hugging Face Detector serving runtime","level":2,"index":1,"id":"guardrails-configuring-the-hugging-face-detector-serving-runtime_{context}"},{"parentId":"guardrails-configuring-the-hugging-face-detector-serving-runtime_{context}","name":"Guardrails Detector Hugging Face serving runtime configuration values","level":3,"index":0,"id":"_guardrails_detector_hugging_face_serving_runtime_configuration_values"},{"parentId":null,"name":"Orchestrator Configuration Parameters","level":1,"index":1,"id":"guardrails-orchestrator-config-parameters_{context}"},{"parentId":null,"name":"Guardrails Gateway Config Parameters","level":1,"index":2,"id":"guardrails-gateway-config-parameters_{context}"},{"parentId":null,"name":"Deploying the Guardrails Orchestrator","level":1,"index":3,"id":"deploying-the-guardrails-orchestrator-service_{context}"},{"parentId":null,"name":"Auto-configuring Guardrails","level":1,"index":4,"id":"guardrails-auto-config_{context}"},{"parentId":null,"name":"Configuring the OpenTelemetry exporter","level":1,"index":5,"id":"configuring-the-opentelemetry-exporter_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enabling-external-resource-access-for-lmeval-jobs/"},"sections":[{"parentId":null,"name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":1,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_{context}"},{"parentId":null,"name":"Updating LMEval job configuration using the web console","level":1,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/enforcing-local-queues/"},"sections":[{"parentId":null,"name":"Enforcing the local-queue labeling policy for all projects","level":1,"index":0,"id":"enforcing-lqlabel-all_{context}"},{"parentId":null,"name":"Disabling the local-queue labeling policy for all projects","level":1,"index":1,"id":"disabling-lqlabel-all_{context}"},{"parentId":null,"name":"Enforcing the local-queue labeling policy for some projects only","level":1,"index":2,"id":"enforcing-lqlabel-some_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-large-language-models/"},"sections":[{"parentId":null,"name":"Setting up LM-Eval","level":1,"index":0,"id":"setting-up-lmeval_{context}"},{"parentId":null,"name":"Enabling external resource access for LMEval jobs","level":1,"index":1,"id":"enabling-external-resource-access-for-lmeval-jobs_{context}"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_{context}","name":"Enabling online access and remote code execution for LMEval Jobs using the CLI","level":2,"index":0,"id":"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_{context}"},{"parentId":"enabling-external-resource-access-for-lmeval-jobs_{context}","name":"Updating LMEval job configuration using the web console","level":2,"index":1,"id":"updating-lmeval-job-configuration-using-the-web-console_{context}"},{"parentId":null,"name":"LM-Eval evaluation job","level":1,"index":2,"id":"lmeval-evaluation-job_{context}"},{"parentId":null,"name":"LM-Eval evaluation job properties","level":1,"index":3,"id":"lmeval-evaluation-job-properties_{context}"},{"parentId":"lmeval-evaluation-job-properties_{context}","name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":2,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"},{"parentId":null,"name":"Performing model evaluations in the dashboard","level":1,"index":4,"id":"performing-model-evaluations-in-the-dashboard_{context}"},{"parentId":null,"name":"LM-Eval scenarios","level":1,"index":5,"id":"lmeval-scenarios_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Accessing Hugging Face models with an environment variable token","level":2,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using a custom Unitxt card","level":2,"index":1,"id":"using-a-custom-unitxt-card_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using PVCs as storage","level":2,"index":2,"id":"using-pvcs-as-storage_{context}"},{"parentId":"using-pvcs-as-storage_{context}","name":"Managed PVCs","level":3,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_{context}","name":"Existing PVCs","level":3,"index":1,"id":"_existing_pvcs"},{"parentId":"lmeval-scenarios_{context}","name":"Using a KServe Inference Service","level":2,"index":3,"id":"using-a-kserve-inference-service_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Setting up LM-Eval S3 Support","level":2,"index":4,"id":"setting-up-lmeval-s3-support_{context}"},{"parentId":"lmeval-scenarios_{context}","name":"Using LLM-as-a-Judge metrics with LM-Eval","level":2,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-rag-systems-with-llama-stack/"},"sections":[{"parentId":null,"name":"Understanding RAG evaluation providers","level":1,"index":0,"id":"understanding-rag-evaluation-providers_{context}"},{"parentId":null,"name":"Using Ragas with Llama Stack","level":1,"index":1,"id":"using-ragas-with-llama-stack_{context}"},{"parentId":null,"name":"Benchmarking embedding models with BEIR datasets and Llama Stack","level":1,"index":2,"id":"benchmarking-embedding-models-with-BEIR-datasets-and-Llama-Stack_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/evaluating-rag-systems/"},"sections":[{"parentId":null,"name":"About Ragas evaluation","level":1,"index":0,"id":"_about_ragas_evaluation"},{"parentId":"_about_ragas_evaluation","name":"Key Ragas metrics","level":2,"index":0,"id":"_key_ragas_metrics"},{"parentId":"_about_ragas_evaluation","name":"Use cases for Ragas in AI engineering workflows","level":2,"index":1,"id":"_use_cases_for_ragas_in_ai_engineering_workflows"},{"parentId":"_about_ragas_evaluation","name":"Ragas provider deployment modes","level":2,"index":2,"id":"_ragas_provider_deployment_modes"},{"parentId":null,"name":"Setting up the Ragas inline provider for development","level":1,"index":1,"id":"setting-up-ragas-inline-provider_{context}"},{"parentId":null,"name":"Configuring the Ragas remote provider for production","level":1,"index":2,"id":"configuring-ragas-remote-provider-for-production_{context}"},{"parentId":null,"name":"Evaluating RAG system quality with Ragas metrics","level":1,"index":3,"id":"evaluating-rag-system-quality-with-ragas_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/example-kfto-pytorch-training-scripts/"},"sections":[{"parentId":null,"name":"Example Training Operator PyTorch training script: NCCL","level":1,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training script: DDP","level":1,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training script: FSDP","level":1,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/experimenting-with-models-in-the-gen-ai-playground/"},"sections":[{"parentId":null,"name":"Playground overview","level":1,"index":0,"id":"playground-overview_rhoai-user"},{"parentId":"playground-overview_rhoai-user","name":"Core capabilities","level":2,"index":0,"id":"_core_capabilities"},{"parentId":null,"name":"Playground prerequisites","level":1,"index":1,"id":"playground-prerequisites_rhoai-user"},{"parentId":"playground-prerequisites_rhoai-user","name":"Cluster administrator prerequisites","level":2,"index":0,"id":"_cluster_administrator_prerequisites"},{"parentId":"playground-prerequisites_rhoai-user","name":"User prerequisites","level":2,"index":1,"id":"_user_prerequisites"},{"parentId":"playground-prerequisites_rhoai-user","name":"Model and runtime requirements for the playground","level":2,"index":2,"id":"_model_and_runtime_requirements_for_the_playground"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Key model selection factors","level":3,"index":0,"id":"_key_model_selection_factors"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Example model configuration","level":3,"index":1,"id":"_example_model_configuration"},{"parentId":"playground-prerequisites_rhoai-user","name":"Configuring Model Control Protocol (MCP) servers","level":2,"index":3,"id":"configuring-model-control-protocol-servers_rhoai-user"},{"parentId":null,"name":"About the AI assets endpoint page","level":1,"index":2,"id":"About-the-ai-assets-endpoint-page_rhoai-user"},{"parentId":null,"name":"Configuring a playground for your project","level":1,"index":3,"id":"configuring-a-playground-for-your-project_rhoai-user"},{"parentId":null,"name":"Testing baseline model responses","level":1,"index":4,"id":"testing-baseline-model-responses_rhoai-user"},{"parentId":null,"name":"Testing your model with retrieval augmented generation (RAG)","level":1,"index":5,"id":"testing-your-model-with-rag_rhoai-user"},{"parentId":"testing-your-model-with-rag_rhoai-user","name":"Understanding RAG settings","level":2,"index":0,"id":"understanding-rag-settings_rhoai-user"},{"parentId":null,"name":"Testing with model control protocol (MCP) servers","level":1,"index":6,"id":"testing-with-model-control-protocol-servers_rhoai-user"},{"parentId":null,"name":"Exporting your playground configuration","level":1,"index":7,"id":"exporting-your-playground-configuration_rhoai-user"},{"parentId":null,"name":"Updating your playground configuration","level":1,"index":8,"id":"updating-your-playground-configuration_rhoai-user"},{"parentId":null,"name":"Deleting a playground from your project","level":1,"index":9,"id":"Deleting-a-playground-from-your-project_rhoai-user"},{"parentId":null,"name":"Next steps","level":1,"index":10,"id":"next-steps_rhoai-user"},{"parentId":null,"name":"Troubleshooting playground issues","level":1,"index":11,"id":"troubleshooting-playground-issues_rhoai-user"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The chatbot thinks indefinitely","level":2,"index":0,"id":"_the_chatbot_thinks_indefinitely"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The model does not use RAG data","level":2,"index":1,"id":"_the_model_does_not_use_rag_data"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"MCP servers are missing from the UI","level":2,"index":2,"id":"_mcp_servers_are_missing_from_the_ui"},{"parentId":"troubleshooting-playground-issues_rhoai-user","name":"The model fails to call MCP tools","level":2,"index":3,"id":"_the_model_fails_to_call_mcp_tools"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/fine-tuning-a-model-by-using-kubeflow-training/"},"sections":[{"parentId":null,"name":"Configuring the fine-tuning job","level":1,"index":0,"id":"configuring-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Running the fine-tuning job","level":1,"index":1,"id":"running-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Deleting the fine-tuning job","level":1,"index":2,"id":"deleting-the-fine-tuning-job_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/generate-synthetic-data/"},"sections":[{"parentId":null,"name":"Explore the SDG Hub examples","level":1,"index":0,"id":"explore-the-sdg-hub-examples_{context}"},{"parentId":null,"name":"Guided example - Build a KFP pipeline for SDG","level":1,"index":1,"id":"guided-example-build-a-kfp-pipeline-for-sdg_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v1/"},"sections":[{"parentId":null,"name":"Installing the Open Data Hub Operator version 1","level":1,"index":0,"id":"installing-the-odh-operator-v1_installv1"},{"parentId":null,"name":"Creating a new project for your Open Data Hub instance","level":1,"index":1,"id":"creating-a-new-project-for-your-odh-instance_installv1"},{"parentId":null,"name":"Adding an Open Data Hub instance","level":1,"index":2,"id":"adding-an-odh-instance_installv1"},{"parentId":null,"name":"Accessing the dashboard","level":1,"index":3,"id":"accessing-the-dashboard_installv1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/installing-odh-v2/"},"sections":[{"parentId":null,"name":"Configuring custom namespaces","level":1,"index":0,"id":"configuring-custom-namespaces"},{"parentId":null,"name":"Installing the Open Data Hub Operator version 2","level":1,"index":1,"id":"installing-the-odh-operator-v2_installv2"},{"parentId":null,"name":"Installing Open Data Hub components","level":1,"index":2,"id":"installing-odh-components_installv2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/lmeval-scenarios/"},"sections":[{"parentId":null,"name":"Accessing Hugging Face models with an environment variable token","level":1,"index":0,"id":"accessing-hugging-face-models-with-an-environment-variable-token_{context}"},{"parentId":null,"name":"Using a custom Unitxt card","level":1,"index":1,"id":"using-a-custom-unitxt-card_{context}"},{"parentId":null,"name":"Using PVCs as storage","level":1,"index":2,"id":"using-pvcs-as-storage_{context}"},{"parentId":"using-pvcs-as-storage_{context}","name":"Managed PVCs","level":2,"index":0,"id":"_managed_pvcs"},{"parentId":"using-pvcs-as-storage_{context}","name":"Existing PVCs","level":2,"index":1,"id":"_existing_pvcs"},{"parentId":null,"name":"Using a KServe Inference Service","level":1,"index":3,"id":"using-a-kserve-inference-service_{context}"},{"parentId":null,"name":"Setting up LM-Eval S3 Support","level":1,"index":4,"id":"setting-up-lmeval-s3-support_{context}"},{"parentId":null,"name":"Using LLM-as-a-Judge metrics with LM-Eval","level":1,"index":5,"id":"using-llm-as-a-judge-metrics-with-lmeval_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-access-to-projects/"},"sections":[{"parentId":null,"name":"Granting access to a project","level":1,"index":0,"id":"granting-access-to-a-project_{context}"},{"parentId":null,"name":"Updating access to a project","level":1,"index":1,"id":"updating-access-to-a-project_{context}"},{"parentId":null,"name":"Removing access to a project","level":1,"index":2,"id":"removing-access-to-a-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-ai-pipelines/"},"sections":[{"parentId":null,"name":"Configuring a pipeline server","level":1,"index":0,"id":"configuring-a-pipeline-server_{context}"},{"parentId":"configuring-a-pipeline-server_{context}","name":"Configuring a pipeline server with an external Amazon RDS database","level":2,"index":0,"id":"configuring-a-pipeline-server-with-an-external-amazon-rds-db_{context}"},{"parentId":null,"name":"Defining a pipeline","level":1,"index":1,"id":"defining-a-pipeline_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Compiling the pipeline YAML with the Kubeflow Pipelines SDK","level":2,"index":0,"id":"compiling-the-pipeline-yaml-with-kfp-sdk_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Compiling Kubernetes-native manifests with the Kubeflow Pipelines SDK","level":2,"index":1,"id":"compiling-kubernetes-native-manifests-with-kfp-sdk_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Authenticating the Kubeflow Pipelines SDK with a pipeline server","level":2,"index":2,"id":"authenticating-kfp-sdk-with-pipeline-server_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Defining a pipeline by using the Kubernetes API","level":2,"index":3,"id":"defining-a-pipeline-by-using-the-kubernetes-api_{context}"},{"parentId":"defining-a-pipeline_{context}","name":"Migrating pipelines from database to Kubernetes API storage","level":2,"index":4,"id":"migrating-pipelines-from-database-to-kubernetes-api_{context}"},{"parentId":null,"name":"Importing a pipeline","level":1,"index":2,"id":"importing-a-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline","level":1,"index":3,"id":"deleting-a-pipeline_{context}"},{"parentId":null,"name":"Deleting a pipeline server","level":1,"index":4,"id":"deleting-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline server","level":1,"index":5,"id":"viewing-the-details-of-a-pipeline-server_{context}"},{"parentId":null,"name":"Viewing existing pipelines","level":1,"index":6,"id":"viewing-existing-pipelines_{context}"},{"parentId":null,"name":"Overview of pipeline versions","level":1,"index":7,"id":"overview-of-pipeline-versions_{context}"},{"parentId":null,"name":"Uploading a pipeline version","level":1,"index":8,"id":"uploading-a-pipeline-version_{context}"},{"parentId":null,"name":"Deleting a pipeline version","level":1,"index":9,"id":"deleting-a-pipeline-version_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline version","level":1,"index":10,"id":"viewing-the-details-of-a-pipeline-version_{context}"},{"parentId":null,"name":"Downloading a pipeline version","level":1,"index":11,"id":"downloading-a-pipeline-version_{context}"},{"parentId":null,"name":"Overview of pipelines caching","level":1,"index":12,"id":"overview-of-pipelines-caching_{context}"},{"parentId":"overview-of-pipelines-caching_{context}","name":"Caching criteria","level":2,"index":0,"id":"_caching_criteria"},{"parentId":"overview-of-pipelines-caching_{context}","name":"Viewing cached steps in the {productname-short} user interface","level":2,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"},{"parentId":"overview-of-pipelines-caching_{context}","name":"Controlling caching in pipelines","level":2,"index":2,"id":"controlling-caching-in-pipelines_{context}"},{"parentId":"controlling-caching-in-pipelines_{context}","name":"Disabling caching for individual tasks","level":3,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":"controlling-caching-in-pipelines_{context}","name":"Disabling caching for a pipeline at submit time","level":3,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":"controlling-caching-in-pipelines_{context}","name":"Disabling caching for a pipeline at compile time","level":3,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":"controlling-caching-in-pipelines_{context}","name":"Disabling caching for all pipelines (pipeline server)","level":3,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-and-monitoring-models/"},"sections":[{"parentId":null,"name":"Adding a custom model-serving runtime","level":1,"index":0,"id":"adding-a-custom-model-serving-runtime_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models","level":0,"index":1,"id":"_managing_and_monitoring_models"},{"parentId":"_managing_and_monitoring_models","name":"Setting a timeout for KServe","level":1,"index":0,"id":"setting-timeout-for-kserve_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Deploying models by using multiple GPU nodes","level":1,"index":1,"id":"deploying-models-using-multiple-gpu-nodes_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Configuring an inference service for Kueue","level":1,"index":2,"id":"configuring-an-inference-service-for-kueue_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Configuring an inference service for Spyre","level":1,"index":3,"id":"configuring-inference-service-for-spyre_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Optimizing performance and tuning","level":1,"index":4,"id":"_optimizing_performance_and_tuning"},{"parentId":"_optimizing_performance_and_tuning","name":"Determining GPU requirements for LLM-powered applications","level":2,"index":0,"id":"determining-gpu-requirements-for-llm-powered-applications_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Performance considerations for text-summarization and retrieval-augmented generation (RAG) applications","level":2,"index":1,"id":"performance-considerations-for-document-based-apps_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Inference performance metrics","level":2,"index":2,"id":"inference-performance-metrics_cluster-admin"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Latency","level":3,"index":0,"id":"_latency"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Throughput","level":3,"index":1,"id":"_throughput"},{"parentId":"inference-performance-metrics_cluster-admin","name":"Cost per million tokens","level":3,"index":2,"id":"_cost_per_million_tokens"},{"parentId":"_optimizing_performance_and_tuning","name":"Configuring metrics-based autoscaling","level":2,"index":3,"id":"configuring-metrics-based-autoscaling_cluster-admin"},{"parentId":"_optimizing_performance_and_tuning","name":"Guidelines for metrics-based autoscaling","level":2,"index":4,"id":"guidelines-for-metrics-based-autoscaling_cluster-admin"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing metrics for latency and throughput-optimized scaling","level":3,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Choosing the right sliding window","level":3,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Optimizing HPA scale-down configuration","level":3,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":"guidelines-for-metrics-based-autoscaling_cluster-admin","name":"Considering model size for optimal scaling","level":3,"index":3,"id":"_considering_model_size_for_optimal_scaling"},{"parentId":"_managing_and_monitoring_models","name":"Monitoring models","level":1,"index":5,"id":"_monitoring_models"},{"parentId":"_monitoring_models","name":"Configuring monitoring for the model serving platform","level":2,"index":0,"id":"configuring-monitoring-for-the-model-serving-platform_cluster-admin"},{"parentId":"_managing_and_monitoring_models","name":"Using Grafana to monitor model performance","level":1,"index":6,"id":"_using_grafana_to_monitor_model_performance"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a Grafana metrics dashboard","level":2,"index":0,"id":"deploying-a-grafana-metrics-dashboard_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Deploying a vLLM/GPU metrics dashboard on a Grafana instance","level":2,"index":1,"id":"deploying-vllm-gpu-metrics-dashboard-grafana_cluster-admin"},{"parentId":"_using_grafana_to_monitor_model_performance","name":"Grafana metrics","level":2,"index":2,"id":"ref-grafana-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"Accelerator metrics","level":3,"index":0,"id":"ref-accelerator-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"CPU metrics","level":3,"index":1,"id":"ref-cpu-metrics_cluster-admin"},{"parentId":"ref-grafana-metrics_cluster-admin","name":"vLLM metrics","level":3,"index":2,"id":"ref-vllm-metrics_cluster-admin"},{"parentId":null,"name":"Managing and monitoring models on the NVIDIA NIM model serving platform","level":0,"index":2,"id":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Customizing model selection options for the NVIDIA NIM model serving platform","level":1,"index":0,"id":"Customizing-model-selection-options_cluster-admin"},{"parentId":"_managing_and_monitoring_models_on_the_nvidia_nim_model_serving_platform","name":"Enabling NVIDIA NIM metrics for an existing NIM deployment","level":1,"index":1,"id":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin"},{"parentId":"enabling-nim-metrics-for-an-existing-nim-deployment_cluster-admin","name":"Enabling graph generation for an existing NIM deployment","level":2,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-applications-that-show-in-the-dashboard/"},"sections":[{"parentId":null,"name":"Adding an application to the dashboard","level":1,"index":0,"id":"adding-an-application-to-the-dashboard_dashboard"},{"parentId":null,"name":"Preventing users from adding applications to the dashboard","level":1,"index":1,"id":"preventing-users-from-adding-applications-to-the-dashboard_dashboard"},{"parentId":null,"name":"Disabling applications connected to {productname-short}","level":1,"index":2,"id":"disabling-applications-connected_dashboard"},{"parentId":null,"name":"Showing or hiding information about available applications","level":1,"index":3,"id":"showing-hiding-information-about-available-applications_dashboard"},{"parentId":null,"name":"Hiding the default basic workbench application","level":1,"index":4,"id":"hiding-the-default-basic-workbench-application_dashboard"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-basic-workbenches/"},"sections":[{"parentId":null,"name":"Accessing the administration interface for basic workbenches","level":1,"index":0,"id":"accessing-the-administration-interface-for-basic-workbenches_{context}"},{"parentId":null,"name":"Starting basic workbenches owned by other users","level":1,"index":1,"id":"starting-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Accessing basic workbenches owned by other users","level":1,"index":2,"id":"accessing-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping basic workbenches owned by other users","level":1,"index":3,"id":"stopping-basic-workbenches-owned-by-other-users_{context}"},{"parentId":null,"name":"Stopping idle workbenches","level":1,"index":4,"id":"stopping-idle-workbenches_{context}"},{"parentId":null,"name":"Adding workbench pod tolerations","level":1,"index":5,"id":"adding-workbench-pod-tolerations_{context}"},{"parentId":null,"name":"Troubleshooting common problems in workbenches for administrators","level":1,"index":6,"id":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":2,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"A user&#8217;s workbench does not start","level":2,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":"troubleshooting-common-problems-in-workbenches-for-administrators_{context}","name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":2,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-cluster-pvc-size/"},"sections":[{"parentId":null,"name":"Configuring the default PVC size for your cluster","level":1,"index":0,"id":"configuring-the-default-pvc-size-for-your-cluster_{context}"},{"parentId":null,"name":"Restoring the default PVC size for your cluster","level":1,"index":1,"id":"restoring-the-default-pvc-size-for-your-cluster_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-connection-types/"},"sections":[{"parentId":null,"name":"Viewing connection types","level":1,"index":0,"id":"viewing-connection-types_{context}"},{"parentId":null,"name":"Creating a connection type","level":1,"index":1,"id":"creating-a-connection-type_{context}"},{"parentId":null,"name":"Duplicating a connection type","level":1,"index":2,"id":"duplicating-a-connection-type_{context}"},{"parentId":null,"name":"Editing a connection type","level":1,"index":3,"id":"editing-a-connection-type_{context}"},{"parentId":null,"name":"Enabling a connection type","level":1,"index":4,"id":"enabling-a-connection-type_{context}"},{"parentId":null,"name":"Deleting a connection type","level":1,"index":5,"id":"deleting-a-connection-type_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-custom-training-images/"},"sections":[{"parentId":null,"name":"About base training images","level":1,"index":0,"id":"about-base-training-images_{context}"},{"parentId":null,"name":"Creating a custom training image","level":1,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":null,"name":"Pushing an image to the integrated OpenShift image registry","level":1,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-distributed-workloads/"},"sections":[{"parentId":null,"name":"Configuring quota management for distributed workloads","level":1,"index":0,"id":"configuring-quota-management-for-distributed-workloads_{context}"},{"parentId":null,"name":"Example Kueue resource configurations for distributed workloads","level":1,"index":1,"id":"ref-example-kueue-resource-configurations_{context}"},{"parentId":"ref-example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs without shared cohort","level":2,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":3,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":3,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":3,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":3,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":"ref-example-kueue-resource-configurations_{context}","name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":2,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":3,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":3,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":3,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":3,"index":3,"id":"_nvidia_gpu_cluster_queue"},{"parentId":null,"name":"Configuring a cluster for RDMA","level":1,"index":2,"id":"configuring-a-cluster-for-rdma_{context}"},{"parentId":null,"name":"Troubleshooting common problems with distributed workloads for administrators","level":1,"index":3,"id":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a suspended state","level":2,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster is in a failed state","level":2,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user&#8217;s Ray cluster does not start","level":2,"index":2,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"A user cannot create a Ray cluster or submit jobs","level":2,"index":3,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":"troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}","name":"Additional resources","level":2,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-observability/"},"sections":[{"parentId":null,"name":"Enabling the observability stack","level":1,"index":0,"id":"enabling-the-observability-stack_{context}"},{"parentId":null,"name":"Collecting metrics from user workloads","level":1,"index":1,"id":"collecting-metrics-from-user-workloads_{context}"},{"parentId":null,"name":"Exporting metrics to external observability tools","level":1,"index":2,"id":"exporting-metrics-to-external-observability-tools_{context}"},{"parentId":null,"name":"Viewing traces in external tracing platforms","level":1,"index":3,"id":"viewing-traces-in-external-tracing-platforms_{context}"},{"parentId":null,"name":"Accessing built-in alerts","level":1,"index":4,"id":"accessing-built-in-alerts_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-experiments/"},"sections":[{"parentId":null,"name":"Overview of pipeline experiments","level":1,"index":0,"id":"overview-of-pipeline-experiments_{context}"},{"parentId":null,"name":"Creating a pipeline experiment","level":1,"index":1,"id":"creating-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Archiving a pipeline experiment","level":1,"index":2,"id":"archiving-a-pipeline-experiment_{context}"},{"parentId":null,"name":"Deleting an archived pipeline experiment","level":1,"index":3,"id":"deleting-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Restoring an archived pipeline experiment","level":1,"index":4,"id":"restoring-an-archived-pipeline-experiment_{context}"},{"parentId":null,"name":"Viewing pipeline task executions","level":1,"index":5,"id":"viewing-pipeline-task-executions_{context}"},{"parentId":null,"name":"Viewing pipeline artifacts","level":1,"index":6,"id":"viewing-pipeline-artifacts_{context}"},{"parentId":null,"name":"Comparing runs in an experiment","level":1,"index":7,"id":"comparing-runs-in-an-experiment_{context}"},{"parentId":null,"name":"Comparing runs in different experiments","level":1,"index":8,"id":"comparing-runs-in-different-experiments_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-pipeline-runs/"},"sections":[{"parentId":null,"name":"Overview of pipeline runs","level":1,"index":0,"id":"overview-of-pipeline-runs_{context}"},{"parentId":null,"name":"Storing data with pipelines","level":1,"index":1,"id":"storing-data-with-pipelines_{context}"},{"parentId":null,"name":"Understanding pipeline run workspaces","level":1,"index":2,"id":"configuring-pipeline-run-workspaces_{context}"},{"parentId":"configuring-pipeline-run-workspaces_{context}","name":"Configuring default workspace PVC settings in DSPA","level":2,"index":0,"id":"configuring-default-workspace-pvc-settings-in-dspa_{context}"},{"parentId":"configuring-pipeline-run-workspaces_{context}","name":"Adding external artifacts to pipeline run workspaces","level":2,"index":1,"id":"adding-external-artifacts-to-pipeline-run-workspaces_{context}"},{"parentId":null,"name":"Viewing active pipeline runs","level":1,"index":3,"id":"viewing-active-pipeline-runs_{context}"},{"parentId":null,"name":"Executing a pipeline run","level":1,"index":4,"id":"executing-a-pipeline-run_{context}"},{"parentId":null,"name":"Stopping an active pipeline run","level":1,"index":5,"id":"stopping-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an active pipeline run","level":1,"index":6,"id":"duplicating-an-active-pipeline-run_{context}"},{"parentId":null,"name":"Viewing scheduled pipeline runs","level":1,"index":7,"id":"viewing-scheduled-pipeline-runs_{context}"},{"parentId":null,"name":"Scheduling a pipeline run using a cron job","level":1,"index":8,"id":"scheduling-a-pipeline-run-using-a-cron-job_{context}"},{"parentId":null,"name":"Scheduling a pipeline run","level":1,"index":9,"id":"scheduling-a-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating a scheduled pipeline run","level":1,"index":10,"id":"duplicating-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Deleting a scheduled pipeline run","level":1,"index":11,"id":"deleting-a-scheduled-pipeline-run_{context}"},{"parentId":null,"name":"Viewing the details of a pipeline run","level":1,"index":12,"id":"viewing-the-details-of-a-pipeline-run_{context}"},{"parentId":null,"name":"Viewing archived pipeline runs","level":1,"index":13,"id":"viewing-archived-pipeline-runs_{context}"},{"parentId":null,"name":"Archiving a pipeline run","level":1,"index":14,"id":"archiving-a-pipeline-run_{context}"},{"parentId":null,"name":"Restoring an archived pipeline run","level":1,"index":15,"id":"restoring-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Deleting an archived pipeline run","level":1,"index":16,"id":"deleting-an-archived-pipeline-run_{context}"},{"parentId":null,"name":"Duplicating an archived pipeline run","level":1,"index":17,"id":"duplicating-an-archived-pipeline-run_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages-in-code-server/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your code-server workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-code-server-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your code-server workbench","level":1,"index":1,"id":"installing-python-packages-on-your-code-server-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-python-packages/"},"sections":[{"parentId":null,"name":"Viewing Python packages installed on your workbench","level":1,"index":0,"id":"viewing-python-packages-installed-on-your-workbench_{context}"},{"parentId":null,"name":"Installing Python packages on your workbench","level":1,"index":1,"id":"installing-python-packages-on-your-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-storage-classes/"},"sections":[{"parentId":null,"name":"About persistent storage","level":1,"index":0,"id":"about-persistent-storage_{context}"},{"parentId":"about-persistent-storage_{context}","name":"Storage classes in {productname-short}","level":2,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":"about-persistent-storage_{context}","name":"Access modes","level":2,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":3,"index":0,"id":"_using_shared_storage_rwx"},{"parentId":null,"name":"Configuring storage class settings","level":1,"index":1,"id":"configuring-storage-class-settings_{context}"},{"parentId":null,"name":"Configuring the default storage class for your cluster","level":1,"index":2,"id":"configuring-the-default-storage-class-for-your-cluster_{context}"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":3,"id":"overview-of-object-storage-endpoints_{context}"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_{context}","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-users-and-groups/"},"sections":[{"parentId":null,"name":"Overview of user types and permissions","level":1,"index":0,"id":"overview-of-user-types-and-permissions_{context}"},{"parentId":null,"name":"Viewing {productname-short} users","level":1,"index":1,"id":"viewing-data-science-users_{context}"},{"parentId":null,"name":"Adding users to {productname-short} user groups","level":1,"index":2,"id":"adding-users-to-user-groups_{context}"},{"parentId":null,"name":"Selecting {productname-short} administrator and user groups","level":1,"index":3,"id":"selecting-admin-and-user-groups_{context}"},{"parentId":null,"name":"Deleting users","level":1,"index":4,"id":"_deleting_users"},{"parentId":"_deleting_users","name":"About deleting users and their resources","level":2,"index":0,"id":"about-deleting-users-and-resources_{context}"},{"parentId":"_deleting_users","name":"Stopping basic workbenches owned by other users","level":2,"index":1,"id":"stopping-basic-workbenches-owned-by-other-users_{context}"},{"parentId":"_deleting_users","name":"Revoking user access to basic workbenches","level":2,"index":2,"id":"revoking-user-access-to-basic-workbenches_{context}"},{"parentId":"_deleting_users","name":"Backing up storage data","level":2,"index":3,"id":"backing-up-storage-data_{context}"},{"parentId":"_deleting_users","name":"Cleaning up after deleting users","level":2,"index":4,"id":"cleaning-up-after-deleting-users_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Overview of managing workloads with Kueue","level":1,"index":0,"id":"overview-of-managing-workloads-with-kueue_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue management states","level":2,"index":0,"id":"_kueue_management_states"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Queue enforcement for projects","level":2,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Restrictions for managing workloads with Kueue","level":2,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"},{"parentId":"overview-of-managing-workloads-with-kueue_kueue","name":"Kueue workflow","level":2,"index":3,"id":"kueue-workflow_kueue"},{"parentId":null,"name":"Configuring workload management with Kueue","level":1,"index":1,"id":"configuring-workload-management-with-kueue_kueue"},{"parentId":"configuring-workload-management-with-kueue_kueue","name":"Enabling Kueue in the dashboard","level":2,"index":0,"id":"enabling-kueue-in-the-dashboard_kueue"},{"parentId":null,"name":"Troubleshooting common problems with Kueue","level":1,"index":2,"id":"troubleshooting-common-problems-with-Kueue_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"failed to call webhook\" error message for Kueue","level":2,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":2,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"A user receives a \"local_queue provided does not exist\" error message","level":2,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"The pod provisioned by Kueue is terminated before the image is pulled","level":2,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":"troubleshooting-common-problems-with-Kueue_kueue","name":"Additional resources","level":2,"index":4,"id":"_additional_resources"},{"parentId":null,"name":"Migrating to the {rhbok-productname} Operator","level":1,"index":3,"id":"migrating-to-the-rhbok-operator_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-data-drift/"},"sections":[{"parentId":null,"name":"Creating a drift metric","level":1,"index":0,"id":"creating-a-drift-metric_drift-monitoring"},{"parentId":"creating-a-drift-metric_drift-monitoring","name":"Creating a drift metric by using the CLI","level":2,"index":0,"id":"creating-a-drift-metric-using-cli_drift-monitoring"},{"parentId":null,"name":"Deleting a drift metric by using the CLI","level":1,"index":1,"id":"deleting-a-drift-metric-by-using-cli_drift-monitoring"},{"parentId":null,"name":"Viewing drift metrics for a model","level":1,"index":2,"id":"viewing-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using drift metrics","level":1,"index":3,"id":"using-drift-metrics_drift-monitoring"},{"parentId":null,"name":"Using a drift metric in a credit card scenario","level":1,"index":4,"id":"using-a-drift-metric-in-a-credit-card-scenario_drift-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-distributed-workloads/"},"sections":[{"parentId":null,"name":"Viewing project metrics for distributed workloads","level":1,"index":0,"id":"viewing-project-metrics-for-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing the status of distributed workloads","level":1,"index":1,"id":"viewing-the-status-of-distributed-workloads_{context}"},{"parentId":null,"name":"Viewing Kueue alerts for distributed workloads","level":1,"index":2,"id":"viewing-kueue-alerts-for-distributed-workloads_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-bias/"},"sections":[{"parentId":null,"name":"Creating a bias metric","level":1,"index":0,"id":"creating-a-bias-metric_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the dashboard","level":2,"index":0,"id":"creating-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Creating a bias metric by using the CLI","level":2,"index":1,"id":"creating-a-bias-metric-using-cli_bias-monitoring"},{"parentId":"creating-a-bias-metric_bias-monitoring","name":"Duplicating a bias metric","level":2,"index":2,"id":"duplicating-a-bias-metric_bias-monitoring"},{"parentId":null,"name":"Deleting a bias metric","level":1,"index":1,"id":"deleting-a-bias-metric_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the dashboard","level":2,"index":0,"id":"deleting-a-bias-metric-using-dashboard_bias-monitoring"},{"parentId":"deleting-a-bias-metric_bias-monitoring","name":"Deleting a bias metric by using the CLI","level":2,"index":1,"id":"deleting-a-bias-metric-using-cli_bias-monitoring"},{"parentId":null,"name":"Viewing bias metrics for a model","level":1,"index":2,"id":"viewing-bias-metrics_bias-monitoring"},{"parentId":null,"name":"Using bias metrics","level":1,"index":3,"id":"using-bias-metrics_bias-monitoring"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/monitoring-model-performance/"},"sections":[{"parentId":null,"name":"Viewing performance metrics for all models on a model server","level":1,"index":0,"id":"viewing-performance-metrics-for-model-server_monitoring-model-performance"},{"parentId":null,"name":"Viewing HTTP request metrics for a deployed model","level":1,"index":1,"id":"viewing-http-request-metrics-for-a-deployed-model_monitoring-model-performance"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/overview-of-ml-features-and-feature-store/"},"sections":[{"parentId":null,"name":"Audience for Feature Store","level":1,"index":0,"id":"audience-for-feature-store_{context}"},{"parentId":null,"name":"Overview of machine learning features","level":1,"index":1,"id":"overview-of-machine-learning-features_{context}"},{"parentId":null,"name":"Overview of Feature Store","level":1,"index":2,"id":"overview-of-feature-store_{context}"},{"parentId":null,"name":"Feature Store workflow","level":1,"index":3,"id":"feature-store-workflow_{context}"},{"parentId":null,"name":"Setting up the Feature Store user interface for initial use","level":1,"index":4,"id":"setting-up-feature-store-UI_{context}"},{"parentId":null,"name":"Additional resources","level":1,"index":5,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/prepare-your-data-for-ai-consumption/"},"sections":[{"parentId":null,"name":"Process data by using Docling","level":1,"index":0,"id":"_process_data_by_using_docling"},{"parentId":null,"name":"Explore the data processing examples","level":1,"index":1,"id":"explore-the-data-processing-examples_{context}"},{"parentId":null,"name":"Automate data processing steps by building AI pipelines","level":1,"index":2,"id":"_automate_data_processing_steps_by_building_ai_pipelines"},{"parentId":null,"name":"Explore the kubeflow pipeline examples","level":1,"index":3,"id":"explore-the-kubeflow-pipeline-examples_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/preparing-the-distributed-training-environment/"},"sections":[{"parentId":null,"name":"Creating a workbench for distributed training","level":1,"index":0,"id":"creating-a-workbench-for-distributed-training_{context}"},{"parentId":null,"name":"Using the cluster server and token to authenticate","level":1,"index":1,"id":"using-the-cluster-server-and-token-to-authenticate_{context}"},{"parentId":null,"name":"Managing custom training images","level":1,"index":2,"id":"managing-custom-training-images_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"About base training images","level":2,"index":0,"id":"about-base-training-images_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"Creating a custom training image","level":2,"index":1,"id":"creating-a-custom-training-image_{context}"},{"parentId":"managing-custom-training-images_{context}","name":"Pushing an image to the integrated OpenShift image registry","level":2,"index":2,"id":"pushing-an-image-to-the-integrated-openshift-image-registry_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/retrieving-features-for-model-training/"},"sections":[{"parentId":null,"name":"Retrieving data science features","level":1,"index":0,"id":"retrieving-data-science-features_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-kfto-based-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Using the Kubeflow Training Operator to run distributed training workloads","level":1,"index":0,"id":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":2,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorchJob resource","level":2,"index":1,"id":"creating-a-kfto-pytorchjob-resource_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":2,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Training Operator PyTorch training scripts","level":2,"index":3,"id":"example-kfto-pytorch-training-scripts_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: NCCL","level":3,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: DDP","level":3,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: FSDP","level":3,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Dockerfile for a Training Operator PyTorch training script","level":2,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_{context}"},{"parentId":"using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}","name":"Example Training Operator PyTorchJob resource for multi-node training","level":2,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_{context}"},{"parentId":null,"name":"Using the Training Operator SDK to run distributed training workloads","level":1,"index":1,"id":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"Configuring a training job by using the Training Operator SDK","level":2,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"Running a training job by using the Training Operator SDK","level":2,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":"using-the-kubeflow-sdk-to-run-distributed-training-workloads_{context}","name":"TrainingClient API: Job-related methods","level":2,"index":2,"id":"ref-trainingclient-api-job-related-methods_{context}"},{"parentId":null,"name":"Fine-tuning a model by using Kubeflow Training","level":1,"index":2,"id":"fine-tuning-a-model-by-using-kubeflow-training_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Configuring the fine-tuning job","level":2,"index":0,"id":"configuring-the-fine-tuning-job_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Running the fine-tuning job","level":2,"index":1,"id":"running-the-fine-tuning-job_{context}"},{"parentId":"fine-tuning-a-model-by-using-kubeflow-training_{context}","name":"Deleting the fine-tuning job","level":2,"index":2,"id":"deleting-the-fine-tuning-job_{context}"},{"parentId":null,"name":"Creating a multi-node PyTorch training job with RDMA","level":1,"index":3,"id":"creating-a-multi-node-pytorch-training-job-with-rdma_{context}"},{"parentId":null,"name":"Example Training Operator PyTorchJob resource configured to run with RDMA","level":1,"index":4,"id":"ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/running-ray-based-distributed-workloads/"},"sections":[{"parentId":null,"name":"Running distributed data science workloads from Jupyter notebooks","level":1,"index":0,"id":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Downloading the demo Jupyter notebooks from the CodeFlare SDK","level":2,"index":0,"id":"downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Running the demo Jupyter notebooks from the CodeFlare SDK","level":2,"index":1,"id":"running-the-demo-jupyter-notebooks-from-the-codeflare-sdk_{context}"},{"parentId":"running-distributed-data-science-workloads-from-jupyter-notebooks_{context}","name":"Managing Ray clusters from within a Jupyter notebook","level":2,"index":2,"id":"managing-ray-clusters-from-within-a-jupyter-notebook_{context}"},{"parentId":null,"name":"Running distributed data science workloads from AI pipelines","level":1,"index":1,"id":"running-distributed-data-science-workloads-from-ai-pipelines_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/set-up-your-working-environment/"},"sections":[{"parentId":null,"name":"About the {org-name} Python Index","level":1,"index":0,"id":"about-the-python-index_{context}"},{"parentId":null,"name":"Mirror the Python Index for your disconnected environment","level":1,"index":1,"id":"mirror-the-python-index_{context}"},{"parentId":null,"name":"Install packages","level":1,"index":2,"id":"install-packages_{context}"},{"parentId":null,"name":"Import example notebooks","level":1,"index":3,"id":"import-example-notebooks_{context}"},{"parentId":"import-example-notebooks_{context}","name":"Clone an example Git repository","level":2,"index":0,"id":"clone-an-example-git-repository_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-feature-store/"},"sections":[{"parentId":null,"name":"Before you begin","level":1,"index":0,"id":"before-you-begin_{context}"},{"parentId":null,"name":"Enabling the Feature Store component","level":1,"index":1,"id":"enabling-the-feature-store-component_{context}"},{"parentId":null,"name":"Creating a Feature Store instance in a project","level":1,"index":2,"id":"creating-a-feature-store-instance-in-a-project_{context}"},{"parentId":null,"name":"Configuring and managing Role Based Access Control","level":1,"index":3,"id":"configuring-and-managing-role-based-access-control_{context}"},{"parentId":null,"name":"Adding feature definitions and initializing your Feature Store instance","level":1,"index":4,"id":"adding-feature-definitions-and-initializing-your-feature-store-instance_{context}"},{"parentId":"adding-feature-definitions-and-initializing-your-feature-store-instance_{context}","name":"Specifying files to ignore","level":2,"index":0,"id":"specifying-files-to-ignore_{context}"},{"parentId":null,"name":"Viewing Feature Store objects in the web-based UI","level":1,"index":5,"id":"viewing-feature-store-objects-in-the-web-based-ui_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/setting-up-trustyai-for-your-project/"},"sections":[{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":0,"id":"authenticating-trustyai-service_{context}"},{"parentId":null,"name":"Uploading training data to TrustyAI","level":1,"index":1,"id":"uploading-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Sending training data to TrustyAI","level":1,"index":2,"id":"sending-training-data-to-trustyai_{context}"},{"parentId":null,"name":"Labeling data fields","level":1,"index":3,"id":"labeling-data-fields_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/train-the-model-by-using-your-prepared-data/"},"sections":[{"parentId":null,"name":"Explore the Training Hub examples","level":1,"index":0,"id":"explore-the-training-hub-examples_{context}"},{"parentId":null,"name":"Estimate memory usage","level":1,"index":1,"id":"estimate-memory-usage_{context}"},{"parentId":null,"name":"Compare the performance of OSFT and SFT training algorithms","level":1,"index":2,"id":"compare-the-performance-of-osft-and-sft_{context}"},{"parentId":null,"name":"Distribute training jobs by using the KubeFlow Trainer Operator","level":1,"index":3,"id":"_distribute_training_jobs_by_using_the_kubeflow_trainer_operator"},{"parentId":null,"name":"Distributed fine-tuning with Training Hub and Kubeflow Trainer","level":1,"index":4,"id":"_distributed_fine_tuning_with_training_hub_and_kubeflow_trainer"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v1-to-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 1","level":1,"index":0,"id":"requirements-for-upgrading-odh-v1_upgradev1"},{"parentId":null,"name":"Upgrading the Open Data Hub Operator","level":1,"index":1,"id":"upgrading-the-odh-operator_upgradev1"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/upgrading-odh-v2/"},"sections":[{"parentId":null,"name":"Requirements for upgrading {productname-short} version 2","level":1,"index":0,"id":"requirements-for-upgrading-odh-v2_upgradev2"},{"parentId":null,"name":"Upgrading the Open Data Hub Operator","level":1,"index":1,"id":"upgrading-the-odh-operator_upgradev2"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-basic-workbenches/"},"sections":[{"parentId":null,"name":"Starting a basic workbench","level":1,"index":0,"id":"starting-a-basic-workbench_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-connections/"},"sections":[{"parentId":null,"name":"Adding a connection to your project","level":1,"index":0,"id":"adding-a-connection-to-your-project_{context}"},{"parentId":null,"name":"Updating a connection","level":1,"index":1,"id":"updating-a-connection_{context}"},{"parentId":null,"name":"Deleting a connection","level":1,"index":2,"id":"deleting-a-connection_{context}"},{"parentId":null,"name":"Using the connections API","level":1,"index":3,"id":"using-connections-api_{context}"},{"parentId":"using-connections-api_{context}","name":"Namespace isolation in connections API","level":2,"index":0,"id":"_namespace_isolation_in_connections_api"},{"parentId":"using-connections-api_{context}","name":"Role-based access control (RBAC) requirements in connections API","level":2,"index":1,"id":"_role_based_access_control_rbac_requirements_in_connections_api"},{"parentId":"using-connections-api_{context}","name":"Validation scope","level":2,"index":2,"id":"_validation_scope"},{"parentId":"using-connections-api_{context}","name":"Using connection annotations based on workload type","level":2,"index":3,"id":"_using_connection_annotations_based_on_workload_type"},{"parentId":"using-connections-api_{context}","name":"Creating an Amazon S3-compatible connection type using the connections API","level":2,"index":4,"id":"creating-s3-compatible-connection-type-api_{context}"},{"parentId":"creating-s3-compatible-connection-type-api_{context}","name":"Using an Amazon S3 connection with <code>InferenceService</code> custom resource","level":3,"index":0,"id":"_using_an_amazon_s3_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-s3-compatible-connection-type-api_{context}","name":"Using an Amazon S3 connection with <code>LLMInferenceService</code> custom resource","level":3,"index":1,"id":"_using_an_amazon_s3_connection_with_llminferenceservice_custom_resource"},{"parentId":"using-connections-api_{context}","name":"Creating a URI-compatible connection type using the connections API","level":2,"index":5,"id":"creating-uri-compatible-connection-type-api_{context}"},{"parentId":"creating-uri-compatible-connection-type-api_{context}","name":"Using a URI connection with <code>InferenceService</code> custom resource","level":3,"index":0,"id":"_using_a_uri_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-uri-compatible-connection-type-api_{context}","name":"Using a URI connection with <code>LLMInferenceService</code> custom resource","level":3,"index":1,"id":"_using_a_uri_connection_with_llminferenceservice_custom_resource"},{"parentId":"using-connections-api_{context}","name":"Creating an OCI-compatible connection type using the connections API","level":2,"index":6,"id":"creating-oci-compatible-connection-type-api_{context}"},{"parentId":"creating-oci-compatible-connection-type-api_{context}","name":"Using an OCI connection with <code>InferenceService</code> custom resource","level":3,"index":0,"id":"_using_an_oci_connection_with_inferenceservice_custom_resource"},{"parentId":"creating-oci-compatible-connection-type-api_{context}","name":"Using an OCI connection with <code>LLMInferenceService</code> custom resource","level":3,"index":1,"id":"_using_an_oci_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-explainability/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation","level":1,"index":0,"id":"requesting-a-lime-explanation_explainers"},{"parentId":"requesting-a-lime-explanation_explainers","name":"Requesting a LIME explanation by using the CLI","level":2,"index":0,"id":"requesting-a-lime-explanation-using-CLI_explainers"},{"parentId":null,"name":"Requesting a SHAP explanation","level":1,"index":1,"id":"requesting-a-shap-explanation_explainers"},{"parentId":"requesting-a-shap-explanation_explainers","name":"Requesting a SHAP explanation by using the CLI","level":2,"index":0,"id":"requesting-a-shap-explanation-using-CLI_explainers"},{"parentId":null,"name":"Using explainers","level":1,"index":2,"id":"using-explainers_explainers"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-guardrails-for-ai-safety/"},"sections":[{"parentId":null,"name":"Detecting PII and sensitive data","level":1,"index":0,"id":"_detecting_pii_and_sensitive_data"},{"parentId":null,"name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":1,"index":1,"id":"detecting-pii-by-using-guardrails-with-llama-stack_{context}"},{"parentId":null,"name":"Filtering flagged content by sending requests to the regex detector","level":1,"index":2,"id":"filtering-flagged-content-by-sending-requests-to-the-regex-detector_{context}"},{"parentId":null,"name":"Securing prompts","level":1,"index":3,"id":"_securing_prompts"},{"parentId":null,"name":"Mitigating Prompt Injection by using a Hugging Face Prompt Injection detector","level":1,"index":4,"id":"mitigating-prompt-injection-by-using-a-hugging-face-prompt-injection-detector_{context}"},{"parentId":null,"name":"Moderating and safeguarding content","level":1,"index":5,"id":"_moderating_and_safeguarding_content"},{"parentId":null,"name":"Detecting hateful and profane language","level":1,"index":6,"id":"detecting-hateful-and-profane-language_{context}"},{"parentId":null,"name":"Enforcing configured safety pipelines for LLM inference by using Guardrails Gateway","level":1,"index":7,"id":"enforcing-configured-safety-pipelines-for-llm-inference-using-guardrails-gateway_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-llama-stack-with-trustyai/"},"sections":[{"parentId":null,"name":"Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI","level":1,"index":0,"id":"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_{context}"},{"parentId":null,"name":"Running custom evaluations with LM-Eval and Llama Stack","level":1,"index":1,"id":"running-custom-evaluations-with-LMEval-and-llama-stack_{context}"},{"parentId":null,"name":"Detecting personally identifiable information (PII) by using Guardrails with Llama Stack","level":1,"index":2,"id":"detecting-pii-by-using-guardrails-with-llama-stack_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-project-workbenches/"},"sections":[{"parentId":null,"name":"Creating a workbench and selecting an IDE","level":1,"index":0,"id":"creating-a-workbench-select-ide_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"About workbench images","level":2,"index":0,"id":"about-workbench-images_{context}"},{"parentId":"creating-a-workbench-select-ide_{context}","name":"Creating a workbench","level":2,"index":1,"id":"creating-a-project-workbench_{context}"},{"parentId":null,"name":"Starting a workbench","level":1,"index":1,"id":"starting-a-workbench_{context}"},{"parentId":null,"name":"Updating a project workbench","level":1,"index":2,"id":"updating-a-project-workbench_{context}"},{"parentId":null,"name":"Deleting a workbench from a project","level":1,"index":3,"id":"deleting-a-workbench-from-a-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-projects/"},"sections":[{"parentId":null,"name":"Creating a project","level":1,"index":0,"id":"creating-a-project_{context}"},{"parentId":null,"name":"Updating a project","level":1,"index":1,"id":"updating-a-project_{context}"},{"parentId":null,"name":"Deleting a project","level":1,"index":2,"id":"deleting-a-project_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-the-kfto-sdk-to-run-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Configuring a training job by using the Training Operator SDK","level":1,"index":0,"id":"configuring-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":null,"name":"Running a training job by using the Training Operator SDK","level":1,"index":1,"id":"running-a-training-job-by-using-the-kfto-sdk_{context}"},{"parentId":null,"name":"TrainingClient API: Job-related methods","level":1,"index":2,"id":"ref-trainingclient-api-job-related-methods_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/using-the-kubeflow-training-operator-to-run-distributed-training-workloads/"},"sections":[{"parentId":null,"name":"Creating a Training Operator PyTorch training script ConfigMap resource","level":1,"index":0,"id":"creating-a-kfto-pytorch-training-script-configmap-resource_{context}"},{"parentId":null,"name":"Creating a Training Operator PyTorchJob resource","level":1,"index":1,"id":"creating-a-kfto-pytorchjob-resource_{context}"},{"parentId":null,"name":"Creating a Training Operator PyTorchJob resource by using the CLI","level":1,"index":2,"id":"creating-a-kfto-pytorchjob-resource-by-using-the-cli_{context}"},{"parentId":null,"name":"Example Training Operator PyTorch training scripts","level":1,"index":3,"id":"example-kfto-pytorch-training-scripts_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: NCCL","level":2,"index":0,"id":"ref-example-kfto-pytorch-training-script-nccl{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: DDP","level":2,"index":1,"id":"ref-example-kfto-pytorch-training-script-ddp_{context}"},{"parentId":"example-kfto-pytorch-training-scripts_{context}","name":"Example Training Operator PyTorch training script: FSDP","level":2,"index":2,"id":"ref-example-kfto-pytorch-training-script-fsdp_{context}"},{"parentId":null,"name":"Example Dockerfile for a Training Operator PyTorch training script","level":1,"index":4,"id":"ref-example-dockerfile-for-a-kfto-pytorch-training-script_{context}"},{"parentId":null,"name":"Example Training Operator PyTorchJob resource for multi-node training","level":1,"index":5,"id":"ref-example-kfto-pytorchjob-resource-for-multi-node-training_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/viewing-logs-and-audit-records/"},"sections":[{"parentId":null,"name":"Configuring the {productname-short} Operator logger","level":1,"index":0,"id":"configuring-the-operator-logger_{context}"},{"parentId":"configuring-the-operator-logger_{context}","name":"Viewing the {productname-short} Operator logs","level":2,"index":0,"id":"_viewing_the_productname_short_operator_logs"},{"parentId":null,"name":"Viewing audit records","level":1,"index":1,"id":"viewing-audit-records_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-certificates/"},"sections":[{"parentId":null,"name":"Understanding how {productname-short} handles certificates","level":1,"index":0,"id":"understanding-certificates_certs"},{"parentId":null,"name":"Adding certificates","level":1,"index":1,"id":"_adding_certificates"},{"parentId":null,"name":"Adding certificates to a cluster-wide CA bundle","level":1,"index":2,"id":"adding-certificates-to-a-cluster-ca-bundle_certs"},{"parentId":null,"name":"Adding certificates to a custom CA bundle","level":1,"index":3,"id":"adding-certificates-to-a-custom-ca-bundle_certs"},{"parentId":null,"name":"Using self-signed certificates with {productname-short} components","level":1,"index":4,"id":"_using_self_signed_certificates_with_productname_short_components"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Accessing S3-compatible object storage with self-signed certificates","level":2,"index":0,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Configuring a certificate for pipelines","level":2,"index":1,"id":"configuring-a-certificate-for-pipelines_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Configuring a certificate for workbenches","level":2,"index":2,"id":"configuring-a-certificate-for-workbenches_certs"},{"parentId":"_using_self_signed_certificates_with_productname_short_components","name":"Using the cluster-wide CA bundle for the model serving platform","level":2,"index":3,"id":"using-the-cluster-CA-bundle-for-model-serving_certs"},{"parentId":null,"name":"Managing certificates without the {productname-long} Operator","level":1,"index":5,"id":"managing-certificates-without-the-operator_certs"},{"parentId":null,"name":"Removing the CA bundle","level":1,"index":6,"id":"_removing_the_ca_bundle"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from all namespaces","level":2,"index":0,"id":"removing-the-ca-bundle-from-all-namespaces_certs"},{"parentId":"_removing_the_ca_bundle","name":"Removing the CA bundle from a single namespace","level":2,"index":1,"id":"removing-the-ca-bundle-from-a-single-namespace_certs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-data-in-s3-compatible-object-store/"},"sections":[{"parentId":null,"name":"Prerequisites","level":1,"index":0,"id":"s3-prerequisites_s3"},{"parentId":null,"name":"Creating an S3 client","level":1,"index":1,"id":"creating-an-s3-client_s3"},{"parentId":null,"name":"Listing available buckets in your object store","level":1,"index":2,"id":"listing-available-amazon-buckets_s3"},{"parentId":null,"name":"Creating a bucket in your object store","level":1,"index":3,"id":"creating-an-s3-bucket_s3"},{"parentId":null,"name":"Listing files in your bucket","level":1,"index":4,"id":"listing-files-in-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Downloading files from your bucket","level":1,"index":5,"id":"downloading-files-from-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Uploading files to your bucket","level":1,"index":6,"id":"uploading-files-to-available-amazon-s3-buckets-using-notebook-cells_s3"},{"parentId":null,"name":"Copying files between buckets","level":1,"index":7,"id":"copying-files-to-between-buckets_s3"},{"parentId":null,"name":"Deleting files from your bucket","level":1,"index":8,"id":"Deleting-files-on-your-object-store_s3"},{"parentId":null,"name":"Deleting a bucket from your object store","level":1,"index":9,"id":"deleting-a-s3-bucket_s3"},{"parentId":null,"name":"Overview of object storage endpoints","level":1,"index":10,"id":"overview-of-object-storage-endpoints_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"MinIO (On-Cluster)","level":2,"index":0,"id":"_minio_on_cluster"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Amazon S3","level":2,"index":1,"id":"_amazon_s3"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Other S3-Compatible Object Stores","level":2,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":"overview-of-object-storage-endpoints_s3","name":"Verification and Troubleshooting","level":2,"index":3,"id":"_verification_and_troubleshooting"},{"parentId":null,"name":"Accessing S3-compatible object storage with self-signed certificates","level":1,"index":11,"id":"accessing-s3-compatible-object-storage-with-self-signed-certificates_s3"},{"parentId":null,"name":"Additional resources","level":0,"index":12,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-model-registries/"},"sections":[{"parentId":null,"name":"Registering a model from the dashboard","level":1,"index":0,"id":"registering-a-model_model-registry"},{"parentId":null,"name":"Registering a model version","level":1,"index":1,"id":"registering-a-model-version_model-registry"},{"parentId":null,"name":"Viewing registered models","level":1,"index":2,"id":"viewing-registered-models_model-registry"},{"parentId":null,"name":"Viewing registered model versions","level":1,"index":3,"id":"viewing-registered-model-versions_model-registry"},{"parentId":null,"name":"Editing model metadata in a model registry","level":1,"index":4,"id":"editing-model-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Editing model version metadata in a model registry","level":1,"index":5,"id":"editing-model-version-metadata-in-a-model-registry_model-registry"},{"parentId":null,"name":"Deploying a model version from a model registry","level":1,"index":6,"id":"deploying-a-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Editing the deployment properties of a deployed model version from a model registry","level":1,"index":7,"id":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry"},{"parentId":"_editing_the_deployment_properties_of_a_deployed_model_version_from_a_model_registry","name":"Editing the deployment properties of a model deployed by using the model serving platform","level":2,"index":0,"id":"editing-the-deployment-properties-of-a-model-deployed-by-using-the-model-serving-platform_model-registry"},{"parentId":null,"name":"Deleting a deployed model version from a model registry","level":1,"index":8,"id":"deleting-a-deployed-model-version-from-a-model-registry_model-registry"},{"parentId":null,"name":"Archiving a model","level":1,"index":9,"id":"archiving-a-model_model-registry"},{"parentId":null,"name":"Archiving a model version","level":1,"index":10,"id":"archiving-a-model-version_model-registry"},{"parentId":null,"name":"Restoring a model","level":1,"index":11,"id":"restoring-a-model_model-registry"},{"parentId":null,"name":"Restoring a model version","level":1,"index":12,"id":"restoring-a-model-version_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipeline-logs/"},"sections":[{"parentId":null,"name":"About pipeline logs","level":1,"index":0,"id":"about-pipeline-logs_{context}"},{"parentId":null,"name":"Viewing pipeline step logs","level":1,"index":1,"id":"viewing-pipeline-step-logs_{context}"},{"parentId":null,"name":"Downloading pipeline step logs","level":1,"index":2,"id":"downloading-pipeline-step-logs_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-pipelines-in-jupyterlab/"},"sections":[{"parentId":null,"name":"Overview of pipelines in JupyterLab","level":1,"index":0,"id":"overview-of-pipelines-in-jupyterlab_{context}"},{"parentId":null,"name":"Accessing the pipeline editor","level":1,"index":1,"id":"accessing-the-pipeline-editor_{context}"},{"parentId":null,"name":"Disabling node caching in Elyra","level":1,"index":2,"id":"disabling-node-caching-in-elyra_{context}"},{"parentId":null,"name":"Creating a runtime configuration","level":1,"index":3,"id":"creating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Updating a runtime configuration","level":1,"index":4,"id":"updating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Deleting a runtime configuration","level":1,"index":5,"id":"deleting-a-runtime-configuration_{context}"},{"parentId":null,"name":"Duplicating a runtime configuration","level":1,"index":6,"id":"duplicating-a-runtime-configuration_{context}"},{"parentId":null,"name":"Running a pipeline in JupyterLab","level":1,"index":7,"id":"running-a-pipeline-in-jupyterlab_{context}"},{"parentId":null,"name":"Exporting a pipeline in JupyterLab","level":1,"index":8,"id":"exporting-a-pipeline-in-jupyterlab_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/working-with-the-model-catalog/"},"sections":[{"parentId":null,"name":"Discovering and evaluating models in the model catalog","level":1,"index":0,"id":"viewing-models-in-the-catalog_model-registry"},{"parentId":null,"name":"Registering a model from the model catalog","level":1,"index":1,"id":"registering-a-model-from-the-model-catalog_model-registry"},{"parentId":null,"name":"Deploying a model from the model catalog","level":1,"index":2,"id":"deploying-a-model-from-the-model-catalog_model-registry"},{"parentId":null,"name":"Configuring model catalog sources in OpenShift","level":1,"index":3,"id":"configuring-model-catalog-sources-in-openshift_model-registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-centralized-auth-oidc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-feature-definitions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-kserve-deployment-modes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-llama-stack-search-types/"},"sections":[{"parentId":null,"name":"Supported search modes","level":1,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":2,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":2,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":2,"index":2,"id":"_hybrid_search"},{"parentId":null,"name":"Retrieval database support","level":1,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Model serving platform","level":1,"index":0,"id":"_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":1,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-organizing-features-by-using-entities/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-persistent-storage/"},"sections":[{"parentId":null,"name":"Storage classes in {productname-short}","level":1,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":null,"name":"Access modes","level":1,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":2,"index":0,"id":"_using_shared_storage_rwx"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-ai-assets-endpoints-page/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-the-python-index/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-authentication-token-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-built-in-alerts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-hugging-face-models-with-an-environment-variable-token/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-inference-endpoint-for-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-administration-interface-for-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/activating-the-llama-stack-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-custom-model-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-connection-to-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-a-tested-and-verified-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-certificates-to-a-cluster-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-cluster-storage-to-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-certificates-to-a-custom-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-external-artifacts-to-pipeline-run-workspaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-feature-definitions-and-initializing-your-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-workbench-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/allocating-additional-resources-to-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-custom-image-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-workbench-overview/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/api-workbench-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/audience-for-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/auth-on-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-kfp-sdk-with-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/before-you-begin/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/backing-up-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/benchmarking-embedding-models-with-beir-datasets-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/clone-an-example-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/collecting-metrics-from-user-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/compare-the-performance-of-osft-and-sft/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/compiling-kubernetes-native-manifests-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/compiling-the-pipeline-yaml-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-certificate-for-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-certificate-for-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-cluster-for-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-playground-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-a-recommended-accelerator-for-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-offline-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-online-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-and-managing-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-authentication-for-llmd/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-default-workspace-pvc-settings-in-dspa/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-mcp-servers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-metric-based-autoscaling/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-model-catalog-sources-in-openshift/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-oidc-auth-gateway-api/"},"sections":[{"parentId":null,"name":"Security considerations","level":1,"index":0,"id":"_security_considerations"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-pipelines-with-your-own-argo-workflows-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-ragas-remote-provider-for-production/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-built-in-detector-and-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-feature-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator logs","level":1,"index":0,"id":"_viewing_the_productname_short_operator_logs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-workload-management-with-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/controlling-caching-in-pipelines/"},"sections":[{"parentId":null,"name":"Disabling caching for individual tasks","level":1,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":null,"name":"Disabling caching for a pipeline at submit time","level":1,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":null,"name":"Disabling caching for a pipeline at compile time","level":1,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":null,"name":"Disabling caching for all pipelines (pipeline server)","level":1,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-feature-store-instance-in-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorch-training-script-configmap-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-kfto-pytorchjob-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-multi-node-pytorch-training-job-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-for-distributed-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-feature-views/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-oci-compatible-connection-types-api/"},"sections":[{"parentId":null,"name":"Using an OCI connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_an_oci_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using an OCI connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_an_oci_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-project-scoped-resources-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-s3-compatible-connection-type-api/"},"sections":[{"parentId":null,"name":"Using an Amazon S3 connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_an_amazon_s3_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using an Amazon S3 connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_an_amazon_s3_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-project-scoped-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/creating-uri-compatible-connection-type-api/"},"sections":[{"parentId":null,"name":"Using a URI connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_a_uri_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using a URI connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_a_uri_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-model-selection-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/customizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/defining-a-pipeline-by-using-the-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-drift-metric-by-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-playground-from-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-a-workbench-from-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-cluster-storage-from-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deleting-files-in-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-llama-model-with-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-llamastackdistribution-instance/"},"sections":[{"parentId":null,"name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":1,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":null,"name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":1,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":null,"name":"Example C: LlamaStackDistribution with <strong>Inline FAISS</strong>","level":1,"index":2,"id":"_example_c_llamastackdistribution_with_inline_faiss"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-remote-milvus-vector-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-model-stored-in-oci-image-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-on-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-distributed-inference/"},"sections":[{"parentId":null,"name":"Configuring authentication for {llmd} using {org-name} Connectivity Link","level":1,"index":0,"id":"configuring-authentication-for-llmd_{context}"},{"parentId":null,"name":"Enabling {llmd}","level":1,"index":1,"id":"enabling-distributed-inference_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-inference-service-for-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/configuring-an-inference-service-for-spyre/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/deploying-vllm-gpu-metrics-dashboard-grafana/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/detecting-hateful-and-profane-language/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/detecting-pii-by-using-guardrails-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/determining-gpu-requirements-for-llm-powered-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/disabling-node-caching-in-elyra/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-an-existing-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-dashboard-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/emptying-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-automatic-authentication-and-publishing-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-distributed-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-kueue-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-feature-store-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-the-observability-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enabling-trustyai-kserve-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/end-to-end-model-customization-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/estimate-memory-usage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/evaluating-rag-system-quality-with-ragas/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/explore-the-data-processing-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/explore-the-kubeflow-pipeline-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/explore-the-sdg-hub-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/explore-the-training-hub-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-metrics-to-external-observability-tools/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/exporting-your-playground-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/feature-store-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/glossary-of-common-terms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/granting-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-auto-config/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-configuring-the-hugging-face-detector-serving-runtime/"},"sections":[{"parentId":null,"name":"Guardrails Detector Hugging Face serving runtime configuration values","level":1,"index":0,"id":"_guardrails_detector_hugging_face_serving_runtime_configuration_values"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-enforcing-configured-safety-pipelines-for-llm-inference-using-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-filtering-flagged-content-by-sending-requests-to-the-regex-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-gateway-config-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-configmap-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-detectors/"},"sections":[{"parentId":null,"name":"Built-in Detector","level":1,"index":0,"id":"_built_in_detector"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guardrails-orchestrator-parameters/"},"sections":[{"parentId":null,"name":"Gateway Pa","level":0,"index":0,"id":"_gateway_pa"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guided-example-build-a-kfp-pipeline-for-sdg/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/guidelines-for-metrics-based-autoscaling/"},"sections":[{"parentId":null,"name":"Choosing metrics for latency and throughput-optimized scaling","level":1,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":null,"name":"Choosing the right sliding window","level":1,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":null,"name":"Optimizing HPA scale-down configuration","level":1,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":null,"name":"Considering model size for optimal scaling","level":1,"index":3,"id":"_considering_model_size_for_optimal_scaling"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/hiding-the-default-basic-workbench-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ibm-spyre-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/import-example-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/importing-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/inference-performance-metrics/"},"sections":[{"parentId":null,"name":"Latency","level":1,"index":0,"id":"_latency"},{"parentId":null,"name":"Throughput","level":1,"index":1,"id":"_throughput"},{"parentId":null,"name":"Cost per million tokens","level":1,"index":2,"id":"_cost_per_million_tokens"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ingesting-content-into-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/install-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-python-packages-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/kueue-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/llama-stack-apis/"},"sections":[{"parentId":null,"name":"Supported Llama Stack APIs in {productname-short}","level":1,"index":0,"id":"_supported_llama_stack_apis_in_productname_short"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Agents API","level":2,"index":0,"id":"_agents_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Datasets_IO API","level":2,"index":1,"id":"_datasets_io_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Evaluation API","level":2,"index":2,"id":"_evaluation_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Inference API","level":2,"index":3,"id":"_inference_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Safety API","level":2,"index":4,"id":"_safety_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Tool Runtime API","level":2,"index":5,"id":"_tool_runtime_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Vector_IO API","level":2,"index":6,"id":"_vector_io_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/llama-stack-providers-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job-properties/"},"sections":[{"parentId":null,"name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":1,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-features-available-for-real-time-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/making-inference-requests-to-models-deployed-on-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-certificates-without-the-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/migrating-pipelines-from-database-to-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/migrating-to-the-rhbok-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/mirror-the-python-index/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/mitigating-prompt-injection-by-using-a-hugging-face-prompt-injection-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/model-serving-runtimes-for-accelerators/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"},{"parentId":null,"name":"IBM Spyre AI accelerators on x86 and IBM Z","level":1,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86_and_ibm_z"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/model-serving-runtimes/"},"sections":[{"parentId":null,"name":"ServingRuntime","level":1,"index":0,"id":"_servingruntime"},{"parentId":null,"name":"InferenceService","level":1,"index":1,"id":"_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/next-steps-playground/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/openai-compatibility-for-rag-apis-in-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/openai-compatible-apis-in-llama-stack/"},"sections":[{"parentId":null,"name":"Supported OpenAI-compatible APIs in {productname-short}","level":1,"index":0,"id":"_supported_openai_compatible_apis_in_productname_short"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Chat Completions API","level":2,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Completions API","level":2,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Embeddings API","level":2,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Files API","level":2,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Stores API","level":2,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Store Files API","level":2,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Models API","level":2,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Responses API","level":2,"index":7,"id":"_responses_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-distributed-workloads/"},"sections":[{"parentId":null,"name":"Distributed workloads infrastructure","level":1,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":null,"name":"Types of distributed workloads","level":1,"index":1,"id":"_types_of_distributed_workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-evaluating-ai-systems/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-faiss-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Kueue management states","level":1,"index":0,"id":"_kueue_management_states"},{"parentId":null,"name":"Queue enforcement for projects","level":1,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":null,"name":"Restrictions for managing workloads with Kueue","level":1,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-milvus-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-ml-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-model-registries/"},"sections":[{"parentId":null,"name":"Model catalog","level":1,"index":0,"id":"_model_catalog"},{"parentId":null,"name":"Model registry","level":1,"index":1,"id":"_model_registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-monitoring-your-ai-systems/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-rag/"},"sections":[{"parentId":null,"name":"Audience for RAG","level":1,"index":0,"id":"_audience_for_rag"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-the-model-customization-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/overview-of-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/performance-considerations-for-doc-apps/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/playground-prerequisites/"},"sections":[{"parentId":null,"name":"Cluster administrator prerequisites","level":1,"index":0,"id":"_cluster_administrator_prerequisites"},{"parentId":null,"name":"User prerequisites","level":1,"index":1,"id":"_user_prerequisites"},{"parentId":null,"name":"Model and runtime requirements for the playground","level":1,"index":2,"id":"_model_and_runtime_requirements_for_the_playground"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Key model selection factors","level":2,"index":0,"id":"_key_model_selection_factors"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Example model configuration","level":2,"index":1,"id":"_example_model_configuration"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preparing-documents-with-docling-for-llama-stack-retrieval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/querying-ingested-content-in-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-accelerator-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-cpu-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-distributed-inference/"},"sections":[{"parentId":null,"name":"Single-node GPU deployment","level":1,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":null,"name":"Multi-node deployment","level":1,"index":1,"id":"_multi_node_deployment"},{"parentId":null,"name":"Intelligent inference scheduler with KV cache routing","level":1,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-default-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-ddp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-nccl/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorch-training-script-fsdp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kubernetes-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-oidc-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-example-pvc-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-grafana-metrics/"},"sections":[{"parentId":null,"name":"Accelerator metrics","level":1,"index":0,"id":"ref-accelerator-metrics_{context}"},{"parentId":null,"name":"CPU metrics","level":1,"index":1,"id":"ref-cpu-metrics_{context}"},{"parentId":null,"name":"vLLM metrics","level":1,"index":2,"id":"ref-vllm-metrics_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":1,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":1,"index":2,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":1,"index":3,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM AMD GPU ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":1,"index":5,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre s390x ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_spyre_s390x_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Seldon MLServer","level":1,"index":8,"id":"_seldon_mlserver"},{"parentId":null,"name":"Additional resources","level":1,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-trainingclient-api-job-related-methods/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/ref-vllm-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-the-ca-bundle-from-a-single-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/removing-the-ca-bundle-from-all-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/retrieving-data-science-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/revoking-user-access-to-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-a-workload-with-a-kueue-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-custom-evaluations-with-LMEval-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-ai-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-distributed-data-science-workloads-from-jupyter-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/running-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-feature-store-UI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval-s3-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-ragas-inline-provider/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/showing-hiding-information-about-available-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/setting-up-your-working-environment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/specifying-files-to-ignore/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/specifying-the-data-source-for-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-basic-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-idle-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/stopping-starting-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/storing-data-with-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/support-philosophy/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-baseline-model-responses/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-your-model-with-rag/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-with-model-control-protocol-servers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-workbenches-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s workbench does not start","level":1,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/testing-your-vllm-model-endpoints/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-gateway-api/"},"sections":[{"parentId":null,"name":"The <code>GatewayConfig</code> status shows as not ready","level":1,"index":0,"id":"_the_gatewayconfig_status_shows_as_not_ready"},{"parentId":null,"name":"Authentication proxy fails to start","level":1,"index":1,"id":"_authentication_proxy_fails_to_start"},{"parentId":null,"name":"The Gateway is inaccessible","level":1,"index":2,"id":"_the_gateway_is_inaccessible"},{"parentId":null,"name":"The OIDC authentication fails","level":1,"index":3,"id":"_the_oidc_authentication_fails"},{"parentId":null,"name":"The dashboard is not accessible after authentication","level":1,"index":4,"id":"_the_dashboard_is_not_accessible_after_authentication"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-in-workbenches-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":2,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":3,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-Kueue/"},"sections":[{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for Kueue","level":1,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":1,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a \"local_queue provided does not exist\" error message","level":1,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"The pod provisioned by Kueue is terminated before the image is pulled","level":1,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for Kueue","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster does not start","level":1,"index":3,"id":"_my_ray_cluster_does_not_start"},{"parentId":null,"name":"I see a \"Default Local Queue not found\" error message","level":1,"index":4,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a \"local_queue provided does not exist\" error message","level":1,"index":5,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":6,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":7,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSPA components","level":1,"index":0,"id":"_common_errors_across_dspa_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/troubleshooting-playground-issues/"},"sections":[{"parentId":null,"name":"The chatbot thinks indefinitely","level":1,"index":0,"id":"_the_chatbot_thinks_indefinitely"},{"parentId":null,"name":"The model does not use RAG data","level":1,"index":1,"id":"_the_model_does_not_use_rag_data"},{"parentId":null,"name":"MCP servers are missing from the UI","level":1,"index":2,"id":"_mcp_servers_are_missing_from_the_ui"},{"parentId":null,"name":"The model fails to call MCP tools","level":1,"index":3,"id":"_the_model_fails_to_call_mcp_tools"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-pipeline-run-workspaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-rag-evaluation-providers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/understanding-rag-settings/"},"sections":[{"parentId":null,"name":"Understanding RAG settings","level":1,"index":0,"id":"understanding-rag-settings_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-lmeval-job-configuration-using-the-web-console/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-workbench-settings-by-restarting-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/playground-overview/"},"sections":[{"parentId":null,"name":"Core capabilities","level":1,"index":0,"id":"_core_capabilities"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/performing-model-evaluations-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-playground-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/upgrading-the-odh-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-model-files-to-pvc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/uploading-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-custom-unitxt-card/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-drift-metric-in-a-credit-card-scenario/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-a-kserve-inference-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-connections-api/"},"sections":[{"parentId":null,"name":"Namespace isolation in connections API","level":1,"index":0,"id":"_namespace_isolation_in_connections_api"},{"parentId":null,"name":"Role-based access control (RBAC) requirements in connections API","level":1,"index":1,"id":"_role_based_access_control_rbac_requirements_in_connections_api"},{"parentId":null,"name":"Validation scope","level":1,"index":2,"id":"_validation_scope"},{"parentId":null,"name":"Using connection annotations based on workload type","level":1,"index":3,"id":"_using_connection_annotations_based_on_workload_type"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-llm-as-a-judge-metrics-with-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-pvcs-as-storage/"},"sections":[{"parentId":null,"name":"Managed PVCs","level":1,"index":0,"id":"_managed_pvcs"},{"parentId":null,"name":"Existing PVCs","level":1,"index":1,"id":"_existing_pvcs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-ragas-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-the-cluster-CA-bundle-for-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/using-the-cluster-server-and-token-to-authenticate/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-feature-store-objects-in-the-web-based-ui/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-installed-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-metrics-for-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-models-in-the-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-python-packages-installed-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/viewing-traces-in-external-tracing-platforms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/modules/working-with-hardware-profiles/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-base-training-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-centralized-auth-oidc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-deleting-users-and-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-feature-definitions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-kserve-deployment-modes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-llama-stack-search-types/"},"sections":[{"parentId":null,"name":"Supported search modes","level":1,"index":0,"id":"_supported_search_modes"},{"parentId":"_supported_search_modes","name":"Keyword search","level":2,"index":0,"id":"_keyword_search"},{"parentId":"_supported_search_modes","name":"Vector search","level":2,"index":1,"id":"_vector_search"},{"parentId":"_supported_search_modes","name":"Hybrid search","level":2,"index":2,"id":"_hybrid_search"},{"parentId":null,"name":"Retrieval database support","level":1,"index":1,"id":"_retrieval_database_support"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-model-serving/"},"sections":[{"parentId":null,"name":"Model serving platform","level":1,"index":0,"id":"_model_serving_platform"},{"parentId":null,"name":"NVIDIA NIM model serving platform","level":1,"index":1,"id":"_nvidia_nim_model_serving_platform"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-organizing-features-by-using-entities/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-pipeline-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-persistent-storage/"},"sections":[{"parentId":null,"name":"Storage classes in {productname-short}","level":1,"index":0,"id":"_storage_classes_in_productname_short"},{"parentId":null,"name":"Access modes","level":1,"index":1,"id":"_access_modes"},{"parentId":"_access_modes","name":"Using shared storage (RWX)","level":2,"index":0,"id":"_using_shared_storage_rwx"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-ai-assets-endpoints-page/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-the-python-index/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/about-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-authentication-token-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-built-in-alerts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-hugging-face-models-with-an-environment-variable-token/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-inference-endpoint-for-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-s3-compatible-object-storage-with-self-signed-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-administration-interface-for-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-the-pipeline-editor/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/accessing-your-workbench-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/activating-the-llama-stack-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-connection-to-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-custom-model-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-application-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-a-tested-and-verified-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-an-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-certificates-to-a-cluster-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-certificates-to-a-custom-ca-bundle/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-cluster-storage-to-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-external-artifacts-to-pipeline-run-workspaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-feature-definitions-and-initializing-your-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-users-to-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/allocating-additional-resources-to-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/adding-workbench-pod-tolerations/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/amd-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-custom-image-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-workbench-creating/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/api-workbench-overview/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/archiving-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/audience-for-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/auth-on-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-kfp-sdk-with-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-storage-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/authenticating-trustyai-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/before-you-begin/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/backing-up-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/benchmarking-embedding-models-with-beir-datasets-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/changing-the-storage-class-for-an-existing-cluster-storage-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-after-deleting-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/cleaning-up-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/clone-an-example-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/collecting-metrics-from-user-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/compare-the-performance-of-osft-and-sft/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-an-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/comparing-runs-in-different-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/compiling-kubernetes-native-manifests-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/compiling-the-pipeline-yaml-with-kfp-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-certificate-for-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-certificate-for-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-cluster-for-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server-with-an-external-amazon-rds-db/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-playground-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-serving-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-recommended-accelerator-for-workbench-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-inference-service-for-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-inference-service-for-spyre/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-offline-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-an-online-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-and-managing-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-authentication-for-llmd/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-custom-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-default-workspace-pvc-settings-in-dspa/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-mcp-servers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-metric-based-autoscaling/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-model-catalog-sources-in-openshift/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-monitoring-for-your-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-pipelines-with-your-own-argo-workflows-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-quota-management-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-oidc-auth-gateway-api/"},"sections":[{"parentId":null,"name":"Security considerations","level":1,"index":0,"id":"_security_considerations"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-ragas-remote-provider-for-production/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-role-based-access-control/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-storage-class-settings/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-built-in-detector-and-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-default-storage-class-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-feature-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-the-operator-logger/"},"sections":[{"parentId":null,"name":"Viewing the {productname-short} Operator logs","level":1,"index":0,"id":"_viewing_the_productname_short_operator_logs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-trustyai-with-a-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/configuring-workload-management-with-kueue/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/controlling-caching-in-pipelines/"},"sections":[{"parentId":null,"name":"Disabling caching for individual tasks","level":1,"index":0,"id":"_disabling_caching_for_individual_tasks"},{"parentId":null,"name":"Disabling caching for a pipeline at submit time","level":1,"index":1,"id":"_disabling_caching_for_a_pipeline_at_submit_time"},{"parentId":null,"name":"Disabling caching for a pipeline at compile time","level":1,"index":2,"id":"_disabling_caching_for_a_pipeline_at_compile_time"},{"parentId":null,"name":"Disabling caching for all pipelines (pipeline server)","level":1,"index":3,"id":"_disabling_caching_for_all_pipelines_pipeline_server"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/copying-files-between-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bias-metric/"},"sections":[{"parentId":null,"name":"Creating a bias metric by using the dashboard","level":1,"index":0,"id":"creating-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Creating a bias metric by using the CLI","level":1,"index":1,"id":"creating-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-default-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-image-from-your-own-image/"},"sections":[{"parentId":null,"name":"Basic guidelines for creating your own workbench image","level":1,"index":0,"id":"_basic_guidelines_for_creating_your_own_workbench_image"},{"parentId":null,"name":"Advanced guidelines for creating your own workbench image","level":1,"index":1,"id":"_advanced_guidelines_for_creating_your_own_workbench_image"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-custom-training-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-drift-metric/"},"sections":[{"parentId":null,"name":"Creating a drift metric by using the CLI","level":1,"index":0,"id":"creating-a-drift-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-feature-store-instance-in-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorch-training-script-configmap-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-kfto-pytorchjob-resource/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-multi-node-pytorch-training-job-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-new-project-for-your-odh-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-for-distributed-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-a-workbench-select-ide/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-feature-views/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-an-s3-client/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-oci-compatible-connection-types-api/"},"sections":[{"parentId":null,"name":"Using an OCI connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_an_oci_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using an OCI connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_an_oci_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-project-scoped-resources-for-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-s3-compatible-connection-type-api/"},"sections":[{"parentId":null,"name":"Using an Amazon S3 connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_an_amazon_s3_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using an Amazon S3 connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_an_amazon_s3_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-project-scoped-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/creating-uri-compatible-connection-type-api/"},"sections":[{"parentId":null,"name":"Using a URI connection with <code>InferenceService</code> custom resource","level":1,"index":0,"id":"_using_a_uri_connection_with_inferenceservice_custom_resource"},{"parentId":null,"name":"Using a URI connection with <code>LLMInferenceService</code> custom resource","level":1,"index":1,"id":"_using_a_uri_connection_with_llminferenceservice_custom_resource"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizable-model-serving-runtime-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-component-resources/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-model-selection-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-parameters-serving-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/customizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline-by-using-the-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/defining-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bias-metric/"},"sections":[{"parentId":null,"name":"Deleting a bias metric by using the dashboard","level":1,"index":0,"id":"deleting-a-bias-metric-using-dashboard_{context}"},{"parentId":null,"name":"Deleting a bias metric by using the CLI","level":1,"index":1,"id":"deleting-a-bias-metric-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-drift-metric-by-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-playground-from-your-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-a-workbench-from-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-cluster-storage-from-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-files-in-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deleting-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-grafana-metrics-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-llama-model-with-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-llamastackdistribution-instance/"},"sections":[{"parentId":null,"name":"Example A: LlamaStackDistribution with <strong>Inline Milvus</strong>","level":1,"index":0,"id":"_example_a_llamastackdistribution_with_inline_milvus"},{"parentId":null,"name":"Example B: LlamaStackDistribution with <strong>Remote Milvus</strong>","level":1,"index":1,"id":"_example_b_llamastackdistribution_with_remote_milvus"},{"parentId":null,"name":"Example C: LlamaStackDistribution with <strong>Inline FAISS</strong>","level":1,"index":2,"id":"_example_c_llamastackdistribution_with_inline_faiss"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-model-version-from-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-a-remote-milvus-vector-database/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-model-stored-in-oci-image-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-on-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-distributed-inference/"},"sections":[{"parentId":null,"name":"Configuring authentication for {llmd} using {org-name} Connectivity Link","level":1,"index":0,"id":"configuring-authentication-for-llmd_{context}"},{"parentId":null,"name":"Enabling {llmd}","level":1,"index":1,"id":"enabling-distributed-inference_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-multiple-gpu-nodes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-models-using-the-single-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/deploying-vllm-gpu-metrics-dashboard-grafana/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/detecting-hateful-and-profane-language/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/detecting-pii-by-using-guardrails-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/determining-gpu-requirements-for-llm-powered-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/disabling-node-caching-in-elyra/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-files-from-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/downloading-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-bias-metric/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-a-scheduled-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/duplicating-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-an-existing-feature-store-instance/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-model-version-metadata-in-a-model-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-dashboard-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/editing-the-deployment-properties-of-a-model-deployed-by-using-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/emptying-trash-directory/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-a-connection-type/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-amd-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-applications-connected/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-automatic-authentication-and-publishing-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-custom-images/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-distributed-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-gpu-time-slicing/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-intel-gaudi-ai-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-kueue-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-metrics-for-existing-nim-deployment/"},"sections":[{"parentId":null,"name":"Enabling graph generation for an existing NIM deployment","level":1,"index":0,"id":"_enabling_graph_generation_for_an_existing_nim_deployment"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-nvidia-gpus/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-feature-store-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-model-registry-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-nvidia-nim-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-the-observability-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-component/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enabling-trustyai-kserve-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/end-to-end-model-customization-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-all/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/enforcing-lqlabel-some/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/estimate-memory-usage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/evaluating-rag-system-quality-with-ragas/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/executing-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/explore-the-data-processing-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/explore-the-kubeflow-pipeline-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/explore-the-sdg-hub-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/explore-the-training-hub-examples/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-metrics-to-external-observability-tools/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/exporting-your-playground-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/feature-store-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/glossary-of-common-terms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/granting-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-auto-config/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-configuring-the-hugging-face-detector-serving-runtime/"},"sections":[{"parentId":null,"name":"Guardrails Detector Hugging Face serving runtime configuration values","level":1,"index":0,"id":"_guardrails_detector_hugging_face_serving_runtime_configuration_values"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-configuring-the-opentelemetry-exporter/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-deploying-the-guardrails-orchestrator-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-enforcing-configured-safety-pipelines-for-llm-inference-using-guardrails-gateway/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-filtering-flagged-content-by-sending-requests-to-the-regex-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-gateway-config-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-configmap-parameters/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-detectors/"},"sections":[{"parentId":null,"name":"Built-in Detector","level":1,"index":0,"id":"_built_in_detector"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guardrails-orchestrator-parameters/"},"sections":[{"parentId":null,"name":"Gateway Pa","level":0,"index":0,"id":"_gateway_pa"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guided-example-build-a-kfp-pipeline-for-sdg/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/guidelines-for-metrics-based-autoscaling/"},"sections":[{"parentId":null,"name":"Choosing metrics for latency and throughput-optimized scaling","level":1,"index":0,"id":"_choosing_metrics_for_latency_and_throughput_optimized_scaling"},{"parentId":null,"name":"Choosing the right sliding window","level":1,"index":1,"id":"_choosing_the_right_sliding_window"},{"parentId":null,"name":"Optimizing HPA scale-down configuration","level":1,"index":2,"id":"_optimizing_hpa_scale_down_configuration"},{"parentId":null,"name":"Considering model size for optimal scaling","level":1,"index":3,"id":"_considering_model_size_for_optimal_scaling"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/hiding-the-default-basic-workbench-application/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ibm-spyre-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/import-example-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-custom-workbench-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/importing-a-pipeline/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/inference-performance-metrics/"},"sections":[{"parentId":null,"name":"Latency","level":1,"index":0,"id":"_latency"},{"parentId":null,"name":"Throughput","level":1,"index":1,"id":"_throughput"},{"parentId":null,"name":"Cost per million tokens","level":1,"index":2,"id":"_cost_per_million_tokens"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ingesting-content-into-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/install-packages/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-extensions-with-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-odh-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-python-packages-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-distributed-workloads-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-the-odh-operator-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service-using-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/installing-trustyai-service/"},"sections":[{"parentId":null,"name":"Installing the TrustyAI service by using the dashboard","level":1,"index":0,"id":"installing-trustyai-service-using-dashboard_{context}"},{"parentId":null,"name":"Installing the TrustyAI service by using the CLI","level":1,"index":1,"id":"installing-trustyai-service-using-cli_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/intel-gaudi-ai-accelerator-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/kueue-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-available-buckets/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/listing-files-in-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/llama-stack-apis/"},"sections":[{"parentId":null,"name":"Supported Llama Stack APIs in {productname-short}","level":1,"index":0,"id":"_supported_llama_stack_apis_in_productname_short"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Agents API","level":2,"index":0,"id":"_agents_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Datasets_IO API","level":2,"index":1,"id":"_datasets_io_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Evaluation API","level":2,"index":2,"id":"_evaluation_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Inference API","level":2,"index":3,"id":"_inference_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Safety API","level":2,"index":4,"id":"_safety_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Tool Runtime API","level":2,"index":5,"id":"_tool_runtime_api"},{"parentId":"_supported_llama_stack_apis_in_productname_short","name":"Vector_IO API","level":2,"index":6,"id":"_vector_io_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/llama-stack-providers-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-features-available-for-real-time-inference/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/making-inference-requests-to-models-deployed-on-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-certificates-without-the-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-model-registry-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/managing-ray-clusters-from-within-a-jupyter-notebook/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/migrating-pipelines-from-database-to-kubernetes-api/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/migrating-to-the-rhbok-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/mirror-the-python-index/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/mitigating-prompt-injection-by-using-a-hugging-face-prompt-injection-detector/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/model-serving-runtimes-for-accelerators/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs","level":1,"index":0,"id":"_nvidia_gpus"},{"parentId":null,"name":"Intel Gaudi accelerators","level":1,"index":1,"id":"_intel_gaudi_accelerators"},{"parentId":null,"name":"AMD GPUs","level":1,"index":2,"id":"_amd_gpus"},{"parentId":null,"name":"IBM Spyre AI accelerators on x86 and IBM Z","level":1,"index":3,"id":"_ibm_spyre_ai_accelerators_on_x86_and_ibm_z"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/model-serving-runtimes/"},"sections":[{"parentId":null,"name":"ServingRuntime","level":1,"index":0,"id":"_servingruntime"},{"parentId":null,"name":"InferenceService","level":1,"index":1,"id":"_inferenceservice"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-getting-started/"},"sections":[{"parentId":null,"name":"Additional resources","level":1,"index":0,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/next-steps-playground/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/nvidia-gpu-integration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/openai-compatibility-for-rag-apis-in-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/openai-compatible-apis-in-llama-stack/"},"sections":[{"parentId":null,"name":"Supported OpenAI-compatible APIs in {productname-short}","level":1,"index":0,"id":"_supported_openai_compatible_apis_in_productname_short"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Chat Completions API","level":2,"index":0,"id":"_chat_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Completions API","level":2,"index":1,"id":"_completions_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Embeddings API","level":2,"index":2,"id":"_embeddings_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Files API","level":2,"index":3,"id":"_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Stores API","level":2,"index":4,"id":"_vector_stores_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Vector Store Files API","level":2,"index":5,"id":"_vector_store_files_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Models API","level":2,"index":6,"id":"_models_api"},{"parentId":"_supported_openai_compatible_apis_in_productname_short","name":"Responses API","level":2,"index":7,"id":"_responses_api"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/optimizing-the-vllm-runtime/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-for-getting-started/"},"sections":[{"parentId":null,"name":"Data science workflow","level":1,"index":0,"id":"_data_science_workflow"},{"parentId":null,"name":"About this guide","level":1,"index":1,"id":"_about_this_guide"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-accelerators/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-distributed-workloads/"},"sections":[{"parentId":null,"name":"Distributed workloads infrastructure","level":1,"index":0,"id":"_distributed_workloads_infrastructure"},{"parentId":null,"name":"Types of distributed workloads","level":1,"index":1,"id":"_types_of_distributed_workloads"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-evaluating-ai-systems/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-faiss-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-feature-store/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-kueue-resources/"},"sections":[{"parentId":null,"name":"Resource flavor","level":1,"index":0,"id":"_resource_flavor"},{"parentId":null,"name":"Cluster queue","level":1,"index":1,"id":"_cluster_queue"},{"parentId":null,"name":"Local queue","level":1,"index":2,"id":"_local_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-managing-workloads-with-kueue/"},"sections":[{"parentId":null,"name":"Kueue management states","level":1,"index":0,"id":"_kueue_management_states"},{"parentId":null,"name":"Queue enforcement for projects","level":1,"index":1,"id":"_queue_enforcement_for_projects"},{"parentId":null,"name":"Restrictions for managing workloads with Kueue","level":1,"index":2,"id":"_restrictions_for_managing_workloads_with_kueue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-milvus-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-ml-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-monitoring/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-model-registries/"},"sections":[{"parentId":null,"name":"Model catalog","level":1,"index":0,"id":"_model_catalog"},{"parentId":null,"name":"Model registry","level":1,"index":1,"id":"_model_registry"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-monitoring-your-ai-systems/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-object-storage-endpoints/"},"sections":[{"parentId":null,"name":"MinIO (On-Cluster)","level":1,"index":0,"id":"_minio_on_cluster"},{"parentId":null,"name":"Amazon S3","level":1,"index":1,"id":"_amazon_s3"},{"parentId":null,"name":"Other S3-Compatible Object Stores","level":1,"index":2,"id":"_other_s3_compatible_object_stores"},{"parentId":null,"name":"Verification and Troubleshooting","level":1,"index":3,"id":"_verification_and_troubleshooting"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-experiments/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipeline-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-caching/"},"sections":[{"parentId":null,"name":"Caching criteria","level":1,"index":0,"id":"_caching_criteria"},{"parentId":null,"name":"Viewing cached steps in the {productname-short} user interface","level":1,"index":1,"id":"_viewing_cached_steps_in_the_productname_short_user_interface"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-pipelines-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-rag/"},"sections":[{"parentId":null,"name":"Audience for RAG","level":1,"index":0,"id":"_audience_for_rag"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-the-model-customization-workflow/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-upgrading-odh/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-user-types-and-permissions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/overview-of-vector-databases/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/performance-considerations-for-doc-apps/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/performing-model-evaluations-in-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/playground-overview/"},"sections":[{"parentId":null,"name":"Core capabilities","level":1,"index":0,"id":"_core_capabilities"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/playground-prerequisites/"},"sections":[{"parentId":null,"name":"Cluster administrator prerequisites","level":1,"index":0,"id":"_cluster_administrator_prerequisites"},{"parentId":null,"name":"User prerequisites","level":1,"index":1,"id":"_user_prerequisites"},{"parentId":null,"name":"Model and runtime requirements for the playground","level":1,"index":2,"id":"_model_and_runtime_requirements_for_the_playground"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Key model selection factors","level":2,"index":0,"id":"_key_model_selection_factors"},{"parentId":"_model_and_runtime_requirements_for_the_playground","name":"Example model configuration","level":2,"index":1,"id":"_example_model_configuration"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preparing-documents-with-docling-for-llama-stack-retrieval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/preventing-users-from-adding-applications-to-the-dashboard/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-an-image-to-the-integrated-openshift-image-registry/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-in-code-server-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/pushing-project-changes-to-a-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/querying-ingested-content-in-a-llama-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/reenabling-component-resource-customization/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-accelerator-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-cpu-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-dashboard-configuration-options/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-default-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-distributed-inference/"},"sections":[{"parentId":null,"name":"Single-node GPU deployment","level":1,"index":0,"id":"_single_node_gpu_deployment"},{"parentId":null,"name":"Multi-node deployment","level":1,"index":1,"id":"_multi_node_deployment"},{"parentId":null,"name":"Intelligent inference scheduler with KV cache routing","level":1,"index":2,"id":"_intelligent_inference_scheduler_with_kv_cache_routing"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-ddp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-fsdp/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorch-training-script-nccl/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kubernetes-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-kueue-resource-configurations/"},"sections":[{"parentId":null,"name":"NVIDIA GPUs without shared cohort","level":1,"index":0,"id":"_nvidia_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU resource flavor","level":2,"index":0,"id":"_nvidia_rtx_a400_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU resource flavor","level":2,"index":1,"id":"_nvidia_rtx_a1000_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A400 GPU cluster queue","level":2,"index":2,"id":"_nvidia_rtx_a400_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_without_shared_cohort","name":"NVIDIA RTX A1000 GPU cluster queue","level":2,"index":3,"id":"_nvidia_rtx_a1000_gpu_cluster_queue"},{"parentId":null,"name":"NVIDIA GPUs and AMD GPUs without shared cohort","level":1,"index":1,"id":"_nvidia_gpus_and_amd_gpus_without_shared_cohort"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU resource flavor","level":2,"index":0,"id":"_amd_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU resource flavor","level":2,"index":1,"id":"_nvidia_gpu_resource_flavor"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"AMD GPU cluster queue","level":2,"index":2,"id":"_amd_gpu_cluster_queue"},{"parentId":"_nvidia_gpus_and_amd_gpus_without_shared_cohort","name":"NVIDIA GPU cluster queue","level":2,"index":3,"id":"_nvidia_gpu_cluster_queue"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-oidc-authorization-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-grafana-metrics/"},"sections":[{"parentId":null,"name":"Accelerator metrics","level":1,"index":0,"id":"ref-accelerator-metrics_{context}"},{"parentId":null,"name":"CPU metrics","level":1,"index":1,"id":"ref-cpu-metrics_{context}"},{"parentId":null,"name":"vLLM metrics","level":1,"index":2,"id":"ref-vllm-metrics_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-example-pvc-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-supported-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-inference-endpoints/"},"sections":[{"parentId":null,"name":"Caikit TGIS ServingRuntime for KServe","level":1,"index":0,"id":"_caikit_tgis_servingruntime_for_kserve"},{"parentId":null,"name":"OpenVINO Model Server","level":1,"index":1,"id":"_openvino_model_server"},{"parentId":null,"name":"vLLM NVIDIA GPU ServingRuntime for KServe","level":1,"index":2,"id":"_vllm_nvidia_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Intel Gaudi Accelerator ServingRuntime for KServe","level":1,"index":3,"id":"_vllm_intel_gaudi_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM AMD GPU ServingRuntime for KServe","level":1,"index":4,"id":"_vllm_amd_gpu_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre AI Accelerator ServingRuntime for KServe","level":1,"index":5,"id":"_vllm_spyre_ai_accelerator_servingruntime_for_kserve"},{"parentId":null,"name":"vLLM Spyre s390x ServingRuntime for KServe","level":1,"index":6,"id":"_vllm_spyre_s390x_servingruntime_for_kserve"},{"parentId":null,"name":"NVIDIA Triton Inference Server","level":1,"index":7,"id":"_nvidia_triton_inference_server"},{"parentId":null,"name":"Seldon MLServer","level":1,"index":8,"id":"_seldon_mlserver"},{"parentId":null,"name":"Additional resources","level":1,"index":9,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-trainingclient-api-job-related-methods/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-vllm-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/ref-tested-verified-runtimes/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-from-the-model-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/registering-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-disabled-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-the-ca-bundle-from-a-single-namespace/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/removing-the-ca-bundle-from-all-namespaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation/"},"sections":[{"parentId":null,"name":"Requesting a LIME explanation by using the CLI","level":1,"index":0,"id":"requesting-a-lime-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-lime-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requesting-a-shap-explanation/"},"sections":[{"parentId":null,"name":"Requesting a SHAP explanation by using the CLI","level":1,"index":0,"id":"requesting-a-shap-explanation-using-CLI_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v1/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/requirements-for-upgrading-odh-v2/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-a-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-experiment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-an-archived-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/restoring-the-default-pvc-size-for-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/retrieving-data-science-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/revoking-user-access-to-basic-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-pipeline-in-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-training-job-by-using-the-kfto-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-a-workload-with-a-kueue-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-custom-evaluations-with-LMEval-and-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-disconnected-env/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-ai-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/logging-in/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job-properties/"},"sections":[{"parentId":null,"name":"Properties for setting up custom Unitxt cards, templates, or system prompts","level":1,"index":0,"id":"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/lmeval-evaluation-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-distributed-data-science-workloads-from-jupyter-notebooks/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-demo-jupyter-notebooks-from-the-codeflare-sdk/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/running-the-fine-tuning-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/s3-prerequisites/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run-using-a-cron-job/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/scheduling-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/selecting-admin-and-user-groups/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/sending-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-timeout-for-kserve/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-feature-store-UI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval-s3-support/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-ragas-inline-provider/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/showing-hiding-information-about-available-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/specifying-files-to-ignore/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/specifying-the-data-source-for-features/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-basic-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-a-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/starting-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-an-active-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-basic-workbenches-owned-by-other-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/setting-up-your-working-environment/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-idle-workbenches/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/stopping-starting-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-a-model-in-oci-image/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/storing-data-with-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/support-philosophy/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-checking-model-fairness/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-deploying-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-introduction/"},"sections":[{"parentId":null,"name":"About the example models","level":1,"index":0,"id":"_about_the_example_models"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-labeling-data-fields/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-reviewing-the-results/"},"sections":[{"parentId":null,"name":"Are the models biased?","level":1,"index":0,"id":"_are_the_models_biased"},{"parentId":null,"name":"How does the production data compare to the training data?","level":1,"index":1,"id":"_how_does_the_production_data_compare_to_the_training_data"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-a-fairness-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-scheduling-an-identity-metric-request/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-sending-training-data-to-the-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-setting-up-your-environment/"},"sections":[{"parentId":null,"name":"Downloading the tutorial files","level":1,"index":0,"id":"_downloading_the_tutorial_files"},{"parentId":null,"name":"Logging in to the OpenShift cluster from the command line","level":1,"index":1,"id":"_logging_in_to_the_openshift_cluster_from_the_command_line"},{"parentId":null,"name":"Configuring monitoring for the model serving platform","level":1,"index":2,"id":"_configuring_monitoring_for_the_model_serving_platform"},{"parentId":null,"name":"Enabling the TrustyAI component","level":1,"index":3,"id":"enabling-trustyai-component_{context}"},{"parentId":null,"name":"Setting up a project","level":1,"index":4,"id":"_setting_up_a_project"},{"parentId":null,"name":"Authenticating the TrustyAI service","level":1,"index":5,"id":"_authenticating_the_trustyai_service"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/t-bias-simulating-real-world-data/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-baseline-model-responses/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-with-model-control-protocol-servers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-your-model-with-rag/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/testing-your-vllm-model-endpoints/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-gateway-api/"},"sections":[{"parentId":null,"name":"The <code>GatewayConfig</code> status shows as not ready","level":1,"index":0,"id":"_the_gatewayconfig_status_shows_as_not_ready"},{"parentId":null,"name":"Authentication proxy fails to start","level":1,"index":1,"id":"_authentication_proxy_fails_to_start"},{"parentId":null,"name":"The Gateway is inaccessible","level":1,"index":2,"id":"_the_gateway_is_inaccessible"},{"parentId":null,"name":"The OIDC authentication fails","level":1,"index":3,"id":"_the_oidc_authentication_fails"},{"parentId":null,"name":"The dashboard is not accessible after authentication","level":1,"index":4,"id":"_the_dashboard_is_not_accessible_after_authentication"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-workbenches-for-administrators/"},"sections":[{"parentId":null,"name":"A user receives a <strong>404: Page not found</strong> error when logging in to Jupyter","level":1,"index":0,"id":"_a_user_receives_a_404_page_not_found_error_when_logging_in_to_jupyter"},{"parentId":null,"name":"A user&#8217;s workbench does not start","level":1,"index":1,"id":"_a_users_workbench_does_not_start"},{"parentId":null,"name":"The user receives a <strong>database or disk is full</strong> error or a <strong>no space left on device</strong> error when they run notebook cells","level":1,"index":2,"id":"_the_user_receives_a_database_or_disk_is_full_error_or_a_no_space_left_on_device_error_when_they_run_notebook_cells"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-in-workbenches-for-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-Kueue/"},"sections":[{"parentId":null,"name":"A user receives a \"failed to call webhook\" error message for Kueue","level":1,"index":0,"id":"_a_user_receives_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"A user receives a \"Default Local Queue &#8230;&#8203; not found\" error message","level":1,"index":1,"id":"_a_user_receives_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"A user receives a \"local_queue provided does not exist\" error message","level":1,"index":2,"id":"_a_user_receives_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"The pod provisioned by Kueue is terminated before the image is pulled","level":1,"index":3,"id":"_the_pod_provisioned_by_kueue_is_terminated_before_the_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators/"},"sections":[{"parentId":null,"name":"A user&#8217;s Ray cluster is in a suspended state","level":1,"index":0,"id":"_a_users_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster is in a failed state","level":1,"index":1,"id":"_a_users_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"A user&#8217;s Ray cluster does not start","level":1,"index":2,"id":"_a_users_ray_cluster_does_not_start"},{"parentId":null,"name":"A user cannot create a Ray cluster or submit jobs","level":1,"index":3,"id":"_a_user_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"Additional resources","level":1,"index":4,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-common-problems-with-distributed-workloads-for-users/"},"sections":[{"parentId":null,"name":"My Ray cluster is in a suspended state","level":1,"index":0,"id":"_my_ray_cluster_is_in_a_suspended_state"},{"parentId":null,"name":"My Ray cluster is in a failed state","level":1,"index":1,"id":"_my_ray_cluster_is_in_a_failed_state"},{"parentId":null,"name":"I see a \"failed to call webhook\" error message for Kueue","level":1,"index":2,"id":"_i_see_a_failed_to_call_webhook_error_message_for_kueue"},{"parentId":null,"name":"My Ray cluster does not start","level":1,"index":3,"id":"_my_ray_cluster_does_not_start"},{"parentId":null,"name":"I see a \"Default Local Queue not found\" error message","level":1,"index":4,"id":"_i_see_a_default_local_queue_not_found_error_message"},{"parentId":null,"name":"I see a \"local_queue provided does not exist\" error message","level":1,"index":5,"id":"_i_see_a_local_queue_provided_does_not_exist_error_message"},{"parentId":null,"name":"I cannot create a Ray cluster or submit jobs","level":1,"index":6,"id":"_i_cannot_create_a_ray_cluster_or_submit_jobs"},{"parentId":null,"name":"My pod provisioned by Kueue is terminated before my image is pulled","level":1,"index":7,"id":"_my_pod_provisioned_by_kueue_is_terminated_before_my_image_is_pulled"},{"parentId":null,"name":"Additional resources","level":1,"index":8,"id":"_additional_resources"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-dspa-component-errors/"},"sections":[{"parentId":null,"name":"Common errors across DSPA components","level":1,"index":0,"id":"_common_errors_across_dspa_components"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/troubleshooting-playground-issues/"},"sections":[{"parentId":null,"name":"The chatbot thinks indefinitely","level":1,"index":0,"id":"_the_chatbot_thinks_indefinitely"},{"parentId":null,"name":"The model does not use RAG data","level":1,"index":1,"id":"_the_model_does_not_use_rag_data"},{"parentId":null,"name":"MCP servers are missing from the UI","level":1,"index":2,"id":"_mcp_servers_are_missing_from_the_ui"},{"parentId":null,"name":"The model fails to call MCP tools","level":1,"index":3,"id":"_the_model_fails_to_call_mcp_tools"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-rag-evaluation-providers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-certificates/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-pipeline-run-workspaces/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/understanding-rag-settings/"},"sections":[{"parentId":null,"name":"Understanding RAG settings","level":1,"index":0,"id":"understanding-rag-settings_{context}"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-connection/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-hardware-profile/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-a-runtime-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-access-to-a-project/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-cluster-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-lmeval-job-configuration-using-the-web-console/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-the-deployment-properties-of-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-playground-configuration/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-workbench-settings-by-restarting-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-in-code-server-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/updating-your-project-with-changes-from-a-remote-git-repository/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/upgrading-the-odh-operator/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-code-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-from-a-git-repository-using-jupyterlab/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-code-server-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-a-git-repository-using-cli/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-an-existing-notebook-file-to-jupyterlab-from-local-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-files-to-bucket/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-model-files-to-pvc/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/uploading-training-data-to-trustyai/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-custom-unitxt-card/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-drift-metric-in-a-credit-card-scenario/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-a-kserve-inference-service/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-connections-api/"},"sections":[{"parentId":null,"name":"Namespace isolation in connections API","level":1,"index":0,"id":"_namespace_isolation_in_connections_api"},{"parentId":null,"name":"Role-based access control (RBAC) requirements in connections API","level":1,"index":1,"id":"_role_based_access_control_rbac_requirements_in_connections_api"},{"parentId":null,"name":"Validation scope","level":1,"index":2,"id":"_validation_scope"},{"parentId":null,"name":"Using connection annotations based on workload type","level":1,"index":3,"id":"_using_connection_annotations_based_on_workload_type"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-explainers/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-llm-as-a-judge-metrics-with-lmeval/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-oci-containers-for-model-storage/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-pvcs-as-storage/"},"sections":[{"parentId":null,"name":"Managed PVCs","level":1,"index":0,"id":"_managed_pvcs"},{"parentId":null,"name":"Existing PVCs","level":1,"index":1,"id":"_existing_pvcs"}]}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-ragas-with-llama-stack/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-the-cluster-CA-bundle-for-model-serving/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/using-the-cluster-server-and-token-to-authenticate/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/verifying-amd-gpu-availability-on-your-cluster/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-active-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-archived-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-audit-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-bias-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connected-applications/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-connection-types/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-data-science-users/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-drift-metrics/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-existing-pipelines/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-feature-store-objects-in-the-web-based-ui/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-http-request-metrics-for-a-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-installed-components/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-kueue-alerts-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-metrics-for-the-model-serving-platform/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-models-in-the-catalog/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-nvidia-nim-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-a-nim-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-deployed-model/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-performance-metrics-for-model-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-artifacts/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-step-logs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-pipeline-task-executions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-project-metrics-for-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-python-packages-installed-on-your-code-server-workbench/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-model-versions/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-registered-models/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-scheduled-pipeline-runs/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-run/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-server/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-details-of-a-pipeline-version/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-the-status-of-distributed-workloads/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/viewing-traces-in-external-tracing-platforms/"},"sections":null}}},{"node":{"childAsciidoc":{"fields":{"slug":"/docs/assemblies/modules/working-with-hardware-profiles/"},"sections":null}}}]},"asciidoc":{"html":"<div id=\"toc\" class=\"toc\">\n<div id=\"toctitle\">Table of Contents</div>\n<ul class=\"sectlevel1\">\n<li><a href=\"#overview-evaluating-ai-systems_evaluate\">Overview of evaluating AI systems</a></li>\n<li><a href=\"#evaluating-large-language-models_evaluate\">Evaluating large language models</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#setting-up-lmeval_evaluate\">Setting up LM-Eval</a></li>\n<li><a href=\"#enabling-external-resource-access-for-lmeval-jobs_evaluate\">Enabling external resource access for LMEval jobs</a></li>\n<li><a href=\"#lmeval-evaluation-job_evaluate\">LM-Eval evaluation job</a></li>\n<li><a href=\"#lmeval-evaluation-job-properties_evaluate\">LM-Eval evaluation job properties</a></li>\n<li><a href=\"#performing-model-evaluations-in-the-dashboard_evaluate\">Performing model evaluations in the dashboard</a></li>\n<li><a href=\"#lmeval-scenarios_evaluate\">LM-Eval scenarios</a></li>\n</ul>\n</li>\n<li><a href=\"#using-llama-stack-with-trustyai_evaluate\">Using llama stack with TrustyAI</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_evaluate\">Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI</a></li>\n<li><a href=\"#running-custom-evaluations-with-LMEval-and-llama-stack_evaluate\">Running custom evaluations with LM-Eval and Llama Stack</a></li>\n<li><a href=\"#detecting-pii-by-using-guardrails-with-llama-stack_evaluate\">Detecting personally identifiable information (PII) by using Guardrails with Llama Stack</a></li>\n</ul>\n</li>\n<li><a href=\"#evaluating-rag-systems-with-ragas_evaluate\">Evaluating RAG systems with Ragas</a>\n<ul class=\"sectlevel2\">\n<li><a href=\"#_about_ragas_evaluation\">About Ragas evaluation</a></li>\n<li><a href=\"#setting-up-ragas-inline-provider_evaluate\">Setting up the Ragas inline provider for development</a></li>\n<li><a href=\"#configuring-ragas-remote-provider-for-production_evaluate\">Configuring the Ragas remote provider for production</a></li>\n<li><a href=\"#evaluating-rag-system-quality-with-ragas_evaluate\">Evaluating RAG system quality with Ragas metrics</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div class=\"sect1\">\n<h2 id=\"overview-evaluating-ai-systems_evaluate\">Overview of evaluating AI systems</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>Evaluate your AI systems to generate an analysis of your model&#8217;s ability by using the following TrustyAI tools:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><strong>LM-Eval</strong>: You can use TrustyAI to monitor your LLM against a range of different evaluation tasks and to ensure the accuracy and quality of its output. Features such as summarization, language toxicity, and question-answering accuracy are assessed to inform and improve your model parameters.</p>\n</li>\n<li>\n<p><strong>RAGAS</strong>: Use Retrieval-Augmented Generation Assessment (RAGAS) with TrustyAI to measure and improve the quality of your RAG systems in Open Data Hub. RAGAS provides objective metrics that assess retrieval quality, answer relevance, and factual consistency.</p>\n</li>\n<li>\n<p><strong>Llama Stack</strong>: Use Llama Stack components and providers with TrustyAI to evaluate and work with LLMs.</p>\n</li>\n</ul>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"evaluating-large-language-models_evaluate\">Evaluating large language models</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>A large language model (LLM) is a type of artificial intelligence (AI) program that is designed for natural language processing tasks, such as recognizing and generating text.</p>\n</div>\n<div class=\"paragraph\">\n<p>As a data scientist, you might want to monitor your large language models against a range of metrics, in order to ensure the accuracy and quality of its output.  Features such as summarization, language toxicity, and question-answering accuracy can be assessed to inform and improve your model parameters.</p>\n</div>\n<div class=\"paragraph\">\n<p>Open Data Hub now offers Language Model Evaluation as a Service (LM-Eval-aaS), in a feature called LM-Eval. LM-Eval provides a unified framework to test generative language models on a vast range of different evaluation tasks.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following sections show you how to create an <code>LMEvalJob</code> custom resource (CR) which allows you to activate an evaluation job and generate an analysis of your model&#8217;s ability.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"setting-up-lmeval_evaluate\">Setting up LM-Eval</h3>\n<div class=\"paragraph _abstract\">\n<p>LM-Eval is a service designed for evaluating large language models that has been integrated into the TrustyAI Operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>The service is built on top of two open-source projects:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>LM Evaluation Harness, developed by EleutherAI, that provides a comprehensive framework for evaluating language models</p>\n</li>\n<li>\n<p>Unitxt, a tool that enhances the evaluation process with additional functionalities</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The following information explains how to create an <code>LMEvalJob</code> custom resource (CR) to initiate an evaluation job and get the results.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>LM-Eval is only available in the latest community builds. To use LM-Eval on Open Data Hub, ensure that you use ODH 2.20 or later versions and add the following <code>devFlag</code> to your <code>DataScienceCluster</code> resource:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>    trustyai:\n    devFlags:\n        manifests:\n        - contextDir: config\n            sourcePath: ''\n            uri: https://github.com/trustyai-explainability/trustyai-service-operator/tarball/main\n    managementState: Managed</code></pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Global settings for LM-Eval</div>\n<p>Configurable global settings for LM-Eval services are stored in the TrustyAI operator global <code>ConfigMap</code>, named <code>trustyai-service-operator-config</code>. The global settings are located in the same namespace as the operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>You can configure the following properties for LM-Eval:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 1. LM-Eval properties</caption>\n<colgroup>\n<col style=\"width: 14.2857%;\">\n<col style=\"width: 14.2857%;\">\n<col style=\"width: 71.4286%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Property</th>\n<th class=\"tableblock halign-left valign-top\">Default</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-detect-device</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>true/false</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Detect if there are GPUs available and assign a value for the <code>--device argument</code> for LM Evaluation Harness. If GPUs are available, the value is <code>cuda</code>. If there are no GPUs available, the value is <code>cpu</code>.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-pod-image</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>quay.io/trustyai/ta-lmes-job:latest</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The image for the LM-Eval job. The image contains the Python packages for LM Evaluation Harness and Unitxt.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-driver-image</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>quay.io/trustyai/ta-lmes-driver:latest</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The image for the LM-Eval driver. For detailed information about the driver, see the <code>cmd/lmes_driver</code> directory.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-image-pull-policy</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>Always</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The image-pulling policy when running the evaluation job.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-default-batch-size</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">8</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The default batch size when invoking the model inference API. Default batch size is only available for local models.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-max-batch-size</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">24</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The maximum batch size that users can specify in an evaluation job.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>lmes-pod-checking-interval</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">10s</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The interval to check the job pod for an evaluation job.</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"paragraph\">\n<p>After updating the settings in the <code>ConfigMap</code>, restart the operator to apply the new values.</p>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"enabling-external-resource-access-for-lmeval-jobs_evaluate\">Enabling external resource access for LMEval jobs</h3>\n<div class=\"paragraph _abstract\">\n<p>LMEval jobs do not allow internet access or remote code execution by default. When configuring an <code>LMEvalJob</code>, it may require access to external resources, for example task datasets and model tokenizers, usually hosted on <a href=\"https://huggingface.co\">Hugging Face</a>. If you trust the source and have reviewed the content of these artifacts, an <code>LMEvalJob</code> can be configured to automatically download them.</p>\n</div>\n<div class=\"paragraph\">\n<p>Follow the steps below to enable online access and remote code execution for LMEval jobs. Choose to update these settings by using either the CLI or in the console. Enable one or both settings according to your needs.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"enabling-online-access-and-remote-code-execution-LMEvalJob-using-the-cli_evaluate\">Enabling online access and remote code execution for LMEval Jobs using the CLI</h4>\n<div class=\"paragraph _abstract\">\n<p>You can enable online access using the CLI for LMEval jobs by setting the <code>allowOnline</code> specification to <code>true</code> in the <code>LMEvalJob</code> custom resource (CR). You can also enable remote code execution by setting the <code>allowCodeExecution</code> specification to <code>true</code>. Both modes can be used at the same time.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Enabling online access or code execution involves a security risk. Only use these configurations if you trust the source(s).</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Get the current <code>DataScienceCluster</code> resource, which is located in the <code>redhat-ods-operator</code> namespace:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc get datasciencecluster -n redhat-ods-operator</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example output</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">NAME                 AGE\ndefault-dsc          10d</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Enable online access and code execution for the cluster in the <code>DataScienceCluster</code> resource with the <code>permitOnline</code> and <code>permitCodeExecution</code> specifications. For example, create a file named <code>allow-online-code-exec-dsc.yaml</code> with the following contents:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example <code>allow-online-code-exec-dsc.yaml</code> resource enabling online access and remote code execution</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: datasciencecluster.opendatahub.io/v2\nkind: DataScienceCluster\nmetadata:\n  name: default-dsc\nspec:\n# ...\n  components:\n    trustyai:\n      managementState: Managed\n      eval:\n        lmeval:\n           permitOnline: allow\n           permitCodeExecution: allow\n# ...</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>permitCodeExecution</code> and <code>permitOnline</code> settings are disabled by default with a value of <code>deny</code>. You must explicitly enable these settings in the <code>DataScienceCluster</code> resource for the <code>LMEvalJob</code> instance to enable internet access or permission to run any externally downloaded code.</p>\n</div>\n</li>\n<li>\n<p>Apply the updated <code>DataScienceCluster</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc apply -f allow-online-code-exec-dsc.yaml -n redhat-ods-operator</code></pre>\n</div>\n</div>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Optional: Run the following command to check that the <code>DataScienceCluster</code> is in a healthy state:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc get datasciencecluster default-dsc</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example output</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">NAME          READY   REASON\ndefault-dsc   True</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>For new LMEval jobs, define the job in a YAML file as shown in the following example. This configuration requests both internet access, with <code>allowOnline: true</code>, and permission for remote code execution with, <code>allowCodeExecution: true</code>:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example lmevaljob-with-online-code-exec.yaml</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: lmevaljob-with-online-code-exec\n  namespace: &lt;your_namespace&gt;\nspec:\n# ...\n  allowOnline: true\n  allowCodeExecution: true\n# ...</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The <code>allowOnline</code> and <code>allowCodeExecution</code> settings are disabled by default with a value of <code>false</code> in the <code>LMEvalJob</code> CR.</p>\n</div>\n</li>\n<li>\n<p>Deploy the LMEval Job:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc apply -f lmevaljob-with-online-code-exec.yaml -n &lt;your_namespace&gt;</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If you upgrade to version 2.25, some TrustyAI <code>LMEvalJob</code> CR configuration values might be overwritten. The new deployment prioritizes the value on the 2.25 version <code>DataScienceCluster</code>. Existing LMEval jobs are unaffected. Verify that all <code>DataScienceCluster</code> values are explicitly defined and validated during installation.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>Run the following command to verify that the <code>DataScienceCluster</code> has the updated fields:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc get datasciencecluster default-dsc -n redhat-ods-operator -o \"jsonpath={.data}\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Run the following command to verify that the <code>trustyai-dsc-config</code> ConfigMap has the same flag values set in the <code>DataScienceCluster</code>.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc get configmaps trustyai-dsc-config -n redhat-ods-applications -o \"jsonpath={.spec.components.trustyai.eval.lmeval}\"</code></pre>\n</div>\n</div>\n<div class=\"listingblock\">\n<div class=\"title\">Example output</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">{\"eval.lmeval.permitCodeExecution\":\"true\",\"eval.lmeval.permitOnline\":\"true\"}</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"updating-lmeval-job-configuration-using-the-web-console_evaluate\">Updating LMEval job configuration using the web console</h4>\n<div class=\"paragraph _abstract\">\n<p>Follow these steps to enable online access (<code>allowOnline</code>) and remote code execution (<code>allowCodeExecution</code>) modes through the Open Data Hub web console for LMEval jobs.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Enabling online access or code execution involves a security risk. Only use these configurations if you trust the source(s).</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your Open Data Hub cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the OpenShift Container Platform console, click <strong>Operators</strong> &#8594; <strong>Installed Operators</strong>.</p>\n</li>\n<li>\n<p>Search for the <strong>Open Data Hub Operator</strong>, and then click the Operator name to open the Operator details page.</p>\n</li>\n<li>\n<p>Click the <strong>Data Science Cluster</strong> tab.</p>\n</li>\n<li>\n<p>Click the default instance name (for example, <strong>default-dsc</strong>) to open the instance details page.</p>\n</li>\n<li>\n<p>Click the <strong>YAML</strong> tab to show the instance specifications.</p>\n</li>\n<li>\n<p>In the <code>spec:components:trustyai:eval:lmeval</code> section, set the <code>permitCodeExecution</code> and <code>permitOnline</code> fields to a value of <code>allow</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>spec:\n  components:\n    trustyai:\n      managementState: Managed\n      eval:\n        lmeval:\n           permitOnline: allow\n           permitCodeExecution: allow</pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n</li>\n<li>\n<p>From the <strong>Project</strong> drop-down list, select the project that contains the LMEval job you are working with.</p>\n</li>\n<li>\n<p>From the <strong>Resources</strong> drop-down list, select the <code>LMEvalJob</code> instance that you are working with.</p>\n</li>\n<li>\n<p>Click <strong>Actions</strong> &#8594; <strong>Edit YAML</strong></p>\n</li>\n<li>\n<p>Ensure that the <code>allowOnline</code> and <code>allowCodeExecution</code> are set to <code>true</code> to enable online access and code execution for this job when writing your <code>LMEvalJob</code> custom resource:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: example-lmeval\nspec:\n  allowOnline: true\n  allowCodeExecution: true</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Click <strong>Save</strong>.</p>\n</li>\n</ol>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 2. Configuration keys for LMEvalJob custom resource</caption>\n<colgroup>\n<col style=\"width: 30%;\">\n<col style=\"width: 20%;\">\n<col style=\"width: 50%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Field</th>\n<th class=\"tableblock halign-left valign-top\">Default</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>spec.allowOnline</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>false</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Enables this job to access the internet (e.g., to download datasets or tokenizers).</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>spec.allowCodeExecution</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>false</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Allows this job to run code included with downloaded resources.</p></td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"lmeval-evaluation-job_evaluate\">LM-Eval evaluation job</h3>\n<div class=\"paragraph _abstract\">\n<p>LM-Eval service defines a new Custom Resource Definition (CRD) called <code>LMEvalJob</code>. An <code>LMEvalJob</code> object represents an evaluation job. <code>LMEvalJob</code> objects are monitored by the TrustyAI Kubernetes operator.</p>\n</div>\n<div class=\"paragraph\">\n<p>To run an evaluation job, create an <code>LMEvalJob</code> object with the following information: <code>model</code>, <code>model arguments</code>, <code>task</code>, and <code>secret</code>.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>For a list of TrustyAI-supported tasks, see <a href=\"https://trustyai.org/docs/main/component-lm-eval#_lm_eval_task_support\">LMEval task support</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<p>After the <code>LMEvalJob</code> is created, the LM-Eval service runs the evaluation job.  The status and results of the <code>LMEvalJob</code> object update when the information is available.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Other TrustyAI features (such as bias and drift metrics) cannot be used with non-tabular models (including LLMs). Deploying the <code>TrustyAIService</code> custom resource (CR) in a namespace that contains non-tabular models (such as the namespace where an evaluation job is being executed) can cause errors within the TrustyAI service.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Sample LMEvalJob object</div>\n<p>The sample <code>LMEvalJob</code> object contains the following features:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The <code>google/flan-t5-base</code> model from Hugging Face.</p>\n</li>\n<li>\n<p>The dataset from the <code>wnli</code> card, a subset of the GLUE (General Language Understanding Evaluation) benchmark evaluation framework from Hugging Face. For more information about the <code>wnli</code> Unitxt card, see the <a href=\"https://www.unitxt.ai/\">Unitxt website</a>.</p>\n</li>\n<li>\n<p>The following default parameters for the <code>multi_class.relation</code> Unitxt task: <code>f1_micro</code>, <code>f1_macro</code>, and <code>accuracy</code>. This template can be found on the Unitxt website: click <strong>Catalog</strong>, then click <strong>Tasks</strong> and select <strong>Classification</strong> from the menu.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The following is an example of an <code>LMEvalJob</code> object:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  model: hf\n  modelArgs:\n  - name: pretrained\n    value: google/flan-t5-base\n  taskList:\n    taskRecipes:\n    - card:\n        name: \"cards.wnli\"\n      template: \"templates.classification.multi_class.relation.default\"\n  logSamples: true</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>After you apply the sample <code>LMEvalJob</code>, check its state by using the following command:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get lmevaljob evaljob-sample</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Output similar to the following appears:\nNAME: <code>evaljob-sample</code>\nSTATE: <code>Running</code></p>\n</div>\n<div class=\"paragraph\">\n<p>Evaluation results are available when the state of the object changes to <code>Complete</code>. Both the model and dataset in this example are small. The evaluation job should finish within 10 minutes on a CPU-only node.</p>\n</div>\n<div class=\"paragraph\">\n<p>Use the following command to get the results:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get lmevaljobs.trustyai.opendatahub.io evaljob-sample \\\n  -o template --template={{.status.results}} | jq '.results'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>The command returns results similar to the following example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>{\n  \"tr_0\": {\n    \"alias\": \"tr_0\",\n    \"f1_micro,none\": 0.5633802816901409,\n    \"f1_micro_stderr,none\": \"N/A\",\n    \"accuracy,none\": 0.5633802816901409,\n    \"accuracy_stderr,none\": \"N/A\",\n    \"f1_macro,none\": 0.36036036036036034,\n    \"f1_macro_stderr,none\": \"N/A\"\n  }\n}</code></pre>\n</div>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Notes on the results</div>\n<ul>\n<li>\n<p>The <code>f1_micro</code>, <code>f1_macro</code>, and <code>accuracy</code> scores are 0.56, 0.36, and 0.56.</p>\n</li>\n<li>\n<p>The full results are stored in the <code>.status.results</code> of the <code>LMEvalJob</code> object as a JSON document.</p>\n</li>\n<li>\n<p>The command above only retrieves the results field of the JSON document.</p>\n</li>\n</ul>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The provided <code>LMEvalJob</code> uses a dataset from the <code>wnli</code> card, which is in Parquet format and not supported on <code>s390x</code>. To run on <code>s390x</code>, choose a task that uses a non-Parquet dataset.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"lmeval-evaluation-job-properties_evaluate\">LM-Eval evaluation job properties</h3>\n<div class=\"paragraph _abstract\">\n<p>The <code>LMEvalJob</code> object contains the following features:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>The <code>google/flan-t5-base</code> model.</p>\n</li>\n<li>\n<p>The dataset from the <code>wnli</code> card, from the GLUE (General Language Understanding Evaluation) benchmark evaluation framework.</p>\n</li>\n<li>\n<p>The <code>multi_class.relation</code> Unitxt task default parameters.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The following table lists each property in the <code>LMEvalJob</code> and its usage:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 3. LM-EvalJob properties</caption>\n<colgroup>\n<col style=\"width: 28.5714%;\">\n<col style=\"width: 71.4286%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Parameter</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>model</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Specifies which model type or provider is evaluated. This field directly maps to the <code>--model</code> argument of the <code>lm-evaluation-harness</code>. The model types and providers that you can use include:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>hf</code>: HuggingFace models</p>\n</li>\n<li>\n<p><code>openai-completions</code>: OpenAI Completions API models</p>\n</li>\n<li>\n<p><code>openai-chat-completions</code>: OpenAI Chat Completions API models</p>\n</li>\n<li>\n<p><code>local-completions</code> and <code>local-chat-completions</code>: OpenAI API-compatible servers</p>\n</li>\n<li>\n<p><code>textsynth</code>: TextSynth APIs</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>modelArgs</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>A list of paired name and value arguments for the model type. Arguments vary by model provider. You can find further details in the models section of the LM Evaluation Harness library on GitHub. Below are examples for some providers:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>hf</code>: The model designation for the HuggingFace provider</p>\n</li>\n<li>\n<p><code>local-completions</code>: An OpenAI API-compatible server</p>\n</li>\n<li>\n<p><code>local-chat-completions</code>: An OpenAI API-compatible server</p>\n</li>\n<li>\n<p><code>openai-completions</code>: OpenAI Completions API models</p>\n</li>\n<li>\n<p><code>openai-chat-completions</code>: ChatCompletions API models</p>\n</li>\n<li>\n<p><code>textsynth</code>: TextSynth APIs</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>taskList.taskNames</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Specifies a list of tasks supported by <code>lm-evaluation-harness</code>.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>taskList.taskRecipes</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Specifies the task using the Unitxt recipe format:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>card</code>: Use the <code>name</code> to specify a Unitxt card or <code>ref</code> to refer to a custom card:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>name</code>: Specifies a Unitxt card from the catalog section of the Unitxt. Use the card ID as the value. For example, the ID of the Wnli card is <code>cards.wnli</code>.</p>\n</li>\n<li>\n<p><code>ref</code>: Specifies the reference name of a custom card as defined in the <code>custom</code> section. If the dataset used by the custom card requires an API key from an environment variable or a persistent volume, configure the necessary resources in the <code>pod</code> field.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><code>template</code>: Specifies a Unitxt template from the Unitxt catalog. Use <code>name</code> to specify a Unitxt catalog template or <code>ref</code> to refer to a custom template:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>name</code>: Specifies a Unitxt template from the catalog of cards on the Unitxt website. Use the template&#8217;s ID as the value.</p>\n</li>\n<li>\n<p><code>ref</code>: Specifies the reference name of a custom template as defined in the <code>custom</code> section.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><code>systemPrompt</code>: Use <code>name</code> to specify a Unitxt catalog system prompt or <code>ref</code> to refer to a custom prompt:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>name</code>: Specifies a Unitxt system prompt from the catalog on the Unitxt website. Use the system prompt&#8217;s ID as the value.</p>\n</li>\n<li>\n<p><code>ref</code>: Specifies the reference name of a custom system prompt as defined in the <code>custom</code> section.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><code>task</code> (optional): Specifies a Unitxt task from the Unitxt catalog. Use the task ID as the value. A Unitxt card has a predefined task. Only specify a value for this if you want to run a different task.</p>\n</li>\n<li>\n<p><code>metrics</code> (optional):  Specifies a Unitxt task from the Unitxt catalog. Use the metric ID as the value. A Unitxt task has a set of pre-defined metrics. Only specify a set of metrics if you need different metrics.</p>\n</li>\n<li>\n<p><code>format</code> (optional): Specifies a Unitxt format from the Unitxt catalog. Use the format ID as the value.</p>\n</li>\n<li>\n<p><code>loaderLimit</code> (optional): Specifies the maximum number of instances per stream to be returned from the loader. You can use this parameter to reduce loading time in large datasets.</p>\n</li>\n<li>\n<p><code>numDemos</code> (optional): Number of few-shot to be used.</p>\n</li>\n<li>\n<p><code>demosPoolSize</code> (optional): Size of the few-shot pool.</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>numFewShot</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Sets the number of few-shot examples to place in context. If you are using a task from Unitxt, do not use this field. Use <code>numDemos</code> under the <code>taskRecipes</code> instead.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>limit</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Set a limit to run the tasks instead of running the entire dataset. Accepts either an integer or a float between <code>0.0</code> and <code>1.0</code>.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>genArgs</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Maps to the <code>--gen_kwargs</code> parameter for the <code>lm-evaluation-harness</code>. For more information, see the LM Evaluation Harness documentation on GitHub.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>logSamples</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If this flag is passed, then the model outputs and the text fed into the model are saved at per-prompt level.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>batchSize</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Specifies the batch size for the evaluation in integer format. The <code>auto:N</code> batch size is not used for API models, but numeric batch sizes are used for APIs.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>pod</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Specifies extra information for the <code>lm-eval</code> job pod:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>container</code>: Specifies additional container settings for the <code>lm-eval</code> container.</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>env</code>: Specifies environment variables. This parameter uses the <code>EnvVar</code> data structure of Kubernetes.</p>\n</li>\n<li>\n<p><code>volumeMounts</code>: Mounts the volumes into the <code>lm-eval</code> container.</p>\n</li>\n<li>\n<p><code>resources</code>: Specifies the resources for the <code>lm-eval</code> container.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><code>volumes</code>: Specifies the volume information for the <code>lm-eval</code> and other containers. This parameter uses the <code>Volume</code> data structure of Kubernetes.</p>\n</li>\n<li>\n<p><code>sideCars</code>: A list of containers that run along with the <code>lm-eval</code> container. This parameter uses the <code>Container</code> data structure of Kubernetes.</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>outputs</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">This parameter defines a custom output location to store the the evaluation results. Only Persistent Volume Claims (PVC) are supported.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>outputs.pvcManaged</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Creates an operator-managed PVC to store the job results. The PVC is named <code>&lt;job-name&gt;-pvc</code> and is owned by the <code>LMEvalJob</code>. After the job finishes, the PVC is still available, but it is deleted with the <code>LMEvalJob</code>. Supports the following fields:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>size</code>: The PVC size, compatible with standard PVC syntax (for example, 5Gi).</p>\n</li>\n</ul>\n</div></div></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>outputs.pvcName</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Binds an existing PVC to a job by specifying its name. The PVC must be created separately and must already exist when creating the job.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>allowOnline</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If this parameter is set to <code>true</code>, the LMEval job downloads artifacts as needed (for example, models, datasets or tokenizers). If set to <code>false</code>, artifacts are not downloaded and are pulled from local storage instead. This setting is disabled by default. If you want to enable <code>allowOnline</code> mode, you can deploy a new <code>LMEvalJob</code> CR with <code>allowOnline</code> set to <code>true</code> as long as the <code>DataScienceCluster</code> resource specification <code>permitOnline</code> is also set to <code>true</code>.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>allowCodeExecution</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">If this parameter is set to <code>true</code>, the LMEval job runs the necessary code for preparing models or datasets. If set to <code>false</code> it does not run downloaded code. The default setting for this parameter is <code>false</code>. If you want to enable <code>allowCodeExecution</code> mode, you can deploy a new <code>LMEvalJob</code> CR with <code>allowCodeExecution</code> set to <code>true</code> as long as the <code>DataScienceCluster</code> resource specification <code>permitCodeExecution</code> is also set to <code>true</code>.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>offline</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Mount a PVC as the local storage for models and datasets.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>systemInstruction</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">(Optional) Sets the system instruction for all prompts passed to the evaluated model.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>chatTemplate</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Applies the specified chat template to prompts. Contains two fields:\n* <code>enabled</code>: If set to <code>true</code>, a chat template is used. If set to <code>false</code>, no template is used.\n* <code>name</code>: Uses the template name, if provided. If no name argument is provided, uses the default template for the model.</p>\n</div></div></td>\n</tr>\n</tbody>\n</table>\n<div class=\"sect3\">\n<h4 id=\"_properties_for_setting_up_custom_unitxt_cards_templates_or_system_prompts\">Properties for setting up custom Unitxt cards, templates, or system prompts</h4>\n<div class=\"paragraph\">\n<p>You can choose to set up custom Unitxt cards, templates, or system prompts. Use the parameters set out in the Custom Unitxt parameters table in addition to the preceding table parameters to set customized Unitxt items:</p>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 4. Custom Unitxt parameters</caption>\n<colgroup>\n<col style=\"width: 28.5714%;\">\n<col style=\"width: 71.4286%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Parameter</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>taskList.custom</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><div class=\"content\"><div class=\"paragraph\">\n<p>Defines one or more custom resources that is referenced in a task recipe. The following custom cards, templates, and system prompts are supported:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>cards</code>: Defines custom cards to use, each with a <code>name</code> and <code>value</code> field:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>name</code>: The name of this custom card that is referenced in the <code>card.ref</code> field of a task recipe.</p>\n</li>\n<li>\n<p><code>value</code>: A JSON string for a custom Unitxt card that contains the custom dataset. To compose a custom card, store it as a JSON file, and use the JSON content as the value. If the dataset used by the custom card needs an API key from an environment variable or a persistent volume, set up corresponding resources under the <code>pod</code> field in the <code>LMEvalJob`</code> properties table.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><code>templates</code>:  Define custom templates to use, each with a <code>name</code> and <code>value</code> field:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>name</code>: The name of this custom template that is referenced in the <code>template.ref</code> field of a  task recipe.</p>\n</li>\n<li>\n<p><code>value</code>: A JSON string for a custom Unitxt template. Store <code>value</code> as a JSON file and use the JSON content as the value of this field.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p><code>systemPrompts</code>: Defines custom system prompts to use, each with a <code>name</code> and <code>value</code> field:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>name</code>: The name of this custom system prompt that is referenced in the <code>systemPrompt.ref</code> field of a task recipe.</p>\n</li>\n<li>\n<p><code>value</code>: A string for a custom Unitxt system prompt. You can see an overview of the different components that make up a prompt format, including the system prompt, on the Unitxt website.</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div></div></td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"performing-model-evaluations-in-the-dashboard_evaluate\">Performing model evaluations in the dashboard</h3>\n<div class=\"paragraph _abstract\">\n<p>LM-Eval is a Language Model Evaluation as a Service (LM-Eval-aaS) feature integrated into the TrustyAI Operator. It offers a unified framework for testing generative language models across a wide variety of evaluation tasks.\nYou can use LM-Eval through the Open Data Hub dashboard or the OpenShift CLI (<code>oc</code>).\nThese instructions are for using the dashboard.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub with administrator privileges.</p>\n</li>\n<li>\n<p>You have enabled the TrustyAI component, as described in <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#enabling-trustyai-component_monitor\">Enabling the TrustyAI component</a>.</p>\n</li>\n<li>\n<p>You have created a project in Open Data Hub.</p>\n</li>\n<li>\n<p>You have deployed an LLM model in your project.</p>\n</li>\n</ul>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>By default, the <strong>Develop &amp; train</strong> &#8594; <strong>Evaluations</strong> page is hidden from the dashboard navigation menu. To show the <strong>Develop &amp; train</strong> &#8594; <strong>Evaluations</strong> page in the dashboard, go to the <code>OdhDashboardConfig</code> custom resource (CR) in Open Data Hub and set the <code>disableLMEval</code> value to <code>false</code>. For more information about enabling dashboard configuration options, see <a href=\"https://opendatahub.io/docs/managing-resources/#ref-dashboard-configuration-options_dashboard\">Dashboard configuration options</a>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In the dashboard, click <strong>Develop &amp; train</strong> &#8594; <strong>Evaluations</strong>. The Evaluations page opens. It contains:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>A <strong>Start evaluation run</strong> button. If you have not run any previous evaluations, only this button is displayed.</p>\n</li>\n<li>\n<p>A list of evaluations you have previously run, if any exist.</p>\n</li>\n<li>\n<p>A <strong>Project</strong> dropdown option you can click to show the evaluations relating to one project instead of all projects.</p>\n</li>\n<li>\n<p>A filter to sort your evaluations by model or evaluation name.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<p>The following table outlines the elements and functions of the evaluations list:</p>\n</div>\n</li>\n</ol>\n</div>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 5. Evaluations list components</caption>\n<colgroup>\n<col style=\"width: 20%;\">\n<col style=\"width: 80%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Property</th>\n<th class=\"tableblock halign-left valign-top\">Function</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Evaluation</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The name of the evaluation.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Model</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The model that was used in the evaluation.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Evaluated</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The date and time when the evaluation was created.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Status</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">The status of your evaluation: running, completed, or failed.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">More options icon</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Click this icon to access the options to delete the evaluation, or download the evaluation log in JSON format.</p></td>\n</tr>\n</tbody>\n</table>\n<div class=\"openblock\">\n<div class=\"content\">\n\n</div>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\" start=\"2\">\n<li>\n<p>From the <strong>Project</strong> dropdown menu, select the namespace of the project where you want to evaluate the model.</p>\n</li>\n<li>\n<p>Click the <strong>Start evaluation run</strong> button. The Model evaluation form is displayed.</p>\n</li>\n<li>\n<p>Fill in the details of the form. The model argument summary is displayed after you complete the form details:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p><strong>Model name</strong>: Select a model from all the deployed LLMs in your project.</p>\n</li>\n<li>\n<p><strong>Evaluation name</strong>: Give your evaluation a unique name.</p>\n</li>\n<li>\n<p><strong>Tasks</strong>: Choose one or more evaluation tasks against which to measure your LLM. The 100 most common evaluation tasks are supported.</p>\n</li>\n<li>\n<p><strong>Model type</strong>: Choose the type of model based on the type of prompt-formatting you use:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p><strong>Local-completion</strong>: You assemble the entire prompt chain yourself. Use this when you want to evaluate models that take a plain text prompt and return a continuation.</p>\n</li>\n<li>\n<p><strong>Local-chat-completion</strong>: The framework injects roles or templates automatically. Use this for models that simulate a conversation by taking a list of chat messages with roles like <code>user</code> and <code>assistant</code> and reply appropriately.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p><strong>Security settings</strong>:</p>\n<div class=\"olist lowerroman\">\n<ol class=\"lowerroman\" type=\"i\">\n<li>\n<p><strong>Available online</strong>: Choose <strong>enable</strong> to allow your model to access the internet to download datasets.</p>\n</li>\n<li>\n<p><strong>Trust remote code</strong>: Choose <strong>enable</strong> to allow your model to trust code from outside of the project namespace.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The <strong>Security settings</strong> section is grayed out if the security option in global settings is set to <code>active</code>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Observe that a model argument summary is displayed as soon as you fill in the form details.</p>\n</li>\n<li>\n<p>Complete the tokenizer settings:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p><strong>Tokenized requests</strong>: If set to <code>true</code>, the evaluation requests are broken down into tokens. If set to <code>false</code>, the evaluation dataset remains as raw text.</p>\n</li>\n<li>\n<p><strong>Tokenizer</strong>: Type the model&#8217;s tokenizer URL that is required for the evaluations.</p>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Click <strong>Evaluate</strong>. The screen returns to the model evaluation page of your project and your job is displayed in the evaluations list.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"ulist\">\n<ul>\n<li>\n<p>It can take time for your evaluation to complete, depending on factors including hardware support, model size, and the type of evaluation task(s). The status column reports the current status of the evaluation: <em>completed</em>, <em>running</em>, or <em>failed</em>.</p>\n</li>\n<li>\n<p>If your evaluation fails, the evaluation pod logs in your cluster provide more information.</p>\n</li>\n</ul>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"lmeval-scenarios_evaluate\">LM-Eval scenarios</h3>\n<div class=\"paragraph _abstract\">\n<p>The following procedures outline example scenarios that can be useful for an LM-Eval setup.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"accessing-hugging-face-models-with-an-environment-variable-token_evaluate\">Accessing Hugging Face models with an environment variable token</h4>\n<div class=\"paragraph _abstract\">\n<p>If the <code>LMEvalJob</code> needs to access a model on HuggingFace with the access token, you can set up the <code>HF_TOKEN</code> as one of the environment variables for the <code>lm-eval</code> container.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>Your cluster administrator has installed Open Data Hub and enabled the TrustyAI service for the project where the models are deployed.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>To start an evaluation job for a <code>huggingface</code> model, apply the following YAML file to your project through the CLI:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  model: hf\n  modelArgs:\n  - name: pretrained\n    value: huggingfacespace/model\n  taskList:\n    taskNames:\n    - unfair_tos/\n  logSamples: true\n  pod:\n    container:\n      env:\n      - name: HF_TOKEN\n        value: \"My HuggingFace token\"</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>For example:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f &lt;yaml_file&gt; -n &lt;project_name&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>(Optional) You can also create a secret to store the token, then refer the key from the <code>secretKeyRef</code> object using the following reference syntax:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>env:\n  - name: HF_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: my-secret\n        key: hf-token</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"using-a-custom-unitxt-card_evaluate\">Using a custom Unitxt card</h4>\n<div class=\"paragraph _abstract\">\n<p>You can run evaluations using custom Unitxt cards. To do this, include the custom Unitxt card in JSON format within the <code>LMEvalJob</code> YAML.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>Your cluster administrator has installed Open Data Hub and enabled the TrustyAI service for the project where the models are deployed.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Pass a custom Unitxt Card in JSON format:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  model: hf\n  modelArgs:\n  - name: pretrained\n    value: google/flan-t5-base\n  taskList:\n    taskRecipes:\n    - template: \"templates.classification.multi_class.relation.default\"\n      card:\n        custom: |\n          {\n            \"__type__\": \"task_card\",\n            \"loader\": {\n              \"__type__\": \"load_hf\",\n              \"path\": \"glue\",\n              \"name\": \"wnli\"\n            },\n            \"preprocess_steps\": [\n              {\n                \"__type__\": \"split_random_mix\",\n                \"mix\": {\n                  \"train\": \"train[95%]\",\n                  \"validation\": \"train[5%]\",\n                  \"test\": \"validation\"\n                }\n              },\n              {\n                \"__type__\": \"rename\",\n                \"field\": \"sentence1\",\n                \"to_field\": \"text_a\"\n              },\n              {\n                \"__type__\": \"rename\",\n                \"field\": \"sentence2\",\n                \"to_field\": \"text_b\"\n              },\n              {\n                \"__type__\": \"map_instance_values\",\n                \"mappers\": {\n                  \"label\": {\n                    \"0\": \"entailment\",\n                    \"1\": \"not entailment\"\n                  }\n                }\n              },\n              {\n                \"__type__\": \"set\",\n                \"fields\": {\n                  \"classes\": [\n                    \"entailment\",\n                    \"not entailment\"\n                  ]\n                }\n              },\n              {\n                \"__type__\": \"set\",\n                \"fields\": {\n                  \"type_of_relation\": \"entailment\"\n                }\n              },\n              {\n                \"__type__\": \"set\",\n                \"fields\": {\n                  \"text_a_type\": \"premise\"\n                }\n              },\n              {\n                \"__type__\": \"set\",\n                \"fields\": {\n                  \"text_b_type\": \"hypothesis\"\n                }\n              }\n            ],\n            \"task\": \"tasks.classification.multi_class.relation\",\n            \"templates\": \"templates.classification.multi_class.relation.all\"\n          }\n  logSamples: true</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Inside the custom card specify the Hugging Face dataset loader:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>\"loader\": {\n              \"__type__\": \"load_hf\",\n              \"path\": \"glue\",\n              \"name\": \"wnli\"\n            },</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>(Optional) You can use other Unitxt loaders (found on the Unitxt website) that contain the <code>volumes</code> and <code>volumeMounts</code> parameters to mount the dataset from persistent volumes. For example, if you use the <code>LoadCSV</code> Unitxt command, mount the files to the container and make the dataset accessible for the evaluation process.</p>\n</li>\n</ol>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The provided scenario example does not work on <code>s390x</code>, as it uses a Parquet-type dataset, which is not supported on this architecture. To run the scenario on <code>s390x</code>, use a task with a non-Parquet dataset.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"using-pvcs-as-storage_evaluate\">Using PVCs as storage</h4>\n<div class=\"paragraph _abstract\">\n<p>To use a PVC as storage for the <code>LMEvalJob</code> results, you can use either managed PVCs or existing PVCs. Managed PVCs are managed by the TrustyAI operator. Existing PVCs are created by the end-user before the <code>LMEvalJob</code> is created.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>If both managed and existing PVCs are referenced in outputs, the TrustyAI operator defaults to the managed PVC.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>Your cluster administrator has installed Open Data Hub and enabled the TrustyAI service for the project where the models are deployed.</p>\n</li>\n</ul>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_managed_pvcs\">Managed PVCs</h5>\n<div class=\"paragraph\">\n<p>To create a managed PVC, specify its size. The managed PVC is named <code>&lt;job-name&gt;-pvc</code> and is available after the job finishes. When the <code>LMEvalJob</code> is deleted, the managed PVC is also deleted.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Procedure</div>\n<ul>\n<li>\n<p>Enter the following code:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  # other fields omitted ...\n  outputs:\n    pvcManaged:\n      size: 5Gi</code></pre>\n</div>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Notes on the code</div>\n<ul>\n<li>\n<p><code>outputs</code> is the section for specifying custom storage locations</p>\n</li>\n<li>\n<p><code>pvcManaged</code> will create an operator-managed PVC</p>\n</li>\n<li>\n<p><code>size</code> (compatible with standard PVC syntax) is the only supported value</p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect4\">\n<h5 id=\"_existing_pvcs\">Existing PVCs</h5>\n<div class=\"paragraph\">\n<p>To use an existing PVC, pass its name as a reference. The PVC must exist when you create the <code>LMEvalJob</code>.\nThe PVC is not managed by the TrustyAI operator, so it is available after deleting the <code>LMEvalJob</code>.</p>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create a PVC. An example is the following:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: \"my-pvc\"\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Reference the new PVC from the <code>LMEvalJob</code>.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob-sample\nspec:\n  # other fields omitted ...\n  outputs:\n    pvcName: \"my-pvc\"</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"using-a-kserve-inference-service_evaluate\">Using a KServe Inference Service</h4>\n<div class=\"paragraph _abstract\">\n<p>To run an evaluation job on an <code>InferenceService</code> which is already deployed and running in your namespace, define your <code>LMEvalJob</code> CR, then apply this CR into the same namespace as your model.</p>\n</div>\n<div class=\"paragraph\">\n<p>NOTE</p>\n</div>\n<div class=\"openblock\">\n<div class=\"content\">\n<div class=\"paragraph\">\n<p>The following example only works with Hugging Face or vLLM-based model-serving runtimes.</p>\n</div>\n</div>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>Your cluster administrator has installed Open Data Hub and enabled the TrustyAI service for the project where the models are deployed.</p>\n</li>\n<li>\n<p>You have a namespace that contains an InferenceService with a vLLM model. This example assumes that a vLLM model is already deployed in your cluster.</p>\n</li>\n<li>\n<p>Your cluster has Domain Name System (DNS) configured.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Define your <code>LMEvalJob</code> CR:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>  apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n  name: evaljob\nspec:\n  model: local-completions\n  taskList:\n    taskNames:\n      - mmlu\n  logSamples: true\n  batchSize: 1\n  modelArgs:\n    - name: model\n      value: granite\n    - name: base_url\n      value: $ROUTE_TO_MODEL/v1/completions\n    - name: num_concurrent\n      value:  \"1\"\n    - name: max_retries\n      value:  \"3\"\n    - name: tokenized_requests\n      value: false\n    - name: tokenizer\n      value: huggingfacespace/model\n env:\n   - name: OPENAI_TOKEN\n     valueFrom:\n          secretKeyRef:\n            name: &lt;secret-name&gt;\n            key: token</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Apply this CR into the same namespace as your model.</p>\n</li>\n</ol>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>A pod spins up in your model namespace called <code>evaljob</code>. In the pod terminal, you can see the output via <code>tail -f output/stderr.log</code>.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Notes on the code</div>\n<ul>\n<li>\n<p><code>base_url</code> should be set to the route/service URL of your model. Make sure to include the <code>/v1/completions</code> endpoint in the URL.</p>\n</li>\n<li>\n<p><code>env.valueFrom.secretKeyRef.name</code> should point to a secret that contains a token that can authenticate to your model. <code>secretRef.name</code> should be the secret&#8217;s name in the namespace, while <code>secretRef.key</code> should point at the token&#8217;s key within the secret.</p>\n</li>\n<li>\n<p><code>secretKeyRef.name</code> can equal the output of:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>oc get secrets -o custom-columns=SECRET:.metadata.name --no-headers | grep user-one-token</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p><code>secretKeyRef.key</code> is set to <code>token</code></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"setting-up-lmeval-s3-support_evaluate\">Setting up LM-Eval S3 Support</h4>\n<div class=\"paragraph _abstract\">\n<p>Learn how to set up S3 support for your LM-Eval service.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>Your cluster administrator has installed Open Data Hub and enabled the TrustyAI service for the project where the models are deployed.</p>\n</li>\n<li>\n<p>You have a namespace that contains an S3-compatible storage service and bucket.</p>\n</li>\n<li>\n<p>You have created an <code>LMEvalJob</code> that references the S3 bucket containing your model and dataset.</p>\n</li>\n<li>\n<p>You have an S3 bucket that contains the model files and the dataset(s) to be evaluated.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create a Kubernetes Secret containing your S3 connection details:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: Secret\nmetadata:\n    name: \"s3-secret\"\n    namespace: test\n    labels:\n        opendatahub.io/dashboard: \"true\"\n        opendatahub.io/managed: \"true\"\n    annotations:\n        opendatahub.io/connection-type: s3\n        openshift.io/display-name: \"S3 Data Connection - LMEval\"\ndata:\n    AWS_ACCESS_KEY_ID: BASE64_ENCODED_ACCESS_KEY  # Replace with your key\n    AWS_SECRET_ACCESS_KEY: BASE64_ENCODED_SECRET_KEY  # Replace with your key\n    AWS_S3_BUCKET: BASE64_ENCODED_BUCKET_NAME  # Replace with your bucket name\n    AWS_S3_ENDPOINT: BASE64_ENCODED_ENDPOINT  # Replace with your endpoint URL (for example,  https://s3.amazonaws.com)\n    AWS_DEFAULT_REGION: BASE64_ENCODED_REGION  # Replace with your region\ntype: Opaque</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>All values must be <code>base64</code> encoded. For example: <code>echo -n \"my-bucket\" | base64</code></p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Deploy the <code>LMEvalJob</code> CR that references the S3 bucket containing your model and dataset:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n    name: evaljob-sample\nspec:\n    allowOnline: false\n    model: hf  # Model type (HuggingFace in this example)\n    modelArgs:\n        - name: pretrained\n          value: /opt/app-root/src/hf_home/flan  # Path where model is mounted in container\n    taskList:\n        taskNames:\n            - arc_easy  # The evaluation task to run\n    logSamples: true\n    offline:\n        storage:\n            s3:\n                accessKeyId:\n                    name: s3-secret\n                    key: AWS_ACCESS_KEY_ID\n                secretAccessKey:\n                    name: s3-secret\n                    key: AWS_SECRET_ACCESS_KEY\n                bucket:\n                    name: s3-secret\n                    key: AWS_S3_BUCKET\n                endpoint:\n                    name: s3-secret\n                    key: AWS_S3_ENDPOINT\n                region:\n                    name: s3-secret\n                    key: AWS_DEFAULT_REGION\n                path: \"\"  # Optional subfolder within bucket\n                verifySSL: false</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"literalblock\">\n<div class=\"content\">\n<pre>The `LMEvalJob` will copy all the files from the specified bucket/path. If your bucket contains many files and you only want to use a subset, set the `path` field to the specific sub-folder containing the files that you require. For example use `path: \"my-models/\"`.</pre>\n</div>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Set up a secure connection using SSL.</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Create a ConfigMap object with your CA certificate:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: s3-ca-cert\n  namespace: test\n  annotations:\n    service.beta.openshift.io/inject-cabundle: \"true\"  # For injection\ndata: {}  # OpenShift will inject the service CA bundle\n# Or add your custom CA:\n# data:\n#   ca.crt: |-\n#     -----BEGIN CERTIFICATE-----\n#     ...your CA certificate content...\n#     -----END CERTIFICATE-----</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Update the <code>LMEvalJob</code> to use SSL verification:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n    name: evaljob-sample\nspec:\n    # ... same as above ...\n    offline:\n        storage:\n            s3:\n                # ... same as above ...\n                verifySSL: true  # Enable SSL verification\n                caBundle:\n                    name: s3-ca-cert  # ConfigMap name containing your CA\n                    key: service-ca.crt  # Key in ConfigMap containing the certificate</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Verification</div>\n<ol class=\"arabic\">\n<li>\n<p>After deploying the <code>LMEvalJob</code>, open the <code>kubectl</code> command-line and enter this command to check its status: <code>kubectl logs -n test job/evaljob-sample -n test</code></p>\n</li>\n<li>\n<p>View the logs with the <code>kubectl</code> command <code>kubectl logs -n test job/&lt;job-name&gt;</code> to make sure it has functioned correctly.</p>\n</li>\n<li>\n<p>The results are displayed in the logs after the evaluation is completed.</p>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"using-llm-as-a-judge-metrics-with-lmeval_evaluate\">Using LLM-as-a-Judge metrics with LM-Eval</h4>\n<div class=\"paragraph _abstract\">\n<p>You can use a large language model (LLM) to assess the quality of outputs from another LLM, known as LLM-as-a-Judge (LLMaaJ).</p>\n</div>\n<div class=\"paragraph\">\n<p>You can use LLMaaJ to:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Assess work with no clearly correct answer, such as creative writing.</p>\n</li>\n<li>\n<p>Judge quality characteristics such as helpfulness, safety, and depth.</p>\n</li>\n<li>\n<p>Augment traditional quantitative measures that are used to evaluate a model&#8217;s performance (for example, <code>ROUGE</code> metrics).</p>\n</li>\n<li>\n<p>Test specific quality aspects of your model output.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>Follow the custom quality assessment example below to learn more about using your own metrics criteria with LM-Eval to evaluate model responses.</p>\n</div>\n<div class=\"paragraph\">\n<p>This example uses <a href=\"https://www.unitxt.ai\">Unitxt</a> to define custom metrics and to see how the model (<a href=\"https://www.huggingface.co/google/flan-t5-small\">flan-t5-small</a>) answers questions from MT-Bench, a standard benchmark. Custom evaluation criteria and instructions from the <a href=\"https://www.huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\">Mistral-7B</a> model are used to rate the answers from 1-10, based on helpfulness, accuracy, and detail.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Your cluster administrator has installed Open Data Hub and enabled the TrustyAI service for the project where the models are deployed.</p>\n</li>\n<li>\n<p>You are familiar with how to use Unitxt.</p>\n</li>\n<li>\n<p>You have set the following parameters:</p>\n<table class=\"tableblock frame-all grid-all stretch\">\n<caption class=\"title\">Table 6. Parameters</caption>\n<colgroup>\n<col style=\"width: 33.3333%;\">\n<col style=\"width: 66.6667%;\">\n</colgroup>\n<thead>\n<tr>\n<th class=\"tableblock halign-left valign-top\">Parameter</th>\n<th class=\"tableblock halign-left valign-top\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Custom template</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Tells the judge to assign a score between 1 and 10 in a standardized format, based on specific criteria.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>processors.extract_mt_bench_rating_judgment</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Pulls the numerical rating from the judge&#8217;s response.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\"><code>formats.models.mistral.instruction</code></p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Formats the prompts for the Mistral model.</p></td>\n</tr>\n<tr>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Custom LLM-as-judge metric</p></td>\n<td class=\"tableblock halign-left valign-top\"><p class=\"tableblock\">Uses Mistral-7B with your custom instructions.</p></td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI (<code>oc</code>) as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc login <em>&lt;openshift_cluster_url&gt;</em> -u <em>&lt;admin_username&gt;</em> -p <em>&lt;password&gt;</em></code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Apply the following manifest by using the <code>oc apply -f -</code> command. The YAML content defines a custom evaluation job (<code>LMEvalJob</code>), the namespace, and the location of the model you want to evaluate.\nThe YAML contains the following instructions:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Which model to evaluate.</p>\n</li>\n<li>\n<p>What data to use.</p>\n</li>\n<li>\n<p>How to format inputs and outputs.</p>\n</li>\n<li>\n<p>Which judge model to use.</p>\n</li>\n<li>\n<p>How to extract and log results.</p>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>You can also put the YAML manifest into a file using a text editor and then apply it by using the <code>oc apply -f file.yaml</code> command.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-YAML\" data-lang=\"YAML\">apiVersion: trustyai.opendatahub.io/v1alpha1\nkind: LMEvalJob\nmetadata:\n name: custom-eval\n namespace: test\nspec:\n allowOnline: true\n allowCodeExecution: true\n model: hf\n modelArgs:\n   - name: pretrained\n     value: google/flan-t5-small\ntaskList:\n taskRecipes:\n     - card:\n         custom: |\n           {\n               \"__type__\": \"task_card\",\n               \"loader\": {\n                   \"__type__\": \"load_hf\",\n                   \"path\": \"OfirArviv/mt_bench_single_score_gpt4_judgement\",\n                   \"split\": \"train\"\n               },\n               \"preprocess_steps\": [\n                   {\n                       \"__type__\": \"rename_splits\",\n                       \"mapper\": {\n                           \"train\": \"test\"\n                       }\n                   },\n                   {\n                       \"__type__\": \"filter_by_condition\",\n                       \"values\": {\n                           \"turn\": 1\n                       },\n                       \"condition\": \"eq\"\n                   },\n                   {\n                       \"__type__\": \"filter_by_condition\",\n                       \"values\": {\n                           \"reference\": \"[]\"\n                       },\n                       \"condition\": \"eq\"\n                   },\n                   {\n                       \"__type__\": \"rename\",\n                       \"field_to_field\": {\n                           \"model_input\": \"question\",\n                           \"score\": \"rating\",\n                           \"category\": \"group\",\n                           \"model_output\": \"answer\"\n                       }\n                   },\n                   {\n                       \"__type__\": \"literal_eval\",\n                       \"field\": \"question\"\n                   },\n                   {\n                       \"__type__\": \"copy\",\n                       \"field\": \"question/0\",\n                       \"to_field\": \"question\"\n                   },\n                   {\n                       \"__type__\": \"literal_eval\",\n                       \"field\": \"answer\"\n                   },\n                   {\n                       \"__type__\": \"copy\",\n                       \"field\": \"answer/0\",\n                       \"to_field\": \"answer\"\n                   }\n               ],\n               \"task\": \"tasks.response_assessment.rating.single_turn\",\n               \"templates\": [\n                   \"templates.response_assessment.rating.mt_bench_single_turn\"\n               ]\n           }\n       template:\n         ref: response_assessment.rating.mt_bench_single_turn\n       format: formats.models.mistral.instruction\n       metrics:\n       - ref: llmaaj_metric\n   custom:\n     templates:\n       - name: response_assessment.rating.mt_bench_single_turn\n         value: |\n           {\n               \"__type__\": \"input_output_template\",\n               \"instruction\": \"Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \\\"[[rating]]\\\", for example: \\\"Rating: [[5]]\\\".\\n\\n\",\n               \"input_format\": \"[Question]\\n{question}\\n\\n[The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's Answer]\",\n               \"output_format\": \"[[{rating}]]\",\n               \"postprocessors\": [\n                   \"processors.extract_mt_bench_rating_judgment\"\n               ]\n           }\n     tasks:\n       - name: response_assessment.rating.single_turn\n         value: |\n           {\n               \"__type__\": \"task\",\n               \"input_fields\": {\n                   \"question\": \"str\",\n                   \"answer\": \"str\"\n               },\n               \"outputs\": {\n                   \"rating\": \"float\"\n               },\n               \"metrics\": [\n                   \"metrics.spearman\"\n               ]\n           }\n     metrics:\n       - name: llmaaj_metric\n         value: |\n           {\n               \"__type__\": \"llm_as_judge\",\n               \"inference_model\": {\n                   \"__type__\": \"hf_pipeline_based_inference_engine\",\n                   \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n                   \"max_new_tokens\": 256,\n                   \"use_fp16\": true\n               },\n               \"template\": \"templates.response_assessment.rating.mt_bench_single_turn\",\n               \"task\": \"rating.single_turn\",\n               \"format\": \"formats.models.mistral.instruction\",\n               \"main_score\": \"mistral_7b_instruct_v0_2_huggingface_template_mt_bench_single_turn\"\n           }\n logSamples: true\n pod:\n   container:\n     env:\n       - name: HF_TOKEN\n         valueFrom:\n           secretKeyRef:\n             name: hf-token-secret\n             key: token\n     resources:\n       limits:\n         cpu: '2'\n         memory: 16Gi</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<div class=\"title\">Verification</div>\n<p>A processor extracts the numeric rating from the judge&#8217;s natural language response. The final result is available as part of the LMEval Job Custom Resource (CR).</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The provided scenario example does not work for <code>s390x</code>. The scenario works with non-Parquet type dataset task for <code>s390x</code>.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"using-llama-stack-with-trustyai_evaluate\">Using llama stack with TrustyAI</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph\">\n<p>This section contains tutorials for working with Llama Stack in TrustyAI. These tutorials demonstrate how to use various Llama Stack components and providers to evaluate and work with language models.</p>\n</div>\n<div class=\"paragraph\">\n<p>The following sections describe how to work with Llama Stack and provide example use cases:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Using the Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI</p>\n</li>\n<li>\n<p>Running custom evaluations with LM-Eval Llama Stack external evaluation provider</p>\n</li>\n<li>\n<p>Using the trustyai-fms Guardrails Orchestrator with Llama Stack</p>\n</li>\n</ul>\n</div>\n<div class=\"sect2\">\n<h3 id=\"using-llama-stack-external-evaluation-provider-with-lm-evaluation-harness-in-TrustyAI_evaluate\">Using Llama Stack external evaluation provider with lm-evaluation-harness in TrustyAI</h3>\n<div class=\"paragraph _abstract\">\n<p>This example demonstrates how to evaluate a language model in Open Data Hub using the LMEval Llama Stack external eval provider in a Python workbench. To do this, configure a Llama Stack server to use the LMEval eval provider, register a benchmark dataset, and run a benchmark evaluation job on a language model.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub, version 2.29 or later.</p>\n</li>\n<li>\n<p>You have cluster administrator privileges for your Open Data Hub cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have a large language model (LLM) for chat generation or text classification, or both, deployed in your namespace.</p>\n</li>\n<li>\n<p>You have installed TrustyAI Operator in your Open Data Hub cluster.</p>\n</li>\n<li>\n<p>You have set KServe to Raw Deployment mode in your cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Create and activate a Python virtual environment for this tutorial in your local machine:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">python3 -m venv .venv\nsource .venv/bin/activate</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Install the required packages from the Python Package Index (PyPI):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">pip install \\\n    llama-stack \\\n    llama-stack-client \\\n    llama-stack-provider-lmeval</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create the model route:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">oc create route edge vllm --service=&lt;VLLM_SERVICE&gt; --port=&lt;VLLM_PORT&gt; -n &lt;MODEL_NAMESPACE&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Configure the Llama Stack server. Set the variables to configure the runtime endpoint and namespace. The VLLM_URL value should be the <code>v1/completions</code> endpoint of your model route and the TRUSTYAI_LM_EVAL_NAMESPACE should be the namespace where your model is deployed. For example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">export TRUSTYAI_LM_EVAL_NAMESPACE=&lt;MODEL_NAMESPACE&gt;\nexport MODEL_ROUTE=$(oc get route -n \"$TRUSTYAI_LM_EVAL_NAMESPACE\" | awk '/predictor/{print $2; exit}')\nexport VLLM_URL=\"https://${MODEL_ROUTE}/v1/completions\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Download the <code>providers.d</code> provider configuration directory and the <code>run.yaml</code> execution file:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">curl --create-dirs --output providers.d/remote/eval/trustyai_lmeval.yaml https://raw.githubusercontent.com/trustyai-explainability/llama-stack-provider-lmeval/refs/heads/main/providers.d/remote/eval/trustyai_lmeval.yaml\n\ncurl --create-dirs --output run.yaml https://raw.githubusercontent.com/trustyai-explainability/llama-stack-provider-lmeval/refs/heads/main/run.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Start the Llama Stack server in a virtual environment, which uses port <code>8321</code> by default:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">llama stack run run.yaml --image-type venv</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a Python script in a Jupyter workbench and import the following libraries and modules, to interact with the server and run an evaluation:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">import os\nimport subprocess\n\nimport logging\n\nimport time\nimport pprint</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Start the Llama Stack Python client to interact with the running Llama Stack server:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">BASE_URL = \"http://localhost:8321\"\n\ndef create_http_client():\n    from llama_stack_client import LlamaStackClient\n    return LlamaStackClient(base_url=BASE_URL)\n\nclient = create_http_client()</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Print a list of the current available benchmarks:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">benchmarks = client.benchmarks.list()\n\npprint.pprint(f\"Available benchmarks: {benchmarks}\")</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>LMEval provides access to over 100 preconfigured evaluation datasets. Register the ARC-Easy benchmark, a dataset of grade-school level, multiple-choice science questions:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">client.benchmarks.register(\n    benchmark_id=\"trustyai_lmeval::arc_easy\",\n    dataset_id=\"trustyai_lmeval::arc_easy\",\n    scoring_functions=[\"string\"],\n    provider_benchmark_id=\"string\",\n    provider_id=\"trustyai_lmeval\",\n     metadata={\n        \"tokenizer\": \"google/flan-t5-small\",\n        \"tokenized_requests\": False,\n   }\n)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Verify that the benchmark has been registered successfully:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">benchmarks = client.benchmarks.list()\npprint.print(f\"Available benchmarks: {benchmarks}\")</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Run a benchmark evaluation job on your deployed model using the following input. Replace phi-3 with the name of your deployed model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">job = client.eval.run_eval(\n    benchmark_id=\"trustyai_lmeval::arc_easy\",\n    benchmark_config={\n        \"eval_candidate\": {\n            \"type\": \"model\",\n            \"model\": \"phi-3\",\n            \"provider_id\": \"trustyai_lmeval\",\n            \"sampling_params\": {\n                \"temperature\": 0.7,\n                \"top_p\": 0.9,\n                \"max_tokens\": 256\n            },\n        },\n        \"num_examples\": 1000,\n     },\n)\n\nprint(f\"Starting job '{job.job_id}'\")</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Monitor the status of the evaluation job using the following code. The job will run asynchronously, so you can check its status periodically:</p>\n</li>\n</ol>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre>def get_job_status(job_id, benchmark_id):\n    return client.eval.jobs.status(job_id=job_id, benchmark_id=benchmark_id)\n\nwhile True:\n    job = get_job_status(job_id=job.job_id, benchmark_id=\"trustyai_lmeval::arc_easy\")\n    print(job)\n\n    if job.status in ['failed', 'completed']:\n        print(f\"Job ended with status: {job.status}\")\n        break\n\n    time.sleep(20)</pre>\n</div>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\">\n<li>\n<p>Retrieve the evaluation job results once the job status reports back as <code>completed</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">pprint.pprint(client.eval.jobs.retrieve(job_id=job.job_id, benchmark_id=\"trustyai_lmeval::arc_easy\").scores)</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"running-custom-evaluations-with-LMEval-and-llama-stack_evaluate\">Running custom evaluations with LM-Eval and Llama Stack</h3>\n<div class=\"paragraph _abstract\">\n<p>This example demonstrates how to use the <a href=\"https://github.com/trustyai-explainability/llama-stack-provider-lmeval\">LM-Eval Llama Stack external eval provider</a> to evaluate a language model with a custom benchmark. Creating a custom benchmark is useful for evaluating specific model knowledge and behavior.</p>\n</div>\n<div class=\"paragraph\">\n<p>The process involves three steps:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Uploading the task dataset to your Open Data Hub cluster</p>\n</li>\n<li>\n<p>Registering it as a custom benchmark dataset with Llama Stack</p>\n</li>\n<li>\n<p>Running a benchmark evaluation job on a language model</p>\n</li>\n</ul>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have installed Open Data Hub, version 2.29 or later.</p>\n</li>\n<li>\n<p>You have cluster administrator privileges for your Open Data Hub cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have a large language model (LLM) for chat generation or text classification, or both, deployed on vLLM Serving Runtime in your Open Data Hub cluster.</p>\n</li>\n<li>\n<p>You have installed TrustyAI Operator in your Open Data Hub cluster.</p>\n</li>\n<li>\n<p>You have set KServe to Raw Deployment mode in your cluster.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Upload your custom benchmark dataset to your OpenShift cluster using a PersistentVolumeClaim (PVC) and a temporary pod. Create a PVC named <code>my-pvc</code> to store your dataset. Run the following command in your CLI, replacing &lt;MODEL_NAMESPACE&gt; with the namespace of your language model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">oc apply -n &lt;MODEL_NAMESPACE&gt; -f - &lt;&lt; EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: my-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nEOF</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a pod object named <code>dataset-storage-pod</code> to download the task dataset into the PVC. This pod is used to copy your dataset from your local machine to the Open Data Hub cluster:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">oc apply -n &lt;MODEL_NAMESPACE&gt; -f - &lt;&lt; EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dataset-storage-pod\nspec:\n  containers:\n  - name: dataset-container\n    image: 'quay.io/prometheus/busybox:latest'\n    command: [\"/bin/sh\", \"-c\", \"sleep 3600\"]\n    volumeMounts:\n    - mountPath: \"/data/upload_files\"\n      name: dataset-storage\n  volumes:\n  - name: dataset-storage\n    persistentVolumeClaim:\n      claimName: my-pvc\nEOF</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Copy your locally stored task dataset to the pod to place it within the PVC. In this example, the dataset is named <code>example-dk-bench-input-bmo.jsonl</code> locally and it is copied to the <code>dataset-storage-pod</code> under the path <code>/data/upload_files/</code>.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">oc cp example-dk-bench-input-bmo.jsonl dataset-storage-pod:/data/upload_files/example-dk-bench-input-bmo.jsonl -n &lt;MODEL_NAMESPACE&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Once the custom dataset is uploaded to the PVC, register it as a benchmark for evaluations. At a minimum, provide the following metadata and replace the <code>DK_BENCH_DATASET_PATH</code> and any other metadata fields to match your specific configuration:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>The TrustyAI LM-Eval Tasks GitHub web address</p>\n</li>\n<li>\n<p>Your branch</p>\n</li>\n<li>\n<p>The commit hash and path of the custom task.</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">client.benchmarks.register(\n    benchmark_id=\"trustyai_lmeval::dk-bench\",\n    dataset_id=\"trustyai_lmeval::dk-bench\",\n    scoring_functions=[\"accuracy\"],\n    provider_benchmark_id=\"dk-bench\",\n    provider_id=\"trustyai_lmeval\",\n    metadata={\n        \"custom_task\": {\n            \"git\": {\n                \"url\": \"https://github.com/trustyai-explainability/lm-eval-tasks.git\",\n                \"branch\": \"main\",\n                \"commit\": \"8220e2d73c187471acbe71659c98bccecfe77958\",\n                \"path\": \"tasks/\",\n            }\n        },\n        \"env\": {\n            # Path of the dataset inside the PVC\n            \"DK_BENCH_DATASET_PATH\": \"/opt/app-root/src/hf_home/example-dk-bench-input-bmo.jsonl\",\n            \"JUDGE_MODEL_URL\": \"http://phi-3-predictor:8080/v1/chat/completions\",\n            # For simplicity, we use the same model as the one being evaluated\n            \"JUDGE_MODEL_NAME\": \"phi-3\",\n            \"JUDGE_API_KEY\": \"\",\n        },\n        \"tokenized_requests\": False,\n        \"tokenizer\": \"google/flan-t5-small\",\n        \"input\": {\"storage\": {\"pvc\": \"my-pvc\"}}\n    },\n)</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Run a benchmark evaluation on your model:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-bash\" data-lang=\"bash\">job = client.eval.run_eval(\n    benchmark_id=\"trustyai_lmeval::dk-bench\",\n    benchmark_config={\n        \"eval_candidate\": {\n            \"type\": \"model\",\n            \"model\": \"phi-3\",\n            \"provider_id\": \"trustyai_lmeval\",\n            \"sampling_params\": {\n                \"temperature\": 0.7,\n                \"top_p\": 0.9,\n                \"max_tokens\": 256\n            },\n        },\n        \"num_examples\": 1000,\n     },\n)\n\nprint(f\"Starting job '{job.job_id}'\")</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Monitor the status of the evaluation job. The job runs asynchronously, so you can check its status periodically:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-python\" data-lang=\"python\">import time\ndef get_job_status(job_id, benchmark_id):\n    return client.eval.jobs.status(job_id=job_id, benchmark_id=benchmark_id)\n\nwhile True:\n    job = get_job_status(job_id=job.job_id, benchmark_id=\"trustyai_lmeval::dk-bench\")\n    print(job)\n\n    if job.status in ['failed', 'completed']:\n        print(f\"Job ended with status: {job.status}\")\n        break\n\n    time.sleep(20)</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"detecting-pii-by-using-guardrails-with-llama-stack_evaluate\">Detecting personally identifiable information (PII) by using Guardrails with Llama Stack</h3>\n<div class=\"paragraph _abstract\">\n<p>The <code>trustyai_fms</code> Orchestrator server is an external provider for Llama Stack that allows you to configure and use the Guardrails Orchestrator and compatible detection models through the Llama Stack API.\nThis implementation of Llama Stack combines <a href=\"https://github.com/foundation-model-stack/fms-guardrails-orchestrator\">Guardrails Orchestrator</a> with a suite of community-developed detectors to provide robust content filtering and safety monitoring.</p>\n</div>\n<div class=\"paragraph\">\n<p>This example demonstrates how to use the built-in <a href=\"https://github.com/trustyai-explainability/guardrails-regex-detector\">Guardrails Regex Detector</a> to detect personally identifiable information (PII) with Guardrails Orchestrator as Llama Stack safety guardrails, using the <code>LlamaStack</code> Operator to deploy a distribution in your Open Data Hub namespace.</p>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Guardrails Orchestrator with Llama Stack is not supported on <code>s390x</code>, as it requires the LlamaStack Operator, which is currently unavailable for this architecture.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have installed Open Data Hub, version 2.29 or later.</p>\n</li>\n<li>\n<p>You have installed Open Data Hub, version 2.20 or later.</p>\n</li>\n<li>\n<p>You have a large language model (LLM) for chat generation or text classification, or both, deployed in your namespace.</p>\n</li>\n<li>\n<p>A cluster administrator has installed the following Operators in OpenShift Container Platform:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Red Hat Authorino Operator, version 1.2.1 or later</p>\n</li>\n<li>\n<p>Red Hat OpenShift Service Mesh, version 2.6.7-0 or later</p>\n</li>\n</ul>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>Configure your Open Data Hub environment with the following configurations in the <code>DataScienceCluster</code>. Note that you must manually update the <code>spec.llamastack.managementState</code> field to <code>Managed</code>:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">spec:\n  trustyai:\n    managementState: Managed\n  llamastack:\n    managementState: Managed\n  kserve:\n    defaultDeploymentMode: RawDeployment\n    managementState: Managed\n    nim:\n      managementState: Managed\n    rawDeploymentServiceConfig: Headless\n  serving:\n    ingressGateway:\n      certificate:\n        type: OpenshiftDefaultIngress\n    managementState: Removed\n    name: knative-serving\n  serviceMesh:\n    managementState: Removed</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a project in your Open Data Hub namespace:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">PROJECT_NAME=\"lls-minimal-example\"\noc new-project $PROJECT_NAME</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the Guardrails Orchestrator with regex detectors by applying the Orchestrator configuration for regex-based PII detection:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">cat &lt;&lt;EOF | oc apply -f -\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: fms-orchestr8-config-nlp\ndata:\n  config.yaml: |\n    detectors:\n      regex:\n        type: text_contents\n        service:\n          hostname: \"127.0.0.1\"\n          port: 8080\n        chunker_id: whole_doc_chunker\n        default_threshold: 0.5\n---\napiVersion: trustyai.opendatahub.io/v1alpha1\nkind: GuardrailsOrchestrator\nmetadata:\n  name: guardrails-orchestrator\nspec:\n  orchestratorConfig: \"fms-orchestr8-config-nlp\"\n  enableBuiltInDetectors: true\n  enableGuardrailsGateway: false\n  replicas: 1\nEOF</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>In the same namespace, create a Llama Stack distribution:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: llamastack.io/v1alpha1\nkind: LlamaStackDistribution\nmetadata:\n  name: llamastackdistribution-sample\n  namespace: &lt;PROJECT_NAMESPACE&gt;\nspec:\n  replicas: 1\n  server:\n    containerSpec:\n      env:\n        - name: VLLM_URL\n          value: '${VLLM_URL}'\n        - name: INFERENCE_MODEL\n          value: '${INFERENCE_MODEL}'\n        - name: MILVUS_DB_PATH\n          value: '~/.llama/milvus.db'\n        - name: VLLM_TLS_VERIFY\n          value: 'false'\n        - name: FMS_ORCHESTRATOR_URL\n          value: '${FMS_ORCHESTRATOR_URL}'\n      name: llama-stack\n      port: 8321\n    distribution:\n      name: rh-dev\n    storage:\n      size: 20Gi</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"admonitionblock note\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Note</div>\n</td>\n<td class=\"content\">\n&#8201;&#8212;&#8201; After deploying the <code>LlamaStackDistribution</code> CR, a new pod is created in the same namespace. This pod runs the LlamaStack server for your distribution.\n&#8201;&#8212;&#8201;\n</td>\n</tr>\n</table>\n</div>\n<div class=\"olist arabic\">\n<ol class=\"arabic\" start=\"5\">\n<li>\n<p>Once the Llama Stack server is running, use the <code>/v1/shields</code> endpoint to dynamically register a shield. For example, register a shield that uses regex patterns to detect personally identifiable information (PII).</p>\n</li>\n<li>\n<p>Open a port-forward to access it locally:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">oc -n $PROJECT_NAME port-forward svc/llama-stack 8321:8321</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Use the <code>/v1/shields</code> endpoint to dynamically register a shield. For example, register a shield that uses regex patterns to detect personally identifiable information (PII):</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-json\" data-lang=\"json\">curl -X POST http://localhost:8321/v1/shields \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"shield_id\": \"regex_detector\",\n    \"provider_shield_id\": \"regex_detector\",\n    \"provider_id\": \"trustyai_fms\",\n    \"params\": {\n      \"type\": \"content\",\n      \"confidence_threshold\": 0.5,\n      \"message_types\": [\"system\", \"user\"],\n      \"detectors\": {\n        \"regex\": {\n          \"detector_params\": {\n            \"regex\": [\"email\", \"us-social-security-number\", \"credit-card\"]\n          }\n        }\n      }\n    }\n  }'</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Verify that the shield was registered:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-curl\" data-lang=\"curl\">curl -s http://localhost:8321/v1/shields | jq '.'</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>The following output indicates that the shield has been registered successfully:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-curl\" data-lang=\"curl\">{\n  \"data\": [\n    {\n      \"identifier\": \"regex_detector\",\n      \"provider_resource_id\": \"regex_detector\",\n      \"provider_id\": \"trustyai_fms\",\n      \"type\": \"shield\",\n      \"params\": {\n        \"type\": \"content\",\n        \"confidence_threshold\": 0.5,\n        \"message_types\": [\n          \"system\",\n          \"user\"\n        ],\n        \"detectors\": {\n          \"regex\": {\n            \"detector_params\": {\n              \"regex\": [\n                \"email\",\n                \"us-social-security-number\",\n                \"credit-card\"\n              ]\n            }\n          }\n        }\n      }\n    }\n  ]\n}</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Once the shield has been registered, verify that it is working by sending a message containing PII to the <code>/v1/safety/run-shield</code> endpoint:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Email detection example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-curl\" data-lang=\"curl\">curl -X POST http://localhost:8321/v1/safety/run-shield \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"shield_id\": \"regex_detector\",\n  \"messages\": [\n    {\n      \"content\": \"My email is test@example.com\",\n      \"role\": \"user\"\n    }\n  ]\n}' | jq '.'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This should return a response indicating that the email was detected:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-curl\" data-lang=\"curl\">{\n  \"violation\": {\n    \"violation_level\": \"error\",\n    \"user_message\": \"Content violation detected by shield regex_detector (confidence: 1.00, 1/1 processed messages violated)\",\n    \"metadata\": {\n      \"status\": \"violation\",\n      \"shield_id\": \"regex_detector\",\n      \"confidence_threshold\": 0.5,\n      \"summary\": {\n        \"total_messages\": 1,\n        \"processed_messages\": 1,\n        \"skipped_messages\": 0,\n        \"messages_with_violations\": 1,\n        \"messages_passed\": 0,\n        \"message_fail_rate\": 1.0,\n        \"message_pass_rate\": 0.0,\n        \"total_detections\": 1,\n        \"detector_breakdown\": {\n          \"active_detectors\": 1,\n          \"total_checks_performed\": 1,\n          \"total_violations_found\": 1,\n          \"violations_per_message\": 1.0\n        }\n      },\n      \"results\": [\n        {\n          \"message_index\": 0,\n          \"text\": \"My email is test@example.com\",\n          \"status\": \"violation\",\n          \"score\": 1.0,\n          \"detection_type\": \"pii\",\n          \"individual_detector_results\": [\n            {\n              \"detector_id\": \"regex\",\n              \"status\": \"violation\",\n              \"score\": 1.0,\n              \"detection_type\": \"pii\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Social security number (SSN) detection example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-curl\" data-lang=\"curl\">curl -X POST http://localhost:8321/v1/safety/run-shield \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"shield_id\": \"regex_detector\",\n    \"messages\": [\n      {\n        \"content\": \"My SSN is 123-45-6789\",\n        \"role\": \"user\"\n      }\n    ]\n}' | jq '.'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This should return a response indicating that the SSN was detected:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-curl\" data-lang=\"curl\">{\n  \"violation\": {\n    \"violation_level\": \"error\",\n    \"user_message\": \"Content violation detected by shield regex_detector (confidence: 1.00, 1/1 processed messages violated)\",\n    \"metadata\": {\n      \"status\": \"violation\",\n      \"shield_id\": \"regex_detector\",\n      \"confidence_threshold\": 0.5,\n      \"summary\": {\n        \"total_messages\": 1,\n        \"processed_messages\": 1,\n        \"skipped_messages\": 0,\n        \"messages_with_violations\": 1,\n        \"messages_passed\": 0,\n        \"message_fail_rate\": 1.0,\n        \"message_pass_rate\": 0.0,\n        \"total_detections\": 1,\n        \"detector_breakdown\": {\n          \"active_detectors\": 1,\n          \"total_checks_performed\": 1,\n          \"total_violations_found\": 1,\n          \"violations_per_message\": 1.0\n        }\n      },\n      \"results\": [\n        {\n          \"message_index\": 0,\n          \"text\": \"My SSN is 123-45-6789\",\n          \"status\": \"violation\",\n          \"score\": 1.0,\n          \"detection_type\": \"pii\",\n          \"individual_detector_results\": [\n            {\n              \"detector_id\": \"regex\",\n              \"status\": \"violation\",\n              \"score\": 1.0,\n              \"detection_type\": \"pii\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Credit card detection example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-curl\" data-lang=\"curl\">curl -X POST http://localhost:8321/v1/safety/run-shield \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"shield_id\": \"regex_detector\",\n    \"messages\": [\n      {\n        \"content\": \"My credit card number is 4111-1111-1111-1111\",\n        \"role\": \"user\"\n      }\n    ]\n}' | jq '.'</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This should return a response indicating that the credit card number was detected:</p>\n</div>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-curl\" data-lang=\"curl\">{\n  \"violation\": {\n    \"violation_level\": \"error\",\n    \"user_message\": \"Content violation detected by shield regex_detector (confidence: 1.00, 1/1 processed messages violated)\",\n    \"metadata\": {\n      \"status\": \"violation\",\n      \"shield_id\": \"regex_detector\",\n      \"confidence_threshold\": 0.5,\n      \"summary\": {\n        \"total_messages\": 1,\n        \"processed_messages\": 1,\n        \"skipped_messages\": 0,\n        \"messages_with_violations\": 1,\n        \"messages_passed\": 0,\n        \"message_fail_rate\": 1.0,\n        \"message_pass_rate\": 0.0,\n        \"total_detections\": 1,\n        \"detector_breakdown\": {\n          \"active_detectors\": 1,\n          \"total_checks_performed\": 1,\n          \"total_violations_found\": 1,\n          \"violations_per_message\": 1.0\n        }\n      },\n      \"results\": [\n        {\n          \"message_index\": 0,\n          \"text\": \"My credit card number is 4111-1111-1111-1111\",\n          \"status\": \"violation\",\n          \"score\": 1.0,\n          \"detection_type\": \"pii\",\n          \"individual_detector_results\": [\n            {\n              \"detector_id\": \"regex\",\n              \"status\": \"violation\",\n              \"score\": 1.0,\n              \"detection_type\": \"pii\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}</code></pre>\n</div>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n</div>\n<div class=\"sect1\">\n<h2 id=\"evaluating-rag-systems-with-ragas_evaluate\">Evaluating RAG systems with Ragas</h2>\n<div class=\"sectionbody\">\n<div class=\"paragraph _abstract\">\n<p>As an AI engineer, you can use Retrieval-Augmented Generation Assessment (<a href=\"https://docs.ragas.io/en/stable/\">Ragas</a>) to measure and improve the quality of your RAG systems in Open Data Hub. Ragas provides objective metrics that assess retrieval quality, answer relevance, and factual consistency, enabling you to identify issues, optimize configurations, and establish automated quality gates in your development workflows.</p>\n</div>\n<div class=\"paragraph\">\n<p>Ragas is integrated with Open Data Hub through the Llama Stack evaluation API and supports two deployment modes: an inline provider for development and testing, and a remote provider for production-scale evaluations using Open Data Hub pipelines.</p>\n</div>\n<div class=\"sect2\">\n<h3 id=\"_about_ragas_evaluation\">About Ragas evaluation</h3>\n<div class=\"paragraph\">\n<p>Ragas addresses the unique challenges of evaluating RAG systems by providing metrics that assess both the retrieval and generation components of your application. Unlike traditional language model evaluation that focuses solely on output quality, Ragas evaluates how well your system retrieves relevant context and generates responses grounded in that context.</p>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_key_ragas_metrics\">Key Ragas metrics</h4>\n<div class=\"paragraph\">\n<p>Ragas provides <a href=\"https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/\">multiple metrics</a> for evaluating RAG systems. Here are some of the metrics:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\"><strong>Faithfulness</strong></dt>\n<dd>\n<p>Measures the generated answer to determine whether it is consistent with the retrieved context. A high faithfulness score indicates that the answer is well-grounded in the source documents, reducing the risk of hallucinations. This is critical for enterprise and regulated environments where accuracy and trustworthiness are paramount.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Answer Relevancy</strong></dt>\n<dd>\n<p>Evaluates whether the generated answer is consistent with the input question. This metric ensures that your RAG system provides pertinent responses rather than generic or off-topic information.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Context Precision</strong></dt>\n<dd>\n<p>Measures the precision of the retrieval component by evaluating whether the retrieved context chunks contain information relevant to answering the question. High precision indicates that your retrieval system is returning focused, relevant documents rather than irrelevant noise.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Context Recall</strong></dt>\n<dd>\n<p>Measures the recall of the retrieval component by evaluating whether all necessary information for answering the question is present in the retrieved contexts. High recall ensures that your retrieval system is not missing important information.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Answer Correctness</strong></dt>\n<dd>\n<p>Compares the generated answer with a ground truth reference answer to measure accuracy. This metric is useful when you have labeled evaluation datasets with known correct answers.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Answer Similarity</strong></dt>\n<dd>\n<p>Measures the semantic similarity between the generated answer and a reference answer, providing a more nuanced assessment than exact string matching.</p>\n</dd>\n</dl>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_use_cases_for_ragas_in_ai_engineering_workflows\">Use cases for Ragas in AI engineering workflows</h4>\n<div class=\"paragraph\">\n<p>Ragas enables AI engineers to accomplish the following tasks:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\"><strong>Automate quality checks</strong></dt>\n<dd>\n<p>Create reproducible, objective evaluation jobs that can be automatically triggered after every code commit or model update. Automatic quality checks establish quality gates to prevent regressions and ensure that you deploy only high-quality RAG configurations to production.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Enable evaluation-driven development (EDD)</strong></dt>\n<dd>\n<p>Use Ragas metrics to guide iterative optimization. For example, test different chunking strategies, embedding models, or retrieval algorithms against a defined benchmark. You can discover the optimal RAG configuration that maximizes performance metrics. For example, you can maximize faithfulness while minimizing computational cost.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Ensure factual consistency and trustworthiness</strong></dt>\n<dd>\n<p>Measure the reliability of your RAG system by setting thresholds on metrics like faithfulness. Metrics thresholds ensure that responses are consistently grounded in source documents, which is critical for enterprise applications where hallucinations or factual errors are unacceptable.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Achieve production scalability</strong></dt>\n<dd>\n<p>Leverage the remote provider pattern with Open Data Hub pipelines to execute evaluations as distributed jobs. The remote provider pattern allows you to run large-scale benchmarks across thousands of data points without blocking development or consuming excessive local resources.</p>\n</dd>\n<dt class=\"hdlist1\"><strong>Compare model and configuration variants</strong></dt>\n<dd>\n<p>Run comparative evaluations across different models, retrieval strategies, or system configurations to make data-driven decisions about your RAG architecture. For example, compare the impact of different chunk sizes (512 vs 1024 tokens) or different embedding models on retrieval quality metrics.</p>\n</dd>\n</dl>\n</div>\n</div>\n<div class=\"sect3\">\n<h4 id=\"_ragas_provider_deployment_modes\">Ragas provider deployment modes</h4>\n<div class=\"paragraph\">\n<p>Open Data Hub supports two deployment modes for Ragas evaluation:</p>\n</div>\n<div class=\"dlist\">\n<dl>\n<dt class=\"hdlist1\"><strong>Inline provider</strong></dt>\n<dd>\n<p>The inline provider mode runs Ragas evaluation in the same process as the Llama Stack server. Use the inline provider for development and rapid prototyping. It offers the following advantages:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Fast processing with in-memory operations</p>\n</li>\n<li>\n<p>Minimal configuration overhead</p>\n</li>\n<li>\n<p>Local development and testing</p>\n</li>\n<li>\n<p>Evaluation of small to medium-sized datasets</p>\n</li>\n</ul>\n</div>\n</dd>\n<dt class=\"hdlist1\"><strong>Remote provider</strong></dt>\n<dd>\n<p>The remote provider mode runs Ragas evaluation as distributed jobs using Open Data Hub pipelines (powered by Kubeflow Pipelines). Use the remote provider for production environments. It offers the following capabilities:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Running evaluations in parallel across thousands of data points</p>\n</li>\n<li>\n<p>Providing resource isolation and management</p>\n</li>\n<li>\n<p>Integrating with CI/CD pipelines for automated quality gates</p>\n</li>\n<li>\n<p>Storing results in S3-compatible object storage</p>\n</li>\n<li>\n<p>Tracking evaluation history and metrics over time</p>\n</li>\n<li>\n<p>Supporting large-scale batch evaluations without impacting the Llama Stack server</p>\n</li>\n</ul>\n</div>\n</dd>\n</dl>\n</div>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"setting-up-ragas-inline-provider_evaluate\">Setting up the Ragas inline provider for development</h3>\n<div class=\"paragraph _abstract\">\n<p>You can set up the Ragas inline provider to run evaluations directly within the Llama Stack server process. The inline provider is ideal for development environments, rapid prototyping, and lightweight evaluation workloads where simplicity and quick iteration are priorities.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have activated the Llama Stack Operator in Open Data Hub.\nFor more information, see <a href=\"https://opendatahub.io/docs/working-with-llama-stack/#installing-the-llama-stack-operator_rag\">Installing the Llama Stack Operator</a>.</p>\n</li>\n<li>\n<p>You have deployed a Llama model with KServe.\nFor more information, see <a href=\"https://opendatahub.io/docs/working-with-llama-stack/#deploying-a-llama-model-with-kserve_rag\">Deploying a Llama model with KServe</a>.</p>\n</li>\n<li>\n<p>You have created a project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster, log in to the OpenShift CLI (<code>oc</code>) as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc login &lt;openshift_cluster_url&gt; -u &lt;username&gt; -p &lt;password&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Navigate to your project:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc project &lt;project_name&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a ConfigMap for the Ragas inline provider configuration. For example, create a <code>ragas-inline-config.yaml</code> file as follows:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example <code>ragas-inline-config.yaml</code></div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ragas-inline-config\n  namespace: &lt;project_name&gt;\ndata:\n  EMBEDDING_MODEL: \"all-MiniLM-L6-v2\"</code></pre>\n</div>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>EMBEDDING_MODEL</code>: Used by Ragas for semantic similarity calculations. The <code>all-MiniLM-L6-v2</code> model is a lightweight, efficient option suitable for most use cases.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Apply the ConfigMap:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc apply -f ragas-inline-config.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a Llama Stack distribution configuration file with the Ragas inline provider. For example, create a <code>llama-stack-ragas-inline.yaml</code> file as follows:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example <code>llama-stack-ragas-inline.yaml</code></div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: llamastack.trustyai.opendatahub.io/v1alpha1\nkind: LlamaStackDistribution\nmetadata:\n  name: llama-stack-ragas-inline\n  namespace: &lt;project_name&gt;\nspec:\n  replicas: 1\n  server:\n    containerSpec:\n      env:\n# ...\n      - name: VLLM_URL\n        value: &lt;model_url&gt;\n      - name: VLLM_API_TOKEN\n        value: &lt;model_api_token (if necessary)&gt;\n      - name: INFERENCE_MODEL\n        value: &lt;model_name&gt;\n      - name: MILVUS_DB_PATH\n        value: ~/.llama/milvus.db\n      - name: VLLM_TLS_VERIFY\n        value: \"false\"\n      - name: FMS_ORCHESTRATOR_URL\n        value: http://localhost:123\n      - name: EMBEDDING_MODEL\n        value: granite-embedding-125m\n# ...</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the Llama Stack distribution:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f llama-stack-ragas-inline.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Wait for the deployment to complete:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc get pods -w</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Wait until the <code>llama-stack-ragas-inline</code> pod status shows <code>Running</code>.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Next steps</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/working-with-llama-stack/#evaluating-rag-system-quality-with-ragas_rag\">Evaluating RAG system quality with Ragas</a></p>\n</li>\n<li>\n<p><a href=\"https://opendatahub.io/docs/working-with-llama-stack/#configuring-ragas-remote-provider-for-production_rag\">Configuring the Ragas remote provider for production</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"configuring-ragas-remote-provider-for-production_evaluate\">Configuring the Ragas remote provider for production</h3>\n<div class=\"paragraph _abstract\">\n<p>You can configure the Ragas remote provider to run evaluations as distributed jobs using Open Data Hub pipelines. The remote provider enables production-scale evaluations by running Ragas in a separate Kubeflow Pipelines environment, providing resource isolation, improved scalability, and integration with CI/CD workflows.</p>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have cluster administrator privileges for your OpenShift Container Platform cluster.</p>\n</li>\n<li>\n<p>You have installed the Open Data Hub Operator.</p>\n</li>\n<li>\n<p>You have a <code>DataScienceCluster</code> custom resource in your environment; in the <code>spec.components</code> section the <code>llamastackoperator.managementState</code> is enabled with a value of <code>Managed</code>.</p>\n</li>\n<li>\n<p>You have installed the OpenShift CLI (<code>oc</code>) as described in the appropriate documentation for your cluster:</p>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for OpenShift Container Platform</p>\n</li>\n<li>\n<p><a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html/cli_tools/openshift-cli-oc#installing-openshift-cli\" target=\"_blank\" rel=\"noopener\">Installing the OpenShift CLI</a> for Red&#160;Hat OpenShift Service on AWS</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>You have configured a pipeline server in your project.\nFor more information, see <a href=\"https://opendatahub.io/docs/working-with-ai-pipelines/#configuring-a-pipeline-server_ai-pipelines\">Configuring a pipeline server</a>.</p>\n</li>\n<li>\n<p>You have activated the Llama Stack Operator in Open Data Hub.\nFor more information, see <a href=\"https://opendatahub.io/docs/working-with-llama-stack/#installing-the-llama-stack-operator_rag\">Installing the Llama Stack Operator</a>.</p>\n</li>\n<li>\n<p>You have deployed a Large Language Model with KServe.\nFor more information, see <a href=\"https://opendatahub.io/docs/working-with-llama-stack/#deploying-a-llama-model-with-kserve_rag\">Deploying a Llama model with KServe</a>.</p>\n</li>\n<li>\n<p>You have configured S3-compatible object storage for storing evaluation results and you know your S3 credentials: AWS access key, AWS secret access key, and AWS default region.\nFor more information, see <a href=\"https://opendatahub.io/docs/working-on-projects/#adding-a-connection-to-your-project_projects\">Adding a connection to your project</a>.</p>\n</li>\n<li>\n<p>You have created a project.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>In a terminal window, if you are not already logged in to your OpenShift cluster, log in to the OpenShift CLI (<code>oc</code>) as shown in the following example:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc login &lt;openshift_cluster_url&gt; -u &lt;username&gt; -p &lt;password&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Navigate to your project:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc project &lt;project_name&gt;</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a secret for storing S3 credentials:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc create secret generic \"&lt;ragas_s3_credentials&gt;\" \\\n  --from-literal=AWS_ACCESS_KEY_ID=&lt;your_access_key&gt; \\\n  --from-literal=AWS_SECRET_ACCESS_KEY=&lt;your_secret_key&gt; \\\n  --from-literal=AWS_DEFAULT_REGION=&lt;your_region&gt;</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>Replace the placeholder values with your actual S3 credentials. These AWS credentials are required in two locations:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>In the Llama Stack server pod (as environment variables) - to access S3 when creating pipeline runs.</p>\n</li>\n<li>\n<p>In the Kubeflow Pipeline pods (via the secret) - to store evaluation results to S3 during pipeline execution.</p>\n</li>\n</ul>\n</div>\n<div class=\"paragraph\">\n<p>The LlamaStackDistribution configuration loads these credentials from the\n<code>\"&lt;ragas_s3_credentials&gt;\"</code> secret and makes them available to both locations.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n<li>\n<p>Create a secret for the Kubeflow Pipelines API token:</p>\n<div class=\"olist loweralpha\">\n<ol class=\"loweralpha\" type=\"a\">\n<li>\n<p>Get your token by running the following command:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ export KUBEFLOW_PIPELINES_TOKEN=$(oc whoami -t)</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create the secret by running the following command:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc create secret generic kubeflow-pipelines-token \\\n  --from-literal=KUBEFLOW_PIPELINES_TOKEN=\"$KUBEFLOW_PIPELINES_TOKEN\"</code></pre>\n</div>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The Llama Stack distribution service account does not have privileges to create pipeline runs. This secret provides the necessary authentication token for creating and managing pipeline runs.</p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n</li>\n</ol>\n</div>\n</li>\n<li>\n<p>Verify that the Kubeflow Pipelines endpoint is accessible:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ curl -k -H \"Authorization: Bearer $KUBEFLOW_PIPELINES_TOKEN\" \\\n https://$KUBEFLOW_PIPELINES_ENDPOINT/apis/v1beta1/healthz</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a secret for storing your inference model information:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ export INFERENCE_MODEL=\"llama-3-2-3b\"\n$ export VLLM_URL=\"https://llama-32-3b-instruct-predictor:8443/v1\"\n$ export VLLM_TLS_VERIFY=\"false\"  # Use \"true\" in production\n$ export VLLM_API_TOKEN=\"&lt;token_identifier&gt;\"\n\n$ oc create secret generic llama-stack-inference-model-secret \\\n  --from-literal INFERENCE_MODEL=\"$INFERENCE_MODEL\" \\\n  --from-literal VLLM_URL=\"$VLLM_URL\" \\\n  --from-literal VLLM_TLS_VERIFY=\"$VLLM_TLS_VERIFY\" \\\n  --from-literal VLLM_API_TOKEN=\"$VLLM_API_TOKEN\"</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Get the Kubeflow Pipelines endpoint by running the following command and searching for \"pipeline\" in the routes. This is used in a later step for creating a ConfigMap for the Ragas remote provider configuration:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc get routes -A | grep -i pipeline</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>This output should show that the namespace, which is the namespace you specified for <code>KUBEFLOW_NAMESPACE</code>, has the pipeline server endpoint and the associated metadata one. The one to use is <code>ds-pipeline-dspa</code>.</p>\n</div>\n</li>\n<li>\n<p>Create a ConfigMap for the Ragas remote provider configuration. For example, create a <code>kubeflow-ragas-config.yaml</code> file as follows:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example kubeflow-ragas-config.yaml</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kubeflow-ragas-config\n  namespace: &lt;project_name&gt;\ndata:\n  EMBEDDING_MODEL: \"all-MiniLM-L6-v2\"\n  KUBEFLOW_LLAMA_STACK_URL: \"http://$&lt;distribution_name&gt;-service.$&lt;your_namespace&gt;.svc.cluster.local:$&lt;port&gt;\"\n  KUBEFLOW_PIPELINES_ENDPOINT: \"https://&lt;kfp_endpoint&gt;\"\n  KUBEFLOW_NAMESPACE: \"&lt;project_name&gt;\"\n  KUBEFLOW_BASE_IMAGE: \"quay.io/rhoai/odh-trustyai-ragas-lls-provider-dsp-rhel9:rhoai-3.0\"\n  KUBEFLOW_RESULTS_S3_PREFIX: \"s3://&lt;bucket_name&gt;/ragas-results\"\n  KUBEFLOW_S3_CREDENTIALS_SECRET_NAME: \"&lt;ragas_s3_credentials&gt;\"</code></pre>\n</div>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p><code>EMBEDDING_MODEL</code>: Used by Ragas for semantic similarity calculations.</p>\n</li>\n<li>\n<p><code>KUBEFLOW_LLAMA_STACK_URL</code>: The URL for the Llama Stack server. This must be accessible from the Kubeflow Pipeline pods. The &lt;distribution_name&gt;, &lt;namespace&gt;, and &lt;port&gt; are replaced with the name of the LlamaStack distribution you are creating, the namespace where you are creating it, and the port. These 3 elements are present in the LlamaStack distribution YAML.</p>\n</li>\n<li>\n<p><code>KUBEFLOW_PIPELINES_ENDPOINT</code>: The Kubeflow Pipelines API endpoint URL.</p>\n</li>\n<li>\n<p><code>KUBEFLOW_NAMESPACE</code>: The namespace where pipeline runs are executed. This should match your current project namespace.</p>\n</li>\n<li>\n<p><code>KUBEFLOW_BASE_IMAGE</code>: The container image used for Ragas evaluation pipeline components. This image contains the Ragas provider package installed via pip.</p>\n</li>\n<li>\n<p><code>KUBEFLOW_RESULTS_S3_PREFIX</code>: The S3 path prefix where evaluation results are stored. For example: <code>s3://my-bucket/ragas-evaluation-results</code>.</p>\n</li>\n<li>\n<p><code>KUBEFLOW_S3_CREDENTIALS_SECRET_NAME</code>: The name of the secret containing S3 credentials.</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>Apply the ConfigMap:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code>$ oc apply -f kubeflow-ragas-config.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Create a Llama Stack distribution configuration file with the Ragas remote provider. For example, create a llama-stack-ragas-remote.yaml as follows:</p>\n<div class=\"listingblock\">\n<div class=\"title\">Example llama-stack-ragas-remote.yaml</div>\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-yaml\" data-lang=\"yaml\">apiVersion: llamastack.io/v1alpha1\nkind: LlamaStackDistribution\nmetadata:\n  name: llama-stack-pod\nspec:\n  replicas: 1\n  server:\n    containerSpec:\n      resources:\n        requests:\n          cpu: 4\n          memory: \"12Gi\"\n        limits:\n          cpu: 6\n          memory: \"14Gi\"\n      env:\n        - name: INFERENCE_MODEL\n          valueFrom:\n            secretKeyRef:\n              key: INFERENCE_MODEL\n              name: llama-stack-inference-model-secret\n              optional: true\n        - name: VLLM_MAX_TOKENS\n          value: \"4096\"\n        - name: VLLM_URL\n          valueFrom:\n            secretKeyRef:\n              key: VLLM_URL\n              name: llama-stack-inference-model-secret\n              optional: true\n        - name: VLLM_TLS_VERIFY\n          valueFrom:\n            secretKeyRef:\n              key: VLLM_TLS_VERIFY\n              name: llama-stack-inference-model-secret\n              optional: true\n        - name: VLLM_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: VLLM_API_TOKEN\n              name: llama-stack-inference-model-secret\n              optional: true\n        - name: MILVUS_DB_PATH\n          value: ~/milvus.db\n        - name: FMS_ORCHESTRATOR_URL\n          value: \"http://localhost\"\n        - name: KUBEFLOW_PIPELINES_ENDPOINT\n          valueFrom:\n            configMapKeyRef:\n              key: KUBEFLOW_PIPELINES_ENDPOINT\n              name: kubeflow-ragas-config\n              optional: true\n        - name: KUBEFLOW_NAMESPACE\n          valueFrom:\n            configMapKeyRef:\n              key: KUBEFLOW_NAMESPACE\n              name: kubeflow-ragas-config\n              optional: true\n        - name: KUBEFLOW_BASE_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              key: KUBEFLOW_BASE_IMAGE\n              name: kubeflow-ragas-config\n              optional: true\n        - name: KUBEFLOW_LLAMA_STACK_URL\n          valueFrom:\n            configMapKeyRef:\n              key: KUBEFLOW_LLAMA_STACK_URL\n              name: kubeflow-ragas-config\n              optional: true\n        - name: KUBEFLOW_RESULTS_S3_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: KUBEFLOW_RESULTS_S3_PREFIX\n              name: kubeflow-ragas-config\n              optional: true\n        - name: KUBEFLOW_S3_CREDENTIALS_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: KUBEFLOW_S3_CREDENTIALS_SECRET_NAME\n              name: kubeflow-ragas-config\n              optional: true\n        - name: EMBEDDING_MODEL\n          valueFrom:\n            configMapKeyRef:\n              key: EMBEDDING_MODEL\n              name: kubeflow-ragas-config\n              optional: true\n        - name: KUBEFLOW_PIPELINES_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: KUBEFLOW_PIPELINES_TOKEN\n              name: kubeflow-pipelines-token\n              optional: true\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              key: AWS_ACCESS_KEY_ID\n              name: \"&lt;ragas_s3_credentials&gt;\"\n              optional: true\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              key: AWS_SECRET_ACCESS_KEY\n              name: \"&lt;ragas_s3_credentials&gt;\"\n              optional: true\n        - name: AWS_DEFAULT_REGION\n          valueFrom:\n            secretKeyRef:\n              key: AWS_DEFAULT_REGION\n              name: \"&lt;ragas_s3_credentials&gt;\"\n              optional: true\n      name: llama-stack\n      port: 8321\n    distribution:\n      name: rh-dev</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Deploy the Llama Stack distribution:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc apply -f llama-stack-ragas-remote.yaml</code></pre>\n</div>\n</div>\n</li>\n<li>\n<p>Wait for the deployment to complete:</p>\n<div class=\"listingblock\">\n<div class=\"content\">\n<pre class=\"highlight\"><code class=\"language-terminal\" data-lang=\"terminal\">$ oc get pods -w</code></pre>\n</div>\n</div>\n<div class=\"paragraph\">\n<p>Wait until the <code>llama-stack-pod</code> pod status shows <code>Running</code>.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Next steps</div>\n<ul>\n<li>\n<p><a href=\"https://opendatahub.io/docs/working-with-llama-stack/#evaluating-rag-system-quality-with-ragas_rag\">Evaluating RAG system quality with Ragas metrics</a></p>\n</li>\n</ul>\n</div>\n</div>\n<div class=\"sect2\">\n<h3 id=\"evaluating-rag-system-quality-with-ragas_evaluate\">Evaluating RAG system quality with Ragas metrics</h3>\n<div class=\"paragraph _abstract\">\n<p>Evaluate your RAG system quality by testing your setup, using the example provided in the <a href=\"https://github.com/trustyai-explainability/llama-stack-provider-ragas/blob/main/demos/basic_demo.ipynb\">demo notebook</a>. This demo outlines the basic steps for evaluating your RAG system with Ragas using the Python client. You can execute the demo notebook steps from a Jupyter environment.</p>\n</div>\n<div class=\"paragraph\">\n<p>Alternatively, you can submit an evaluation by directly using the <code>http</code> methods of the <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2.34/html/working_with_llama_stack/overview-of-llama-stack_rag#llama_stack_providers_support\">Llama Stack API</a>.</p>\n</div>\n<div class=\"admonitionblock important\">\n<table>\n<tr>\n<td class=\"icon\">\n<div class=\"title\">Important</div>\n</td>\n<td class=\"content\">\n<div class=\"paragraph\">\n<p>The Llama Stack pod must be accessible from the Jupyter environment in the cluster, which may not be the case by default. To configure this setup, see <a href=\"https://docs.redhat.com/en/documentation/open_data_hub/2.34/html/working_with_llama_stack/deploying-a-rag-stack-in-a-project_rag#ingesting-content-into-a-llama-model_rag\">Ingesting content into a Llama model</a></p>\n</div>\n</td>\n</tr>\n</table>\n</div>\n<div class=\"ulist\">\n<div class=\"title\">Prerequisites</div>\n<ul>\n<li>\n<p>You have logged in to Open Data Hub.</p>\n</li>\n<li>\n<p>You have created a project.</p>\n</li>\n<li>\n<p>You have created a pipeline server.</p>\n</li>\n<li>\n<p>You have created a secret for your AWS credentials in your project namespace.</p>\n</li>\n<li>\n<p>You have deployed a Llama Stack distribution with the Ragas evaluation provider enabled (Inline or Remote).\nFor more information, see <a href=\"https://opendatahub.io/docs/monitoring-data-science-models/#configuring-ragas-remote-provider-for-production_monitor\">Configuring the Ragas remote provider for production</a>.</p>\n</li>\n<li>\n<p>You have access to a workbench or notebook environment where you can run Python code.</p>\n</li>\n</ul>\n</div>\n<div class=\"olist arabic\">\n<div class=\"title\">Procedure</div>\n<ol class=\"arabic\">\n<li>\n<p>From the Open Data Hub dashboard, click <strong>Projects</strong>.</p>\n</li>\n<li>\n<p>Click the name of the project that contains the workbench.</p>\n</li>\n<li>\n<p>Click the <strong>Workbenches</strong> tab.</p>\n</li>\n<li>\n<p>If the status of the workbench is <strong>Running</strong>, skip to the next step.</p>\n<div class=\"paragraph\">\n<p>If the status of the workbench is <strong>Stopped</strong>, in the <strong>Status</strong> column for the workbench, click <strong>Start</strong>.</p>\n</div>\n<div class=\"paragraph\">\n<p>The <strong>Status</strong> column changes from <strong>Stopped</strong> to <strong>Starting</strong> when the workbench server is starting, and then to <strong>Running</strong> when the workbench has successfully started.</p>\n</div>\n</li>\n<li>\n<p>Click the open icon (<span class=\"image\"><img src=\"/static/docs/images/open.png\" alt=\"The open icon\"></span>) next to the workbench.</p>\n<div class=\"paragraph\">\n<p>Your Jupyter environment window opens.</p>\n</div>\n</li>\n<li>\n<p>On the toolbar, click the <strong>Git Clone</strong> icon and then select <strong>Clone a Repository</strong>.</p>\n</li>\n<li>\n<p>In the <strong>Clone a repo</strong> dialog, enter the following URL <code>https://github.com/trustyai-explainability/llama-stack-provider-ragas.git</code></p>\n</li>\n<li>\n<p>In the file browser, select the newly-created <code>/llama-stack-provider-ragas/demos</code> folder.</p>\n<div class=\"paragraph\">\n<p>You see a Jupyter notebook named <code>basic_demo.ipynb</code>.</p>\n</div>\n</li>\n<li>\n<p>Double-click the <code>basic_demo.ipynb</code> file to launch the Jupyter notebook.</p>\n<div class=\"paragraph\">\n<p>The Jupyter notebook opens. You see code examples for the following tasks:</p>\n</div>\n<div class=\"ulist\">\n<ul>\n<li>\n<p>Run your Llama Stack distribution</p>\n</li>\n<li>\n<p>Setup and Imports</p>\n</li>\n<li>\n<p>Llama Stack Client Setup</p>\n</li>\n<li>\n<p>Dataset Preparation</p>\n</li>\n<li>\n<p>Dataset Registration</p>\n</li>\n<li>\n<p>Benchmark Registration</p>\n</li>\n<li>\n<p>Evaluation Execution</p>\n</li>\n<li>\n<p>Inline vs Remote Side-by-side</p>\n</li>\n</ul>\n</div>\n</li>\n<li>\n<p>In the Jupyter notebook, run the code cells sequentially through the <strong>Evaluation Execution</strong>.</p>\n</li>\n<li>\n<p>Return to the Open Data Hub dashboard.</p>\n</li>\n<li>\n<p>Click <strong>Develop &amp; train</strong> &#8594; <strong>Pipelines</strong> &#8594; <strong>Runs</strong>. You might need to refresh the page to see that the new evaluation job running.</p>\n</li>\n<li>\n<p>Wait for the job to show <strong>Successful</strong>.</p>\n</li>\n<li>\n<p>Return to the workbench and run the <strong>Results Display</strong> cell.</p>\n</li>\n<li>\n<p>Inspect the results displayed.</p>\n</li>\n</ol>\n</div>\n</div>\n</div>\n</div>","id":"efd01a40-6804-5344-b251-0fb9643fc12f","document":{"title":"Evaluating AI systems"}},"markdownRemark":null},"pageContext":{"id":"efd01a40-6804-5344-b251-0fb9643fc12f"}},"staticQueryHashes":["2604506565"],"slicesMap":{}}